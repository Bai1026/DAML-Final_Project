{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "import random\n",
    "import copy\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 28 21:42:47 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 Ti      On | 00000000:65:00.0 Off |                  N/A |\n",
      "| 30%   44C    P2               37W / 285W|    420MiB / 12282MiB |      8%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4070 Ti      On | 00000000:B3:00.0 Off |                  N/A |\n",
      "|  0%   37C    P8                9W / 285W|      8MiB / 12282MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1236      G   /usr/lib/xorg/Xorg                           56MiB |\n",
      "|    0   N/A  N/A      1400      G   /usr/bin/gnome-shell                          9MiB |\n",
      "|    0   N/A  N/A   1674553      C   ...ai/anaconda3/envs/wmnlab/bin/python      350MiB |\n",
      "|    1   N/A  N/A      1236      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_array_create(dirname, dir_list, time_seq):\n",
    "    \n",
    "    columns = ['RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2',\n",
    "               'nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1', 'nr-RSRP2', 'nr-RSRQ2']\n",
    "    \n",
    "    def reamin_HO_time(y_train):\n",
    "        def f(L):    \n",
    "            for i, e in enumerate(L):\n",
    "                if e: return i+1\n",
    "            return 0\n",
    "\n",
    "        out = []\n",
    "        for a2 in y_train:\n",
    "            a1_out = []\n",
    "            for a1 in a2:\n",
    "                a1_out.append(a1.any())\n",
    "      \n",
    "            out.append(f(a1_out))\n",
    "        return out\n",
    "    \n",
    "    def HO(y_train):\n",
    "        out = []\n",
    "        for a2 in y_train:\n",
    "            if sum(a2.reshape(-1)) == 0: ho = 0\n",
    "            elif sum(a2.reshape(-1)) > 0: ho = 1\n",
    "            out.append(ho)\n",
    "        return out\n",
    "\n",
    "    split_time = []\n",
    "    for i, f in enumerate(tqdm(dir_list)):\n",
    "    \n",
    "        f = os.path.join(dirname, f)\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "        # preprocess data with ffill method\n",
    "        del df['Timestamp'], df['lat'], df['long'], df['gpsspeed']\n",
    "        # df[columns] = df[columns].replace(0, np.nan)\n",
    "        # df[columns] = df[columns].fillna(method='ffill')\n",
    "        # df.dropna(inplace=True)\n",
    "        \n",
    "        df.replace(np.nan,0,inplace=True); df.replace('-',0,inplace=True)\n",
    "        \n",
    "        X = df[features]\n",
    "        Y = df[target]\n",
    "\n",
    "        Xt_list = []\n",
    "        Yt_list = []\n",
    "\n",
    "        for j in range(time_seq):\n",
    "            X_t = X.shift(periods=-j)\n",
    "            Xt_list.append(X_t)\n",
    "    \n",
    "        for j in range(time_seq,time_seq+predict_t):\n",
    "            Y_t = Y.shift(periods=-(j))\n",
    "            Yt_list.append(Y_t)\n",
    "\n",
    "        # YY = Y.shift(periods=-(0))\n",
    "\n",
    "        X_ts = np.array(Xt_list); X_ts = np.transpose(X_ts, (1,0,2)); X_ts = X_ts[:-(time_seq+predict_t-1),:,:]\n",
    "        Y_ts = np.array(Yt_list); Y_ts = np.transpose(Y_ts, (1,0,2)); Y_ts = Y_ts[:-(time_seq+predict_t-1),:,:]\n",
    "        split_time.append(len(X_ts))\n",
    "\n",
    "        if i == 0:\n",
    "            X_final = X_ts\n",
    "            Y_final = Y_ts\n",
    "        else:\n",
    "            X_final = np.concatenate((X_final,X_ts), axis=0)\n",
    "            Y_final = np.concatenate((Y_final,Y_ts), axis=0)\n",
    "\n",
    "    split_time = [(sum(split_time[:i]), sum(split_time[:i])+x) for i, x in enumerate(split_time)]\n",
    "    \n",
    "    return X_final, np.array(HO(Y_final)), np.array(reamin_HO_time(Y_final)), split_time # forecast HO\n",
    "\n",
    "class RNN_Dataset_simple(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset take all csv file specified in dir_list in directory dirname.\n",
    "    Transfer csvs to (features, label) pair\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "\n",
    "        # self.inputs = torch.FloatTensor(X)\n",
    "        # self.labels = torch.FloatTensor(y)\n",
    "        self.inputs = torch.FloatTensor(X.astype(np.float32))\n",
    "        self.labels = torch.FloatTensor(y.astype(np.float32))\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data = self.inputs[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_in_file(file, dates):\n",
    "    \n",
    "    for date in dates:\n",
    "        if date in file: return True \n",
    "    return False\n",
    "\n",
    "def train_valid_split(L, valid_size=0.2):\n",
    "    \n",
    "    length = len(L)\n",
    "    v_num = int(length*valid_size)\n",
    "    v_files = random.sample(L, v_num)\n",
    "    t_files = list(set(L) - set(v_files))\n",
    "    \n",
    "    return t_files, v_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:2\"\n",
    "    \n",
    "    print(f\"Random seed set as {seed}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time sequence length and prediction time length\n",
    "seed = 55688\n",
    "time_seq = 20\n",
    "predict_t = 10\n",
    "valid_ratio = 0.2\n",
    "task = 'classification'\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 55688\n",
      "GPU 0: NVIDIA GeForce RTX 4070 Ti\n",
      "GPU 1: NVIDIA GeForce RTX 4070 Ti\n",
      "Loading training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02dcb78b9f8e4137ba8303232bd69be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading testing data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184dc7b645534e7e8a1eebd813714098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup seed\n",
    "set_seed(seed)\n",
    "\n",
    "# Get GPU\n",
    "device_count = torch.cuda.device_count()\n",
    "num_of_gpus = device_count\n",
    "\n",
    "for i in range(device_count):\n",
    "    print(\"GPU {}: {}\".format(i, torch.cuda.get_device_name(i)))\n",
    "    gpu_id = i\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Save best model to \n",
    "save_path = \"../model\"\n",
    "\n",
    "# Define DataSet\n",
    "dirname = \"../data/single\"\n",
    "dir_list = os.listdir(dirname)\n",
    "dir_list = [f for f in dir_list if ( f.endswith('.csv') and (not 'sm' in f) ) ]\n",
    "\n",
    "train_dates = ['03-26', '04-01']\n",
    "test_dates = ['04-10']\n",
    "    \n",
    "# train_dir_list = [f for f in dir_list if ( f.endswith('.csv') and ('All' in f) and days_in_file(f, train_dates) )]\n",
    "# test_dir_list = [f for f in dir_list if ( f.endswith('.csv') and ('All' in f) and days_in_file(f, test_dates) )]\n",
    "\n",
    "train_dir_list, test_dir_list = train_valid_split(dir_list, valid_ratio)\n",
    "train_dir_list += [f for f in os.listdir(dirname) if 'sm' in f]\n",
    "\n",
    "# features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n",
    "#         'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2',\n",
    "#         'nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1', 'nr-RSRP2', 'nr-RSRQ2' ]\n",
    "features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n",
    "        'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1','nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1']\n",
    "# features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n",
    "#         'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2']\n",
    "\n",
    "num_of_features = len(features)\n",
    "\n",
    "# target = ['LTE_HO', 'MN_HO'] # For eNB HO.\n",
    "# target = ['eNB_to_ENDC'] # Setup gNB\n",
    "target = ['gNB_Rel', 'gNB_HO'] # For gNB HO.\n",
    "# target = ['RLF'] # For RLF\n",
    "# target = ['SCG_RLF'] # For scg failure\n",
    "# target = ['dl-loss'] # For DL loss\n",
    "# target = ['ul-loss'] # For UL loss\n",
    "\n",
    "# Data\n",
    "print('Loading training data...')\n",
    "X_train, y_train1, y_train2, split_time_train = ts_array_create(dirname, train_dir_list, time_seq)\n",
    "\n",
    "train_dataset = RNN_Dataset_simple(X_train, y_train1)\n",
    "train_dataloader1 = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "cond = y_train2 > 0\n",
    "X_train_fore = X_train[cond]\n",
    "y_train2_fore = y_train2[cond]\n",
    "train_dataset = RNN_Dataset_simple(X_train_fore, y_train2_fore)\n",
    "train_dataloader2 = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('Loading testing data...')\n",
    "X_test, y_test1, y_test2, split_time_test = ts_array_create(dirname, test_dir_list, time_seq)\n",
    "\n",
    "test_dataset = RNN_Dataset_simple(X_test, y_test1)\n",
    "test_dataloader1 = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "cond = y_test2 > 0\n",
    "X_test_fore = X_test[cond]\n",
    "y_test2_fore = y_test2[cond]\n",
    "test_dataset = RNN_Dataset_simple(X_test_fore, y_test2_fore)\n",
    "test_dataloader2 = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b = next(iter(train_dataloader1))\n",
    "input_dim, out_dim = a.shape[2], 1\n",
    "a.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Cls(nn.Module):\n",
    "    '''\n",
    "    Using LSTM or GRU.\n",
    "    '''\n",
    "    def __init__(self, input_dim, out_dim, hidden_dim, num_layer, dropout, rnn):\n",
    "\n",
    "        super().__init__()\n",
    "        self.in_dim = input_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dim = hidden_dim\n",
    "        self.num_layer = num_layer\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # input_size: num of features; hidden_size: num of hidden state h\n",
    "        # num_layers: number of recurrent layer; seq; batch_first: batch first than seq\n",
    "        if rnn == 'LSTM':\n",
    "            self.rnn= nn.LSTM(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "        elif rnn == 'GRU':\n",
    "            self.rnn= nn.GRU(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_dim, out_dim) # For binary classification\n",
    "\n",
    "    def forward(self,batch_input):\n",
    "\n",
    "        out,_ = self.rnn(batch_input)\n",
    "        out = self.linear(out[:,-1, :])  #Extract out of last time step (N, L, Hout) -> (Batch, time_seq, output)\n",
    "        \n",
    "        out = torch.sigmoid(out) # Binary Classifier\n",
    "\n",
    "        return out\n",
    "\n",
    "class RNN_Fst(nn.Module):\n",
    "    '''\n",
    "    Using LSTM or GRU.\n",
    "    '''\n",
    "    def __init__(self, input_dim, out_dim, hidden_dim, num_layer, dropout, rnn):\n",
    "\n",
    "        super().__init__()\n",
    "        self.in_dim = input_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dim = hidden_dim\n",
    "        self.num_layer = num_layer\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # input_size: num of features; hidden_size: num of hidden state h\n",
    "        # num_layers: number of recurrent layer; seq; batch_first: batch first than seq\n",
    "        if rnn == 'LSTM':\n",
    "            self.rnn= nn.LSTM(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "        elif rnn == 'GRU':\n",
    "            self.rnn= nn.GRU(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_dim, out_dim) # For binary classification\n",
    "\n",
    "    def forward(self,batch_input):\n",
    "\n",
    "        out,_ = self.rnn(batch_input)\n",
    "        out = self.linear(out[:,-1, :])  #Extract out of last time step (N, L, Hout) -> (Batch, time_seq, output)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_epochs = 600\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "hidden_dim = 128\n",
    "num_layer = 2\n",
    "dropout = 0\n",
    "\n",
    "rnn = 'LSTM' # 'LSTM' or 'GRU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 55688\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "# Define model and optimizer\n",
    "\n",
    "classifier = RNN_Cls(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[600, 1000], gamma=0.4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cls(n_epochs, train_dataloader, test_dataloader, best_model_path, early_stopping_patience=30):\n",
    "    \n",
    "    # 初始化變數\n",
    "    best_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = early_stopping_patience\n",
    "    \n",
    "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "        classifier.train()\n",
    "\n",
    "        train_losses = []\n",
    "        \n",
    "        trues = np.array([])\n",
    "        preds = np.array([])\n",
    "\n",
    "        for i, (features, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = classifier(features)\n",
    "            \n",
    "            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n",
    "            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # metrics calculate\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(trues, preds)\n",
    "        aucpr = auc(recall, precision)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(trues, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        train_losses_for_epochs.append(train_loss) # Record Loss\n",
    "\n",
    "        print(f'Epoch {epoch} train loss: {train_loss}, auc: {roc_auc}, aucpr: {aucpr}', end = '; ')\n",
    "        \n",
    "        # Validate\n",
    "        classifier.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = np.array([])\n",
    "        preds = np.array([])\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = classifier(features)\n",
    "\n",
    "            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n",
    "            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(trues, preds)\n",
    "        aucpr = auc(recall, precision)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(trues, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        valid_losses_for_epochs.append(valid_loss) # Record Loss\n",
    "        \n",
    "        print(f'Epoch {epoch} valid loss: {valid_loss}, auc: {roc_auc}, aucpr: {aucpr}')\n",
    "        \n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(classifier.state_dict(), best_model_path)\n",
    "            # best_model.load_state_dict(copy.deepcopy(classifier.state_dict()))\n",
    "            print(f'Best model found! Loss: {valid_loss}')\n",
    "            \n",
    "        else:\n",
    "            # 驗證損失沒有改善，計數器加1\n",
    "            early_stopping_counter += 1\n",
    "            \n",
    "            # 如果計數器達到早期停止的耐心值，則停止訓練\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print('Early stopping triggered.')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model/lte_HO_cls_RNN.pt\n"
     ]
    }
   ],
   "source": [
    "# For record loss\n",
    "train_losses_for_epochs = []\n",
    "validation_losses_for_epochs = []\n",
    "valid_losses_for_epochs = []\n",
    "\n",
    "# Save best model to ...\n",
    "best_model_path = os.path.join(save_path, 'lte_HO_cls_RNN.pt')\n",
    "print(best_model_path)\n",
    "\n",
    "early_stopping_patience = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visulized on many sample on validation data\n",
    "# sample_value = 2\n",
    "# # samples = random.sample(split_time_test, sample_value)\n",
    "# samples = [split_time_test[8], split_time_test[9]]\n",
    "\n",
    "# fig, axs = plt.subplots(1, sample_value, figsize=(14, 2.5))\n",
    "\n",
    "# # y_test\n",
    "# # preds\n",
    "\n",
    "# for i in range(sample_value):\n",
    "#     true = [y_test1[i] for i in range(samples[i][0], samples[i][1])]\n",
    "#     axs[i].plot(true, label='true')\n",
    "#     prediction = [preds[i] for i in range(samples[i][0], samples[i][1])]\n",
    "#     # prediction = [1 if preds[i] > 0.5 else 0  for i in range(samples[i][0], samples[i][1])]\n",
    "#     axs[i].plot(prediction, label='pred')\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb1165540b34703bb8a4d3d346dcf78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.5220834463641927, auc: 0.7534707135451741, aucpr: 0.5501946054343847; Epoch 1 valid loss: 0.5473681467163206, auc: 0.7633205559653998, aucpr: 0.6172587018045068\n",
      "Best model found! Loss: 0.5473681467163206\n",
      "Epoch 2 train loss: 0.49576472008875866, auc: 0.7924494666800321, aucpr: 0.6100383987147024; Epoch 2 valid loss: 0.5165096078877257, auc: 0.788462156078287, aucpr: 0.6442502933053195\n",
      "Best model found! Loss: 0.5165096078877257\n",
      "Epoch 3 train loss: 0.48351824672400184, auc: 0.8087010709695576, aucpr: 0.6345529642429691; Epoch 3 valid loss: 0.500070560611924, auc: 0.8108908386622166, aucpr: 0.668208002847516\n",
      "Best model found! Loss: 0.500070560611924\n",
      "Epoch 4 train loss: 0.4790722151664825, auc: 0.8131517320604245, aucpr: 0.6409644077930694; Epoch 4 valid loss: 0.4914995712866442, auc: 0.8223694845323126, aucpr: 0.6860832924374562\n",
      "Best model found! Loss: 0.4914995712866442\n",
      "Epoch 5 train loss: 0.467615180087438, auc: 0.8261380817987884, aucpr: 0.6647945728975754; Epoch 5 valid loss: 0.48684500704069483, auc: 0.8253747117345178, aucpr: 0.6968736285448814\n",
      "Best model found! Loss: 0.48684500704069483\n",
      "Epoch 6 train loss: 0.4630988961410288, auc: 0.8307281777893234, aucpr: 0.6744390581569795; Epoch 6 valid loss: 0.4857332229314281, auc: 0.8278052037414452, aucpr: 0.7074882412637938\n",
      "Best model found! Loss: 0.4857332229314281\n",
      "Epoch 7 train loss: 0.4544965529046959, auc: 0.8377602276251508, aucpr: 0.6873539626933454; Epoch 7 valid loss: 0.4756670739634262, auc: 0.8328292910914591, aucpr: 0.7112527826487769\n",
      "Best model found! Loss: 0.4756670739634262\n",
      "Epoch 8 train loss: 0.44947873370752384, auc: 0.8430013443475142, aucpr: 0.6978752203140812; Epoch 8 valid loss: 0.4738127857600147, auc: 0.8380113764502766, aucpr: 0.7225875455120617\n",
      "Best model found! Loss: 0.4738127857600147\n",
      "Epoch 9 train loss: 0.44738234255994125, auc: 0.8451322406686066, aucpr: 0.704046193478846; Epoch 9 valid loss: 0.4632982106858425, auc: 0.8458281965923841, aucpr: 0.7340131455326951\n",
      "Best model found! Loss: 0.4632982106858425\n",
      "Epoch 10 train loss: 0.4346556140111587, auc: 0.855308157148323, aucpr: 0.724364653192439; Epoch 10 valid loss: 0.4491173495444354, auc: 0.8576376498658458, aucpr: 0.7582970652661681\n",
      "Best model found! Loss: 0.4491173495444354\n",
      "Epoch 11 train loss: 0.43132188096363516, auc: 0.8584286517417079, aucpr: 0.7317932094903702; Epoch 11 valid loss: 0.44707376505021523, auc: 0.8581219435933752, aucpr: 0.7616728228619521\n",
      "Best model found! Loss: 0.44707376505021523\n",
      "Epoch 12 train loss: 0.42505689862720974, auc: 0.8628670409906927, aucpr: 0.7407704791869695; Epoch 12 valid loss: 0.43823952444441866, auc: 0.868374301705494, aucpr: 0.7775146011923589\n",
      "Best model found! Loss: 0.43823952444441866\n",
      "Epoch 13 train loss: 0.421441242912934, auc: 0.8656382970092197, aucpr: 0.745847773678089; Epoch 13 valid loss: 0.4374539702677269, auc: 0.8659280484844628, aucpr: 0.7703753570152844\n",
      "Best model found! Loss: 0.4374539702677269\n",
      "Epoch 14 train loss: 0.4131519010322309, auc: 0.8719288311856999, aucpr: 0.7579760737545806; Epoch 14 valid loss: 0.4348419713595889, auc: 0.8692983956390057, aucpr: 0.782791450982956\n",
      "Best model found! Loss: 0.4348419713595889\n",
      "Epoch 15 train loss: 0.4089035809735673, auc: 0.8751758309226283, aucpr: 0.7634162044094978; Epoch 15 valid loss: 0.427204861232404, auc: 0.8753707076144, aucpr: 0.7880706341394886\n",
      "Best model found! Loss: 0.427204861232404\n",
      "Epoch 16 train loss: 0.4058784412950214, auc: 0.8775164949642265, aucpr: 0.7685622348242644; Epoch 16 valid loss: 0.4173775795665796, auc: 0.87979726388885, aucpr: 0.7947376234515589\n",
      "Best model found! Loss: 0.4173775795665796\n",
      "Epoch 17 train loss: 0.402379750882193, auc: 0.8792443957518882, aucpr: 0.7734153519198929; Epoch 17 valid loss: 0.4220024623562781, auc: 0.877333884075469, aucpr: 0.7903232059272096\n",
      "Epoch 18 train loss: 0.39791851485923174, auc: 0.8822372419933766, aucpr: 0.7772518704893348; Epoch 18 valid loss: 0.4198710761531797, auc: 0.8778988241659365, aucpr: 0.7910513186895329\n",
      "Epoch 19 train loss: 0.3929379709948507, auc: 0.8858684858607264, aucpr: 0.7854716997156324; Epoch 19 valid loss: 0.4079217142198751, auc: 0.8866718246303431, aucpr: 0.8069867389592509\n",
      "Best model found! Loss: 0.4079217142198751\n",
      "Epoch 20 train loss: 0.3926210404358482, auc: 0.885678602962873, aucpr: 0.7862186176098991; Epoch 20 valid loss: 0.40744792146796593, auc: 0.8850243473655235, aucpr: 0.8043194540529153\n",
      "Best model found! Loss: 0.40744792146796593\n",
      "Epoch 21 train loss: 0.3886124560953269, auc: 0.8886929354222817, aucpr: 0.7915644514863864; Epoch 21 valid loss: 0.40908089594057434, auc: 0.8835887115038282, aucpr: 0.8032986688183593\n",
      "Epoch 22 train loss: 0.3830117558720799, auc: 0.8918840231785352, aucpr: 0.7983409306113545; Epoch 22 valid loss: 0.4051795722969044, auc: 0.8860550001665168, aucpr: 0.80638507715144\n",
      "Best model found! Loss: 0.4051795722969044\n",
      "Epoch 23 train loss: 0.37867786268288506, auc: 0.8945014178098986, aucpr: 0.8040096556683467; Epoch 23 valid loss: 0.4139461237344235, auc: 0.8801625916161382, aucpr: 0.8002433634100018\n",
      "Epoch 24 train loss: 0.37668569114597317, auc: 0.8956018158056602, aucpr: 0.8054553990151011; Epoch 24 valid loss: 0.40960566978552815, auc: 0.884880359324812, aucpr: 0.8083291081005235\n",
      "Epoch 25 train loss: 0.37607554295624923, auc: 0.8964208121146579, aucpr: 0.8051363479964706; Epoch 25 valid loss: 0.4061038734300641, auc: 0.8853033594419379, aucpr: 0.806344797050939\n",
      "Epoch 26 train loss: 0.3739937803084077, auc: 0.8974901908186609, aucpr: 0.8097348958802044; Epoch 26 valid loss: 0.4151900995501809, auc: 0.8799041789419612, aucpr: 0.8024496420674785\n",
      "Epoch 27 train loss: 0.37278080513116285, auc: 0.8979999049365688, aucpr: 0.8107327258126484; Epoch 27 valid loss: 0.40679472765086516, auc: 0.8864214558149648, aucpr: 0.8093070881606406\n",
      "Epoch 28 train loss: 0.3644911264171165, auc: 0.9027480980019267, aucpr: 0.8203602128436476; Epoch 28 valid loss: 0.39860851599656966, auc: 0.8891909474947357, aucpr: 0.8081640108295136\n",
      "Best model found! Loss: 0.39860851599656966\n",
      "Epoch 29 train loss: 0.36072946948989, auc: 0.904973563047997, aucpr: 0.8230758650932691; Epoch 29 valid loss: 0.4113886152265056, auc: 0.8829203588522725, aucpr: 0.7962579210091248\n",
      "Epoch 30 train loss: 0.3594655897339544, auc: 0.9060653801101889, aucpr: 0.8252513700227242; Epoch 30 valid loss: 0.41518910905897394, auc: 0.8774786141695748, aucpr: 0.7918165637948263\n",
      "Epoch 31 train loss: 0.3582911904568322, auc: 0.9065317942469682, aucpr: 0.8274298359104997; Epoch 31 valid loss: 0.412307035526756, auc: 0.8812724660375517, aucpr: 0.798253397101153\n",
      "Epoch 32 train loss: 0.35560465811776115, auc: 0.9077788615636428, aucpr: 0.8297256779847291; Epoch 32 valid loss: 0.40488986505460595, auc: 0.8853263927793099, aucpr: 0.8078674830415781\n",
      "Epoch 33 train loss: 0.3536108093534886, auc: 0.9092996347587672, aucpr: 0.8326103654207515; Epoch 33 valid loss: 0.4217755811512191, auc: 0.8778800353739822, aucpr: 0.8030158368041411\n",
      "Epoch 34 train loss: 0.34747568649889604, auc: 0.9123678856904593, aucpr: 0.8389984134434857; Epoch 34 valid loss: 0.4132575501691313, auc: 0.8803560597772094, aucpr: 0.803469634830001\n",
      "Epoch 35 train loss: 0.34941765535849484, auc: 0.9117847696147154, aucpr: 0.8353479088808643; Epoch 35 valid loss: 0.39721528113878957, auc: 0.8907789714415705, aucpr: 0.8201003977242141\n",
      "Best model found! Loss: 0.39721528113878957\n",
      "Epoch 36 train loss: 0.35050473245447195, auc: 0.9108047324646793, aucpr: 0.8352613590958242; Epoch 36 valid loss: 0.40538322516675157, auc: 0.8849605901378492, aucpr: 0.8091196455090355\n",
      "Epoch 37 train loss: 0.3500705302174934, auc: 0.9109446763772551, aucpr: 0.836632461776462; Epoch 37 valid loss: 0.4091311280444711, auc: 0.8819730831707098, aucpr: 0.8130396728860837\n",
      "Epoch 38 train loss: 0.3421993258922843, auc: 0.9152944892858337, aucpr: 0.843750518812069; Epoch 38 valid loss: 0.41681560804397705, auc: 0.8798076526363763, aucpr: 0.8000859177639643\n",
      "Epoch 39 train loss: 0.34119411586783893, auc: 0.9159425773578672, aucpr: 0.8446330351540666; Epoch 39 valid loss: 0.40360818800171105, auc: 0.8877382147228261, aucpr: 0.8144958011040573\n",
      "Epoch 40 train loss: 0.3377035633368353, auc: 0.9176645384693941, aucpr: 0.8486700710978634; Epoch 40 valid loss: 0.4007635317387038, auc: 0.8887261252482836, aucpr: 0.8185169970934104\n",
      "Epoch 41 train loss: 0.3316124756737759, auc: 0.9208461797442419, aucpr: 0.854001186555525; Epoch 41 valid loss: 0.4023749736165593, auc: 0.8894145727057683, aucpr: 0.8181871513404448\n",
      "Epoch 42 train loss: 0.3298381450501071, auc: 0.9217452561700639, aucpr: 0.8551273228923992; Epoch 42 valid loss: 0.4007298204496726, auc: 0.8909407687637549, aucpr: 0.82286556419647\n",
      "Epoch 43 train loss: 0.32507316358918664, auc: 0.924089527727697, aucpr: 0.8598263106881786; Epoch 43 valid loss: 0.41048315835049315, auc: 0.8849170167625113, aucpr: 0.8156804557921661\n",
      "Epoch 44 train loss: 0.32371562114741476, auc: 0.924984320463533, aucpr: 0.8614613102118748; Epoch 44 valid loss: 0.4001415195824292, auc: 0.8911224234347822, aucpr: 0.8217376445769409\n",
      "Epoch 45 train loss: 0.3224199771512406, auc: 0.9255665619557089, aucpr: 0.8633109666772304; Epoch 45 valid loss: 0.41988893400281246, auc: 0.8806487553182225, aucpr: 0.8050957355412429\n",
      "Epoch 46 train loss: 0.32022281538406133, auc: 0.9265402929177272, aucpr: 0.8656265953953093; Epoch 46 valid loss: 0.40752033015873546, auc: 0.8868995756582503, aucpr: 0.8150248455890935\n",
      "Epoch 47 train loss: 0.31668517282788894, auc: 0.9283119936636929, aucpr: 0.868680740296651; Epoch 47 valid loss: 0.4073043873669642, auc: 0.8881385376882092, aucpr: 0.8163475566998842\n",
      "Epoch 48 train loss: 0.30868846759163837, auc: 0.932160806288493, aucpr: 0.8750234530023205; Epoch 48 valid loss: 0.39962613323663293, auc: 0.8932572813692535, aucpr: 0.8244506137669371\n",
      "Epoch 49 train loss: 0.3043894211891873, auc: 0.9341518251472894, aucpr: 0.8781556006660688; Epoch 49 valid loss: 0.40676948250609724, auc: 0.8914700902912762, aucpr: 0.8169866213754904\n",
      "Epoch 50 train loss: 0.3078582655984865, auc: 0.93252917501413, aucpr: 0.8765937866635776; Epoch 50 valid loss: 0.4188695450833116, auc: 0.8822840629073688, aucpr: 0.8058024432211932\n",
      "Epoch 51 train loss: 0.30302609623910776, auc: 0.9345072377625834, aucpr: 0.8805046696009267; Epoch 51 valid loss: 0.4242347728640664, auc: 0.8787962041772384, aucpr: 0.8018366053303998\n",
      "Epoch 52 train loss: 0.2991078597850956, auc: 0.9364584747401874, aucpr: 0.8826534255210134; Epoch 52 valid loss: 0.41807388141925694, auc: 0.88552790479918, aucpr: 0.8130230362080294\n",
      "Epoch 53 train loss: 0.29636285216148794, auc: 0.9377039690865016, aucpr: 0.8853334635458058; Epoch 53 valid loss: 0.4125678562794634, auc: 0.8885360408506923, aucpr: 0.8166723269148217\n",
      "Epoch 54 train loss: 0.30029422507483855, auc: 0.9361429968605538, aucpr: 0.8815001605137075; Epoch 54 valid loss: 0.41122188547640093, auc: 0.8894325007157848, aucpr: 0.8149233916148915\n",
      "Epoch 55 train loss: 0.297470972644475, auc: 0.9370560954248606, aucpr: 0.8852552315345331; Epoch 55 valid loss: 0.4149925056810069, auc: 0.8877489596559817, aucpr: 0.816922772298836\n",
      "Epoch 56 train loss: 0.29113857063341353, auc: 0.9399586575725789, aucpr: 0.8894610267010428; Epoch 56 valid loss: 0.41947441505379834, auc: 0.8828706709569624, aucpr: 0.812169785199302\n",
      "Epoch 57 train loss: 0.28552467271106957, auc: 0.9420780931969879, aucpr: 0.8947674689157047; Epoch 57 valid loss: 0.4311404980837685, auc: 0.8807612209307261, aucpr: 0.8086858380476287\n",
      "Epoch 58 train loss: 0.2841838077895412, auc: 0.9426600191915184, aucpr: 0.8960095660822842; Epoch 58 valid loss: 0.41145399414939887, auc: 0.8904121892896287, aucpr: 0.8252206159224076\n",
      "Epoch 59 train loss: 0.28482312708628577, auc: 0.9425477099360581, aucpr: 0.8946563405709749; Epoch 59 valid loss: 0.4349259235756646, auc: 0.878958951327768, aucpr: 0.8066430888546243\n",
      "Epoch 60 train loss: 0.287979098945025, auc: 0.9412021092787085, aucpr: 0.8929463268995675; Epoch 60 valid loss: 0.4295646208879095, auc: 0.8767812323892178, aucpr: 0.7983858544954568\n",
      "Epoch 61 train loss: 0.2796944921313611, auc: 0.9445420770983254, aucpr: 0.8993241654212714; Epoch 61 valid loss: 0.4151194612565363, auc: 0.8873342705368769, aucpr: 0.8196381439042637\n",
      "Epoch 62 train loss: 0.2768832582657279, auc: 0.9456910009285158, aucpr: 0.9019106483210665; Epoch 62 valid loss: 0.4273641411073799, auc: 0.8792794293478794, aucpr: 0.8056427076354346\n",
      "Epoch 63 train loss: 0.2749113825249627, auc: 0.9466001617960776, aucpr: 0.9023285362069646; Epoch 63 valid loss: 0.4318906591894726, auc: 0.8828119893745078, aucpr: 0.8120130099111558\n",
      "Epoch 64 train loss: 0.2794055123953108, auc: 0.9447790054765007, aucpr: 0.8998301246187854; Epoch 64 valid loss: 0.42587947427759065, auc: 0.8865246902832383, aucpr: 0.8183488291507929\n",
      "Epoch 65 train loss: 0.27198474908333764, auc: 0.9475926413374389, aucpr: 0.9049058668831075; Epoch 65 valid loss: 0.430028604499626, auc: 0.8865066435446788, aucpr: 0.8216940264151796\n",
      "Epoch 66 train loss: 0.27511232567016075, auc: 0.9465708760471685, aucpr: 0.9022713464419939; Epoch 66 valid loss: 0.446396725962971, auc: 0.8823028516993231, aucpr: 0.8046291979615866\n",
      "Epoch 67 train loss: 0.2707958642584438, auc: 0.9480830235065512, aucpr: 0.9066648064787475; Epoch 67 valid loss: 0.42449371672099395, auc: 0.8877250061724, aucpr: 0.8182622128411948\n",
      "Epoch 68 train loss: 0.2752180894767736, auc: 0.9464510644986557, aucpr: 0.9029229735723885; Epoch 68 valid loss: 0.4305120701203995, auc: 0.8838429389968592, aucpr: 0.812065159130171\n",
      "Epoch 69 train loss: 0.2617640265271976, auc: 0.9516977989991987, aucpr: 0.9121679739442103; Epoch 69 valid loss: 0.43813369243339484, auc: 0.8846190674834636, aucpr: 0.8129663167120427\n",
      "Epoch 70 train loss: 0.2638340819856683, auc: 0.9508414725043144, aucpr: 0.9108341683828014; Epoch 70 valid loss: 0.438337551333073, auc: 0.8799822429590859, aucpr: 0.8093872728484477\n",
      "Epoch 71 train loss: 0.2668157373576242, auc: 0.9496198934818957, aucpr: 0.9091089155319092; Epoch 71 valid loss: 0.42908220771745487, auc: 0.8871051838128592, aucpr: 0.8170260288234671\n",
      "Epoch 72 train loss: 0.257827707032207, auc: 0.9531412848716903, aucpr: 0.9150581682449146; Epoch 72 valid loss: 0.45227821029884885, auc: 0.883780962697332, aucpr: 0.8076442348292597\n",
      "Epoch 73 train loss: 0.25866769210759993, auc: 0.9528496165225891, aucpr: 0.9149280432135872; Epoch 73 valid loss: 0.41016070486817247, auc: 0.8947486305998242, aucpr: 0.8256190839656825\n",
      "Epoch 74 train loss: 0.25391862593183945, auc: 0.9545984043461871, aucpr: 0.9178842352748037; Epoch 74 valid loss: 0.4681169084937762, auc: 0.875429300150447, aucpr: 0.7964564266131129\n",
      "Epoch 75 train loss: 0.25270516516355895, auc: 0.9550017801333428, aucpr: 0.9185452100744383; Epoch 75 valid loss: 0.46866839118423953, auc: 0.8748769452855536, aucpr: 0.7987758231724554\n",
      "Epoch 76 train loss: 0.25834158116288924, auc: 0.952884427056935, aucpr: 0.9148323756798087; Epoch 76 valid loss: 0.46601142992844546, auc: 0.8786913075093608, aucpr: 0.8047027889940632\n",
      "Epoch 77 train loss: 0.25448622357391915, auc: 0.954149707397582, aucpr: 0.9185132228025785; Epoch 77 valid loss: 0.48077840403887384, auc: 0.8716234566996114, aucpr: 0.7924962910446727\n",
      "Epoch 78 train loss: 0.26173481666619414, auc: 0.9517515190618652, aucpr: 0.9124614994872422; Epoch 78 valid loss: 0.4512318280217681, auc: 0.8840541867572677, aucpr: 0.810275139499562\n",
      "Epoch 79 train loss: 0.2497566435332404, auc: 0.9559318248366862, aucpr: 0.9208765212360699; Epoch 79 valid loss: 0.45735608089247837, auc: 0.8841874595469592, aucpr: 0.8140524485138471\n",
      "Epoch 80 train loss: 0.2591182436683166, auc: 0.9526106679053149, aucpr: 0.9149426921640528; Epoch 80 valid loss: 0.45266377608542946, auc: 0.8831642272799132, aucpr: 0.8139369464679116\n",
      "Epoch 81 train loss: 0.2553599096224537, auc: 0.9537901614995927, aucpr: 0.9185530929252335; Epoch 81 valid loss: 0.4458313489128741, auc: 0.8831793058048936, aucpr: 0.8085813192379332\n",
      "Epoch 82 train loss: 0.2581632896999625, auc: 0.9531437824986341, aucpr: 0.9150810296879393; Epoch 82 valid loss: 0.4283017559362443, auc: 0.8886995300546169, aucpr: 0.8174165524040387\n",
      "Epoch 83 train loss: 0.25482864559103197, auc: 0.9542351327146681, aucpr: 0.9175329347550247; Epoch 83 valid loss: 0.4283322188044002, auc: 0.8866600705045709, aucpr: 0.8132438283740282\n",
      "Epoch 84 train loss: 0.25913804652666683, auc: 0.9525493766521389, aucpr: 0.9146903083585627; Epoch 84 valid loss: 0.4569368657107946, auc: 0.8817058549022035, aucpr: 0.8055490737899995\n",
      "Epoch 85 train loss: 0.25870600839479707, auc: 0.9528279179649649, aucpr: 0.9146062314261909; Epoch 85 valid loss: 0.46873193362218496, auc: 0.878622563682874, aucpr: 0.8025376399683163\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "train_cls(n_epochs, train_dataloader1, test_dataloader1, best_model_path, early_stopping_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.39721528113878957, roc_auc 0.8907789714415705, aucpr 0.8201003977242141\n",
      "\n",
      "roc_auc: 0.8907789714415705, aucpr: 0.8201003977242141, precision: 0.7314088484468152, recall: 0.7454429165334187, f1: 0.738359201773836\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "def test(test_dataloader):\n",
    "    best_model = RNN_Cls(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "    best_model.load_state_dict(torch.load(best_model_path))\n",
    "    best_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        best_model.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = np.array([])\n",
    "        preds = np.array([])\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = best_model(features)\n",
    "\n",
    "            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n",
    "            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(trues, preds)\n",
    "        aucpr = auc(recall, precision)\n",
    "        threshold = 0.5\n",
    "        p = precision_score(trues, [1 if pred > threshold else 0 for pred in preds])\n",
    "        r = recall_score(trues, [1 if pred > threshold else 0 for pred in preds])\n",
    "        f1 = f1_score(trues, [1 if pred > threshold else 0 for pred in preds])\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(trues, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "\n",
    "        print(f'valid loss {valid_loss}, roc_auc {roc_auc}, aucpr {aucpr}')\n",
    "        \n",
    "        return valid_loss, roc_auc, aucpr, p, r, f1\n",
    "        \n",
    "\n",
    "# test(test_dataloader1)\n",
    "_, roc_auc, aucpr, p, r, f1 = test(test_dataloader1)\n",
    "print()\n",
    "print(f'roc_auc: {roc_auc}, aucpr: {aucpr}, precision: {p}, recall: {r}, f1: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import itertools\n",
    "\n",
    "n_epochs = 600\n",
    "lrs = [0.001, 0.01, 0.1]\n",
    "hidden_dims = [32, 64, 128]\n",
    "num_layers = [1, 2]\n",
    "dropout = 0\n",
    "\n",
    "early_stopping_patience = 50\n",
    "rnn = 'GRU'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For learning_rate = 0.001, hidden_dim = 32, num_layer = 1.\n",
      "valid loss 0.549439296380997, roc_auc 0.7551104475240914, aucpr 0.6032514706158869\n",
      "Random seed set as 55688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab0ae9243a044e6b0359b1e69078e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.541000144682842, auc: 0.7118112640753881, aucpr: 0.48047472135696795; Epoch 1 valid loss: 0.5678944449090444, auc: 0.722749202159031, aucpr: 0.5656147478250108\n",
      "Best model found! Loss: 0.5678944449090444\n",
      "Epoch 2 train loss: 0.514731068915407, auc: 0.7666205275832194, aucpr: 0.5752750753710775; Epoch 2 valid loss: 0.5345588791103901, auc: 0.7709914587279697, aucpr: 0.6144170002258098\n",
      "Best model found! Loss: 0.5345588791103901\n",
      "Epoch 3 train loss: 0.49325764254889504, auc: 0.7984053932025881, aucpr: 0.6185327048338755; Epoch 3 valid loss: 0.5354040742046042, auc: 0.7819240720280954, aucpr: 0.6282306262857146\n",
      "Epoch 4 train loss: 0.4835167354683925, auc: 0.8093598748551106, aucpr: 0.6334439065249375; Epoch 4 valid loss: 0.5254436214123103, auc: 0.7894912654082193, aucpr: 0.6352728463651925\n",
      "Best model found! Loss: 0.5254436214123103\n",
      "Epoch 5 train loss: 0.4731254353979983, auc: 0.8202287578174378, aucpr: 0.6513563960219914; Epoch 5 valid loss: 0.540102713319302, auc: 0.7741410301565157, aucpr: 0.6084526143998034\n",
      "Epoch 6 train loss: 0.46488445853333055, auc: 0.82804087671073, aucpr: 0.6648383345462454; Epoch 6 valid loss: 0.48767312922364986, auc: 0.8200837226194914, aucpr: 0.6851879462880681\n",
      "Best model found! Loss: 0.48767312922364986\n",
      "Epoch 7 train loss: 0.46261526161493677, auc: 0.8308102205424546, aucpr: 0.6727976684674848; Epoch 7 valid loss: 0.5210159726350216, auc: 0.7872787293243889, aucpr: 0.639797419720193\n",
      "Epoch 8 train loss: 0.45744669509836716, auc: 0.8349994747039504, aucpr: 0.679486381147696; Epoch 8 valid loss: 0.47200754046272697, auc: 0.8328967289039712, aucpr: 0.7083779696235986\n",
      "Best model found! Loss: 0.47200754046272697\n",
      "Epoch 9 train loss: 0.4536587899066649, auc: 0.8385342702258038, aucpr: 0.6865248122187942; Epoch 9 valid loss: 0.5153228135409762, auc: 0.7938042578311417, aucpr: 0.6461516793617021\n",
      "Epoch 10 train loss: 0.4527065085870147, auc: 0.8397213743536747, aucpr: 0.6878769589747092; Epoch 10 valid loss: 0.4616347989067435, auc: 0.8410699424504942, aucpr: 0.7248010725538417\n",
      "Best model found! Loss: 0.4616347989067435\n",
      "Epoch 11 train loss: 0.4488945175686395, auc: 0.8424070147400087, aucpr: 0.6914228568659566; Epoch 11 valid loss: 0.4928772924642326, auc: 0.8186890184230488, aucpr: 0.6911570676854353\n",
      "Epoch 12 train loss: 0.4435578221761036, auc: 0.8469123534928988, aucpr: 0.7041096932547362; Epoch 12 valid loss: 0.4859507603619652, auc: 0.8279377938420136, aucpr: 0.7081081179165741\n",
      "Epoch 13 train loss: 0.4403677786823027, auc: 0.8505907399746906, aucpr: 0.7115789837845087; Epoch 13 valid loss: 0.48348727906366973, auc: 0.8257250499832326, aucpr: 0.7006236271963855\n",
      "Epoch 14 train loss: 0.4389885087669199, auc: 0.8511579303843904, aucpr: 0.716519401813306; Epoch 14 valid loss: 0.49422052699884167, auc: 0.8220677953041556, aucpr: 0.6988768733780689\n",
      "Epoch 15 train loss: 0.4375762604655436, auc: 0.8520901799946069, aucpr: 0.7170002504311463; Epoch 15 valid loss: 0.4650730121120364, auc: 0.846028432280415, aucpr: 0.7407387321976131\n",
      "Epoch 16 train loss: 0.43163132349540106, auc: 0.8573313243888998, aucpr: 0.7287292287441207; Epoch 16 valid loss: 0.45200159449186106, auc: 0.8541991822096675, aucpr: 0.7534646080125827\n",
      "Best model found! Loss: 0.45200159449186106\n",
      "Epoch 17 train loss: 0.42646133775108014, auc: 0.8610000123300094, aucpr: 0.73257026854915; Epoch 17 valid loss: 0.47552647757647415, auc: 0.8392484388235449, aucpr: 0.7284251556969611\n",
      "Epoch 18 train loss: 0.42778419179665045, auc: 0.8599998691701292, aucpr: 0.7313874689791406; Epoch 18 valid loss: 0.4403454261293028, auc: 0.8608001330234597, aucpr: 0.7650260332820613\n",
      "Best model found! Loss: 0.4403454261293028\n",
      "Epoch 19 train loss: 0.42868469303430473, auc: 0.8589191745294008, aucpr: 0.7315599391413927; Epoch 19 valid loss: 0.43801759725806305, auc: 0.8629081879893137, aucpr: 0.7666632287619026\n",
      "Best model found! Loss: 0.43801759725806305\n",
      "Epoch 20 train loss: 0.42711494862537186, auc: 0.8612583712057573, aucpr: 0.7355824900506966; Epoch 20 valid loss: 0.45904107737853733, auc: 0.8488593362991328, aucpr: 0.739820182701134\n",
      "Epoch 21 train loss: 0.42353859254464443, auc: 0.8633789374261435, aucpr: 0.7388034140912612; Epoch 21 valid loss: 0.4580620421264111, auc: 0.8538678702099934, aucpr: 0.7488788369174935\n",
      "Epoch 22 train loss: 0.4198219791376244, auc: 0.8658848659487965, aucpr: 0.745832473200067; Epoch 22 valid loss: 0.4297411366634434, auc: 0.8691493616352102, aucpr: 0.7768974794742867\n",
      "Best model found! Loss: 0.4297411366634434\n",
      "Epoch 23 train loss: 0.4169997052250876, auc: 0.8683483252129398, aucpr: 0.7499974630230755; Epoch 23 valid loss: 0.44644659745929, auc: 0.8542409449747225, aucpr: 0.7562187849453397\n",
      "Epoch 24 train loss: 0.4243504866987295, auc: 0.8625906808659433, aucpr: 0.738561318719051; Epoch 24 valid loss: 0.4338130144653528, auc: 0.8658731958975251, aucpr: 0.7729806649408552\n",
      "Epoch 25 train loss: 0.4188534615654675, auc: 0.8664370640082789, aucpr: 0.7452609030801896; Epoch 25 valid loss: 0.43006570968508356, auc: 0.8680025329547396, aucpr: 0.77739053927442\n",
      "Epoch 26 train loss: 0.41342002332927996, auc: 0.8710118466918633, aucpr: 0.7562050465904011; Epoch 26 valid loss: 0.4228847871869384, auc: 0.8727840875732236, aucpr: 0.7836459386738976\n",
      "Best model found! Loss: 0.4228847871869384\n",
      "Epoch 27 train loss: 0.4131088347549162, auc: 0.8711164108189714, aucpr: 0.7543432823684353; Epoch 27 valid loss: 0.42759430598230397, auc: 0.8692450271588573, aucpr: 0.7791979048545683\n",
      "Epoch 28 train loss: 0.40976402151962565, auc: 0.8731303441060909, aucpr: 0.7577853207168413; Epoch 28 valid loss: 0.42586166591973007, auc: 0.8709966887202957, aucpr: 0.7802570821555926\n",
      "Epoch 29 train loss: 0.40383795969594205, auc: 0.877894846667862, aucpr: 0.7662279925486625; Epoch 29 valid loss: 0.42826485075143755, auc: 0.8692989892817213, aucpr: 0.7790991980522003\n",
      "Epoch 30 train loss: 0.40384326485024796, auc: 0.8776792365935855, aucpr: 0.7669997262466673; Epoch 30 valid loss: 0.42513232460517564, auc: 0.8719568167666548, aucpr: 0.7764356275198465\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Save best model to ... \u001b[39;00m\n\u001b[1;32m     22\u001b[0m best_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlte_HO_cls_RNN.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtrain_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFor learning_rate = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, hidden_dim = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, num_layer = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_layer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 31\u001b[0m, in \u001b[0;36mtrain_cls\u001b[0;34m(n_epochs, train_dataloader, test_dataloader, best_model_path, early_stopping_patience)\u001b[0m\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out\u001b[38;5;241m.\u001b[39msqueeze(), labels)\n\u001b[1;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 31\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# metrics calculate\u001b[39;00m\n\u001b[1;32m     35\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/anaconda3/envs/wmnlab/lib/python3.9/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/wmnlab/lib/python3.9/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/anaconda3/envs/wmnlab/lib/python3.9/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/wmnlab/lib/python3.9/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/wmnlab/lib/python3.9/site-packages/torch/optim/adam.py:363\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    360\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f_out = 'lte_ho_cls_rnn.csv'\n",
    "f_out = open(f_out, 'w')\n",
    "cols_out = ['lr','hidden_dim','num_layer', 'valid_loss','auc','aucpr', 'p', 'r', 'f1']\n",
    "f_out.write(','.join(cols_out)+'\\n')\n",
    "\n",
    "for lr, hidden_dim, num_layer in itertools.product(lrs, hidden_dims, num_layers):\n",
    "    \n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Model and optimizer\n",
    "    classifier = RNN_Cls(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=lr)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # For record loss\n",
    "    train_losses_for_epochs = []\n",
    "    validation_losses_for_epochs = []\n",
    "    valid_losses_for_epochs = []\n",
    "\n",
    "    # Save best model to ... \n",
    "    best_model_path = os.path.join(save_path, 'lte_HO_cls_RNN.pt')\n",
    "    \n",
    "    train_cls(n_epochs, train_dataloader1, test_dataloader1, best_model_path, early_stopping_patience)\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    print(f'For learning_rate = {lr}, hidden_dim = {hidden_dim}, num_layer = {num_layer}.')\n",
    "    valid_loss, roc_auc, aucpr, p, r, f1 = test(test_dataloader1)\n",
    "    \n",
    "    cols_out = [lr, hidden_dim, num_layer, valid_loss, roc_auc, aucpr, p, r, f1]\n",
    "    cols_out = [str(n) for n in cols_out]\n",
    "    f_out.write(','.join(cols_out)+'\\n')\n",
    "\n",
    "f_out.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_epochs = 600\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "hidden_dim = 128\n",
    "num_layer = 2\n",
    "dropout = 0\n",
    "\n",
    "rnn = 'GRU' # 'LSTM' or 'GRU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 55688\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "forecaster = RNN_Fst(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "optimizer = optim.Adam(forecaster.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fst(n_epochs, train_dataloader, test_dataloader, best_model_path, early_stopping_patience=30):\n",
    "    \n",
    "    def rmse(predictions, targets):\n",
    "        return torch.sqrt(F.mse_loss(predictions, targets))\n",
    "\n",
    "    def mae(predictions, targets):\n",
    "        return torch.mean(torch.abs(predictions - targets))\n",
    "    \n",
    "    # 初始化變數\n",
    "    best_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = early_stopping_patience\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        forecaster.train()\n",
    "\n",
    "        train_losses = []\n",
    "        \n",
    "        trues = torch.tensor([]).to(device)\n",
    "        preds = torch.tensor([]).to(device)\n",
    "\n",
    "        for i, (features, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = forecaster(features)\n",
    "            \n",
    "            trues = torch.cat((trues, labels), axis=0)\n",
    "            preds = torch.cat((preds, out.squeeze().detach()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                    \n",
    "            # metrics calculate\n",
    "      \n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        train_losses_for_epochs.append(train_loss) # Record Loss\n",
    "\n",
    "        rmse_error = rmse(preds, trues)\n",
    "        mae_error = mae(preds, trues)\n",
    "        \n",
    "        print(f'Epoch {epoch} train loss: {train_loss}, rmse: {rmse_error}, mae: {mae_error}', end = '; ')\n",
    "        \n",
    "        # Validate\n",
    "        forecaster.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = torch.tensor([]).to(device)\n",
    "        preds = torch.tensor([]).to(device)\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = forecaster(features)\n",
    "\n",
    "            trues = torch.cat((trues, labels), axis=0)\n",
    "            preds = torch.cat((preds, out.squeeze().detach()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        valid_losses_for_epochs.append(valid_loss) # Record Loss\n",
    "        \n",
    "        rmse_error = rmse(preds, trues)\n",
    "        mae_error = mae(preds, trues)\n",
    "\n",
    "        print(f'Epoch {epoch} valid loss: {valid_loss}, rmse: {rmse_error}, mae: {mae_error}')\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(forecaster.state_dict(), best_model_path)\n",
    "            # best_model.load_state_dict(copy.deepcopy(classifier.state_dict()))\n",
    "            print(f'Best model found! Loss: {valid_loss}')\n",
    "            \n",
    "        else:\n",
    "            # 驗證損失沒有改善，計數器加1\n",
    "            early_stopping_counter += 1\n",
    "            \n",
    "            # 如果計數器達到早期停止的耐心值，則停止訓練\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print('Early stopping triggered.')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For record loss\n",
    "train_losses_for_epochs = []\n",
    "validation_losses_for_epochs = []\n",
    "valid_losses_for_epochs = []\n",
    "\n",
    "# Save best model to ... \n",
    "best_model_path = os.path.join(save_path, 'lte_HO_fst_RNN.pt')\n",
    "\n",
    "early_stopping_patience = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 7.177642323063538, rmse: 2.6792397499084473, mae: 2.269796133041382; Epoch 1 valid loss: 6.251005211654975, rmse: 2.50063419342041, mae: 2.120218515396118\n",
      "Best model found! Loss: 6.251005211654975\n",
      "Epoch 2 train loss: 6.235113834912798, rmse: 2.4970955848693848, mae: 2.0826523303985596; Epoch 2 valid loss: 5.995686071259635, rmse: 2.4488203525543213, mae: 2.076934576034546\n",
      "Best model found! Loss: 5.995686071259635\n",
      "Epoch 3 train loss: 6.041770482485274, rmse: 2.4580719470977783, mae: 2.032453775405884; Epoch 3 valid loss: 5.85856412381542, rmse: 2.420503854751587, mae: 2.0303750038146973\n",
      "Best model found! Loss: 5.85856412381542\n",
      "Epoch 4 train loss: 5.9665968686078505, rmse: 2.4427034854888916, mae: 2.0079736709594727; Epoch 4 valid loss: 5.812350878910142, rmse: 2.410991668701172, mae: 2.02079701423645\n",
      "Best model found! Loss: 5.812350878910142\n",
      "Epoch 5 train loss: 5.937625103503202, rmse: 2.436760425567627, mae: 2.0041685104370117; Epoch 5 valid loss: 5.8309624462711565, rmse: 2.414891004562378, mae: 2.0111961364746094\n",
      "Epoch 6 train loss: 5.887356036110262, rmse: 2.4264676570892334, mae: 1.9918245077133179; Epoch 6 valid loss: 5.845810031404301, rmse: 2.417786121368408, mae: 2.0106773376464844\n",
      "Epoch 7 train loss: 5.898553825901673, rmse: 2.4287467002868652, mae: 1.9954277276992798; Epoch 7 valid loss: 5.766581615623163, rmse: 2.4017906188964844, mae: 2.0020530223846436\n",
      "Best model found! Loss: 5.766581615623163\n",
      "Epoch 8 train loss: 5.9324295141000665, rmse: 2.4356985092163086, mae: 1.9998656511306763; Epoch 8 valid loss: 5.77507543807127, rmse: 2.403625249862671, mae: 1.9861944913864136\n",
      "Epoch 9 train loss: 5.874866164258096, rmse: 2.423844814300537, mae: 1.9886709451675415; Epoch 9 valid loss: 5.992690624022971, rmse: 2.448265790939331, mae: 1.9931846857070923\n",
      "Epoch 10 train loss: 5.843480322635279, rmse: 2.417355537414551, mae: 1.9799675941467285; Epoch 10 valid loss: 5.664578498626242, rmse: 2.380171298980713, mae: 1.9664798974990845\n",
      "Best model found! Loss: 5.664578498626242\n",
      "Epoch 11 train loss: 5.7970495122723875, rmse: 2.4077389240264893, mae: 1.9713393449783325; Epoch 11 valid loss: 5.667027857838844, rmse: 2.3807790279388428, mae: 1.971176266670227\n",
      "Epoch 12 train loss: 5.805677485255013, rmse: 2.409543991088867, mae: 1.97300124168396; Epoch 12 valid loss: 5.68980733715758, rmse: 2.38545560836792, mae: 1.966158390045166\n",
      "Epoch 13 train loss: 5.856381542070777, rmse: 2.4200148582458496, mae: 1.9828343391418457; Epoch 13 valid loss: 5.975192734173366, rmse: 2.445129156112671, mae: 1.9518733024597168\n",
      "Epoch 14 train loss: 5.760627260039338, rmse: 2.40016770362854, mae: 1.9585793018341064; Epoch 14 valid loss: 5.590773156711033, rmse: 2.3650150299072266, mae: 1.9201580286026\n",
      "Best model found! Loss: 5.590773156711033\n",
      "Epoch 15 train loss: 5.742260121666225, rmse: 2.3963472843170166, mae: 1.9578332901000977; Epoch 15 valid loss: 5.633085752020077, rmse: 2.373892307281494, mae: 1.9310554265975952\n",
      "Epoch 16 train loss: 5.71174426796162, rmse: 2.3899481296539307, mae: 1.9477673768997192; Epoch 16 valid loss: 5.620034643581936, rmse: 2.371180534362793, mae: 1.9086997509002686\n",
      "Epoch 17 train loss: 5.686694891895868, rmse: 2.3846960067749023, mae: 1.9424667358398438; Epoch 17 valid loss: 5.58494418981124, rmse: 2.363651752471924, mae: 1.9114372730255127\n",
      "Best model found! Loss: 5.58494418981124\n",
      "Epoch 18 train loss: 5.670823307691422, rmse: 2.381378650665283, mae: 1.9400668144226074; Epoch 18 valid loss: 5.571526987212045, rmse: 2.3609838485717773, mae: 1.8906440734863281\n",
      "Best model found! Loss: 5.571526987212045\n",
      "Epoch 19 train loss: 5.6588205965219345, rmse: 2.3788933753967285, mae: 1.9350745677947998; Epoch 19 valid loss: 5.460497121421659, rmse: 2.337120294570923, mae: 1.907657504081726\n",
      "Best model found! Loss: 5.460497121421659\n",
      "Epoch 20 train loss: 5.6301978066959215, rmse: 2.37280535697937, mae: 1.9300698041915894; Epoch 20 valid loss: 5.914474178333672, rmse: 2.4326558113098145, mae: 1.959164023399353\n",
      "Epoch 21 train loss: 5.596541390587798, rmse: 2.3657236099243164, mae: 1.922722578048706; Epoch 21 valid loss: 5.55241503764172, rmse: 2.3568832874298096, mae: 1.9079723358154297\n",
      "Epoch 22 train loss: 5.580860404314193, rmse: 2.3624284267425537, mae: 1.9192980527877808; Epoch 22 valid loss: 5.513344171095867, rmse: 2.3487319946289062, mae: 1.8828778266906738\n",
      "Epoch 23 train loss: 5.561263294979534, rmse: 2.3582417964935303, mae: 1.913390874862671; Epoch 23 valid loss: 5.615526948656354, rmse: 2.3707289695739746, mae: 1.8971858024597168\n",
      "Epoch 24 train loss: 5.538404432229236, rmse: 2.3534185886383057, mae: 1.9079290628433228; Epoch 24 valid loss: 5.593515381521108, rmse: 2.3661515712738037, mae: 1.8814880847930908\n",
      "Epoch 25 train loss: 5.510878609134033, rmse: 2.347576379776001, mae: 1.9016033411026; Epoch 25 valid loss: 5.579961562643246, rmse: 2.3630831241607666, mae: 1.8891977071762085\n",
      "Epoch 26 train loss: 5.484922290586792, rmse: 2.3420441150665283, mae: 1.8944942951202393; Epoch 26 valid loss: 5.746499683175768, rmse: 2.3983561992645264, mae: 1.885026216506958\n",
      "Epoch 27 train loss: 5.483082630571011, rmse: 2.3416225910186768, mae: 1.8975324630737305; Epoch 27 valid loss: 5.482449830794821, rmse: 2.3425145149230957, mae: 1.8844062089920044\n",
      "Epoch 28 train loss: 5.481111445891119, rmse: 2.3411991596221924, mae: 1.894838809967041; Epoch 28 valid loss: 5.65271731298797, rmse: 2.378593921661377, mae: 1.907228946685791\n",
      "Epoch 29 train loss: 5.449527368397839, rmse: 2.3344626426696777, mae: 1.8894070386886597; Epoch 29 valid loss: 5.515175953203318, rmse: 2.3491404056549072, mae: 1.8824914693832397\n",
      "Epoch 30 train loss: 5.421251563266315, rmse: 2.328406572341919, mae: 1.878890037536621; Epoch 30 valid loss: 5.502031917474707, rmse: 2.346547842025757, mae: 1.8799325227737427\n",
      "Epoch 31 train loss: 5.429663828094449, rmse: 2.330198049545288, mae: 1.884876012802124; Epoch 31 valid loss: 5.638532099675159, rmse: 2.375387668609619, mae: 1.9180842638015747\n",
      "Epoch 32 train loss: 5.424349422053953, rmse: 2.328996181488037, mae: 1.8815743923187256; Epoch 32 valid loss: 5.99789745710334, rmse: 2.4503560066223145, mae: 1.9165018796920776\n",
      "Epoch 33 train loss: 5.3676573500169065, rmse: 2.3168282508850098, mae: 1.8662205934524536; Epoch 33 valid loss: 5.8015183575299325, rmse: 2.409656047821045, mae: 1.8784410953521729\n",
      "Epoch 34 train loss: 5.289277325689265, rmse: 2.299872636795044, mae: 1.8463712930679321; Epoch 34 valid loss: 5.623788806856895, rmse: 2.372603416442871, mae: 1.8575267791748047\n",
      "Epoch 35 train loss: 5.265836216496155, rmse: 2.2947723865509033, mae: 1.8428659439086914; Epoch 35 valid loss: 5.6792912288587925, rmse: 2.3843138217926025, mae: 1.872987985610962\n",
      "Epoch 36 train loss: 5.25732670357797, rmse: 2.292907476425171, mae: 1.84025239944458; Epoch 36 valid loss: 5.418429703128581, rmse: 2.328784942626953, mae: 1.8362172842025757\n",
      "Best model found! Loss: 5.418429703128581\n",
      "Epoch 37 train loss: 5.210197897505971, rmse: 2.28263258934021, mae: 1.8289299011230469; Epoch 37 valid loss: 5.312865301054352, rmse: 2.3061115741729736, mae: 1.8278865814208984\n",
      "Best model found! Loss: 5.312865301054352\n",
      "Epoch 38 train loss: 5.177985753738775, rmse: 2.275559425354004, mae: 1.8223319053649902; Epoch 38 valid loss: 5.683113253846461, rmse: 2.3850560188293457, mae: 1.8547734022140503\n",
      "Epoch 39 train loss: 5.109627983844386, rmse: 2.2605042457580566, mae: 1.8054147958755493; Epoch 39 valid loss: 5.498236787562468, rmse: 2.345797538757324, mae: 1.8457087278366089\n",
      "Epoch 40 train loss: 5.125164150453247, rmse: 2.2639107704162598, mae: 1.8063150644302368; Epoch 40 valid loss: 5.228084736940812, rmse: 2.287256956100464, mae: 1.8154584169387817\n",
      "Best model found! Loss: 5.228084736940812\n",
      "Epoch 41 train loss: 5.0981970060188155, rmse: 2.2579174041748047, mae: 1.8000953197479248; Epoch 41 valid loss: 5.390680850768576, rmse: 2.3228485584259033, mae: 1.8293812274932861\n",
      "Epoch 42 train loss: 5.083081555577506, rmse: 2.2546627521514893, mae: 1.797478199005127; Epoch 42 valid loss: 5.578088234881966, rmse: 2.3630120754241943, mae: 1.8314340114593506\n",
      "Epoch 43 train loss: 5.020550851273326, rmse: 2.240741014480591, mae: 1.7834498882293701; Epoch 43 valid loss: 5.542303800582886, rmse: 2.3553338050842285, mae: 1.8473920822143555\n",
      "Epoch 44 train loss: 5.054173450132387, rmse: 2.2481884956359863, mae: 1.7898238897323608; Epoch 44 valid loss: 5.19545146518824, rmse: 2.280529737472534, mae: 1.8086814880371094\n",
      "Best model found! Loss: 5.19545146518824\n",
      "Epoch 45 train loss: 4.966719508382072, rmse: 2.228644847869873, mae: 1.769689917564392; Epoch 45 valid loss: 5.202413425153615, rmse: 2.2821104526519775, mae: 1.8151510953903198\n",
      "Epoch 46 train loss: 4.952397542189708, rmse: 2.225435256958008, mae: 1.7653393745422363; Epoch 46 valid loss: 5.170142305140593, rmse: 2.2744812965393066, mae: 1.780599594116211\n",
      "Best model found! Loss: 5.170142305140593\n",
      "Epoch 47 train loss: 4.903471782988151, rmse: 2.214416742324829, mae: 1.7546255588531494; Epoch 47 valid loss: 5.41146644889092, rmse: 2.3273091316223145, mae: 1.8011904954910278\n",
      "Epoch 48 train loss: 4.886662028743102, rmse: 2.210602283477783, mae: 1.7523702383041382; Epoch 48 valid loss: 5.595371294994743, rmse: 2.3667306900024414, mae: 1.8284580707550049\n",
      "Epoch 49 train loss: 4.895012055878091, rmse: 2.212468385696411, mae: 1.7545844316482544; Epoch 49 valid loss: 5.424379409575949, rmse: 2.3303048610687256, mae: 1.8046643733978271\n",
      "Epoch 50 train loss: 4.84371538985092, rmse: 2.200895071029663, mae: 1.7421599626541138; Epoch 50 valid loss: 5.572958960825083, rmse: 2.362081527709961, mae: 1.811216115951538\n",
      "Epoch 51 train loss: 4.787177119212868, rmse: 2.1880390644073486, mae: 1.727722406387329; Epoch 51 valid loss: 5.560657661788317, rmse: 2.359513521194458, mae: 1.8009828329086304\n",
      "Epoch 52 train loss: 4.82674859445707, rmse: 2.1970019340515137, mae: 1.738289475440979; Epoch 52 valid loss: 5.320175195226864, rmse: 2.307589530944824, mae: 1.785744071006775\n",
      "Epoch 53 train loss: 4.748758643865585, rmse: 2.1791255474090576, mae: 1.7194839715957642; Epoch 53 valid loss: 5.840684793433365, rmse: 2.4183309078216553, mae: 1.8595861196517944\n",
      "Epoch 54 train loss: 4.799142417971011, rmse: 2.190713882446289, mae: 1.7307713031768799; Epoch 54 valid loss: 6.024377479845164, rmse: 2.4560511112213135, mae: 1.8903425931930542\n",
      "Epoch 55 train loss: 4.843175121324252, rmse: 2.200732707977295, mae: 1.7419989109039307; Epoch 55 valid loss: 5.875269262158141, rmse: 2.425187587738037, mae: 1.8658461570739746\n",
      "Epoch 56 train loss: 4.854643914446367, rmse: 2.2033209800720215, mae: 1.7438997030258179; Epoch 56 valid loss: 5.883396657145753, rmse: 2.4272756576538086, mae: 1.8612531423568726\n",
      "Epoch 57 train loss: 4.795981052386022, rmse: 2.189969778060913, mae: 1.7298378944396973; Epoch 57 valid loss: 5.370525110741051, rmse: 2.3183753490448, mae: 1.7871038913726807\n",
      "Epoch 58 train loss: 4.738394799063691, rmse: 2.176791191101074, mae: 1.7187223434448242; Epoch 58 valid loss: 6.109771048536106, rmse: 2.4735453128814697, mae: 1.8967492580413818\n",
      "Epoch 59 train loss: 4.758443841206289, rmse: 2.1814236640930176, mae: 1.720717191696167; Epoch 59 valid loss: 5.700496077537537, rmse: 2.3889877796173096, mae: 1.8346434831619263\n",
      "Epoch 60 train loss: 4.719822158855674, rmse: 2.1725027561187744, mae: 1.7139720916748047; Epoch 60 valid loss: 6.190152231527835, rmse: 2.4898900985717773, mae: 1.914730191230774\n",
      "Epoch 61 train loss: 4.729774057548658, rmse: 2.1748130321502686, mae: 1.7125020027160645; Epoch 61 valid loss: 5.341087185606664, rmse: 2.312469720840454, mae: 1.7782906293869019\n",
      "Epoch 62 train loss: 4.703425980563712, rmse: 2.1686923503875732, mae: 1.708516240119934; Epoch 62 valid loss: 5.315766344265062, rmse: 2.306617021560669, mae: 1.7929058074951172\n",
      "Epoch 63 train loss: 4.6863223129669125, rmse: 2.1648166179656982, mae: 1.7051656246185303; Epoch 63 valid loss: 5.3286630912702915, rmse: 2.30997896194458, mae: 1.7661349773406982\n",
      "Epoch 64 train loss: 4.62329076866133, rmse: 2.15018367767334, mae: 1.6889967918395996; Epoch 64 valid loss: 5.820789342023889, rmse: 2.4136159420013428, mae: 1.8549834489822388\n",
      "Epoch 65 train loss: 4.658817033008137, rmse: 2.1584115028381348, mae: 1.7020142078399658; Epoch 65 valid loss: 5.4261431900822386, rmse: 2.330064296722412, mae: 1.8006460666656494\n",
      "Epoch 66 train loss: 4.6256643860741, rmse: 2.150703191757202, mae: 1.6915696859359741; Epoch 66 valid loss: 5.488996651707863, rmse: 2.3444178104400635, mae: 1.809834361076355\n",
      "Epoch 67 train loss: 4.583178968767149, rmse: 2.1408638954162598, mae: 1.6790310144424438; Epoch 67 valid loss: 5.44808554649353, rmse: 2.3358097076416016, mae: 1.7906639575958252\n",
      "Epoch 68 train loss: 4.586642483061394, rmse: 2.141629934310913, mae: 1.6787320375442505; Epoch 68 valid loss: 5.622646513033886, rmse: 2.3729445934295654, mae: 1.809532642364502\n",
      "Epoch 69 train loss: 4.525526977666711, rmse: 2.127296209335327, mae: 1.6671464443206787; Epoch 69 valid loss: 5.461719028803767, rmse: 2.3384461402893066, mae: 1.7931097745895386\n",
      "Epoch 70 train loss: 4.535345847311273, rmse: 2.129638433456421, mae: 1.6688367128372192; Epoch 70 valid loss: 5.5758195215342, rmse: 2.3628647327423096, mae: 1.8424986600875854\n",
      "Epoch 71 train loss: 4.544358146929108, rmse: 2.1317691802978516, mae: 1.6704014539718628; Epoch 71 valid loss: 5.171866268527751, rmse: 2.275360584259033, mae: 1.7479907274246216\n",
      "Epoch 72 train loss: 4.530630625085493, rmse: 2.1285276412963867, mae: 1.6664602756500244; Epoch 72 valid loss: 6.371578666628624, rmse: 2.5254244804382324, mae: 1.9003890752792358\n",
      "Epoch 73 train loss: 4.507507162600492, rmse: 2.123063325881958, mae: 1.6602925062179565; Epoch 73 valid loss: 5.701133119816682, rmse: 2.389261245727539, mae: 1.809790015220642\n",
      "Epoch 74 train loss: 4.487130797278565, rmse: 2.1183202266693115, mae: 1.6567038297653198; Epoch 74 valid loss: 5.874895696737329, rmse: 2.425145149230957, mae: 1.8364429473876953\n",
      "Epoch 75 train loss: 4.461726366418652, rmse: 2.11226224899292, mae: 1.6518640518188477; Epoch 75 valid loss: 5.527857678277152, rmse: 2.3524153232574463, mae: 1.7860684394836426\n",
      "Epoch 76 train loss: 4.4035884264296135, rmse: 2.0984373092651367, mae: 1.6390111446380615; Epoch 76 valid loss: 5.745335758948813, rmse: 2.3979716300964355, mae: 1.8432062864303589\n",
      "Epoch 77 train loss: 4.393493412975716, rmse: 2.096021890640259, mae: 1.6381185054779053; Epoch 77 valid loss: 5.489066907337734, rmse: 2.3444440364837646, mae: 1.8019044399261475\n",
      "Epoch 78 train loss: 4.345482028796609, rmse: 2.084561347961426, mae: 1.626481056213379; Epoch 78 valid loss: 5.636919067830456, rmse: 2.3753209114074707, mae: 1.7986750602722168\n",
      "Epoch 79 train loss: 4.405497899572406, rmse: 2.098776340484619, mae: 1.6359965801239014; Epoch 79 valid loss: 5.909225529553939, rmse: 2.4314329624176025, mae: 1.8385330438613892\n",
      "Epoch 80 train loss: 4.372677084777208, rmse: 2.091036558151245, mae: 1.633982539176941; Epoch 80 valid loss: 5.807745639158755, rmse: 2.410832166671753, mae: 1.8364604711532593\n",
      "Epoch 81 train loss: 4.302538176184207, rmse: 2.074258804321289, mae: 1.612940788269043; Epoch 81 valid loss: 5.307889133083577, rmse: 2.3039591312408447, mae: 1.7637709379196167\n",
      "Epoch 82 train loss: 4.280000912088209, rmse: 2.0688095092773438, mae: 1.6069577932357788; Epoch 82 valid loss: 5.512008204752085, rmse: 2.3480968475341797, mae: 1.7767210006713867\n",
      "Epoch 83 train loss: 4.324067765368825, rmse: 2.0794546604156494, mae: 1.6205167770385742; Epoch 83 valid loss: 5.910947914026221, rmse: 2.431873083114624, mae: 1.8387941122055054\n",
      "Epoch 84 train loss: 4.278930283436733, rmse: 2.0686028003692627, mae: 1.6088595390319824; Epoch 84 valid loss: 5.984011978519206, rmse: 2.4471023082733154, mae: 1.8438255786895752\n",
      "Epoch 85 train loss: 4.278016454297885, rmse: 2.0682268142700195, mae: 1.6057199239730835; Epoch 85 valid loss: 5.492267202357857, rmse: 2.3444831371307373, mae: 1.7825788259506226\n",
      "Epoch 86 train loss: 4.274107131599325, rmse: 2.0673725605010986, mae: 1.6078263521194458; Epoch 86 valid loss: 5.732951434291139, rmse: 2.395064115524292, mae: 1.8248600959777832\n",
      "Epoch 87 train loss: 4.251160230415057, rmse: 2.061833143234253, mae: 1.6032555103302002; Epoch 87 valid loss: 5.215773954683421, rmse: 2.2841174602508545, mae: 1.7666865587234497\n",
      "Epoch 88 train loss: 4.22855325103861, rmse: 2.056421995162964, mae: 1.5991910696029663; Epoch 88 valid loss: 5.608022018354767, rmse: 2.368166446685791, mae: 1.8032276630401611\n",
      "Epoch 89 train loss: 4.237895296790959, rmse: 2.0586659908294678, mae: 1.5976883172988892; Epoch 89 valid loss: 5.536567885048536, rmse: 2.353308916091919, mae: 1.7925705909729004\n",
      "Epoch 90 train loss: 4.200881783360929, rmse: 2.0496840476989746, mae: 1.5922410488128662; Epoch 90 valid loss: 5.478992634890031, rmse: 2.340999126434326, mae: 1.781551718711853\n",
      "Epoch 91 train loss: 4.168629638283654, rmse: 2.0417816638946533, mae: 1.586014747619629; Epoch 91 valid loss: 5.6290631123951504, rmse: 2.3724684715270996, mae: 1.7928239107131958\n",
      "Epoch 92 train loss: 4.149226367157118, rmse: 2.0370466709136963, mae: 1.5789270401000977; Epoch 92 valid loss: 5.655807718938711, rmse: 2.37823748588562, mae: 1.8089909553527832\n",
      "Epoch 93 train loss: 4.201266027024362, rmse: 2.049736261367798, mae: 1.5902262926101685; Epoch 93 valid loss: 5.852409070851851, rmse: 2.4205167293548584, mae: 1.83092200756073\n",
      "Epoch 94 train loss: 4.118979973603139, rmse: 2.029628038406372, mae: 1.5693645477294922; Epoch 94 valid loss: 5.5283123425074985, rmse: 2.35132098197937, mae: 1.7969025373458862\n",
      "Epoch 95 train loss: 4.166424349345992, rmse: 2.0412516593933105, mae: 1.5833009481430054; Epoch 95 valid loss: 5.347978416754275, rmse: 2.3131766319274902, mae: 1.796465277671814\n",
      "Epoch 96 train loss: 4.10129744679527, rmse: 2.0251364707946777, mae: 1.5689313411712646; Epoch 96 valid loss: 5.430064723199727, rmse: 2.3303372859954834, mae: 1.7973684072494507\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "train_fst(n_epochs, train_dataloader2, test_dataloader2, best_model_path, early_stopping_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 5.170142305140593, rmse 2.2744812965393066, mae 1.780599594116211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.170142305140593, 2.2744812965393066, 1.780599594116211)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "def rmse(predictions, targets):\n",
    "    return torch.sqrt(F.mse_loss(predictions, targets))\n",
    "\n",
    "def mae(predictions, targets):\n",
    "    return torch.mean(torch.abs(predictions - targets))\n",
    "\n",
    "def test2(test_dataloader):\n",
    "    best_model = RNN_Fst(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "    best_model.load_state_dict(torch.load(best_model_path))\n",
    "    best_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        best_model.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = torch.tensor([]).to(device)\n",
    "        preds = torch.tensor([]).to(device)\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = best_model(features)\n",
    "\n",
    "            trues = torch.cat((trues, labels), axis=0)\n",
    "            preds = torch.cat((preds, out.squeeze().detach()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        rmse_error = rmse(preds, trues)\n",
    "        mae_error = mae(preds, trues)\n",
    "\n",
    "        print(f'valid loss {valid_loss}, rmse {rmse_error}, mae {mae_error}')\n",
    "        \n",
    "        return valid_loss, rmse_error.item(), mae_error.item()\n",
    "        \n",
    "test2(test_dataloader2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "save_path = \"../model\"\n",
    "best_model_path = os.path.join(save_path, 'lte_HO_cls_RNN.pt')\n",
    "torch.save(classifier.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# m_path = os.path.join('/home/wmnlab/Documents/sheng-ru/model', 'lte_HO_cls_RNN.pt')\n",
    "# classifier = RNN(input_dim, out_dim, hidden_dim, num_layers, dropout, rnn)\n",
    "# classifier.load_state_dict(torch.load(m_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheng-ru",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7771dd1fbefba0f9e49b3f12d6cb05ea3fc9d8cb4bbb591d0ecb9d07210ade7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
