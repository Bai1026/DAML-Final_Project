{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "import random\n",
    "import copy\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 25 02:31:43 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 Ti      On | 00000000:65:00.0 Off |                  N/A |\n",
      "|  0%   37C    P8               11W / 285W|     68MiB / 12282MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4070 Ti      On | 00000000:B3:00.0 Off |                  N/A |\n",
      "|  0%   39C    P8                9W / 285W|      6MiB / 12282MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1236      G   /usr/lib/xorg/Xorg                           56MiB |\n",
      "|    0   N/A  N/A      1400      G   /usr/bin/gnome-shell                          9MiB |\n",
      "|    1   N/A  N/A      1236      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_array_create(dirname, dir_list, time_seq):\n",
    "    \n",
    "    columns = ['RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2',\n",
    "               'nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1', 'nr-RSRP2', 'nr-RSRQ2']\n",
    "    \n",
    "    def reamin_HO_time(y_train):\n",
    "        def f(L):    \n",
    "            for i, e in enumerate(L):\n",
    "                if e: return i+1\n",
    "            return 0\n",
    "\n",
    "        out = []\n",
    "        for a2 in y_train:\n",
    "            a1_out = []\n",
    "            for a1 in a2:\n",
    "                a1_out.append(a1.any())\n",
    "      \n",
    "            out.append(f(a1_out))\n",
    "        return out\n",
    "    \n",
    "    def HO(y_train):\n",
    "        out = []\n",
    "        for a2 in y_train:\n",
    "            if sum(a2.reshape(-1)) == 0: ho = 0\n",
    "            elif sum(a2.reshape(-1)) > 0: ho = 1\n",
    "            out.append(ho)\n",
    "        return out\n",
    "\n",
    "    split_time = []\n",
    "    for i, f in enumerate(tqdm(dir_list)):\n",
    "    \n",
    "        f = os.path.join(dirname, f)\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "        # preprocess data with ffill method\n",
    "        del df['Timestamp'], df['lat'], df['long'], df['gpsspeed']\n",
    "        # df[columns] = df[columns].replace(0, np.nan)\n",
    "        # df[columns] = df[columns].fillna(method='ffill')\n",
    "        # df.dropna(inplace=True)\n",
    "        \n",
    "        df.replace(np.nan,0,inplace=True); df.replace('-',0,inplace=True)\n",
    "        \n",
    "        X = df[features]\n",
    "        Y = df[target]\n",
    "\n",
    "        Xt_list = []\n",
    "        Yt_list = []\n",
    "\n",
    "        for j in range(time_seq):\n",
    "            X_t = X.shift(periods=-j)\n",
    "            Xt_list.append(X_t)\n",
    "    \n",
    "        for j in range(time_seq,time_seq+predict_t):\n",
    "            Y_t = Y.shift(periods=-(j))\n",
    "            Yt_list.append(Y_t)\n",
    "\n",
    "        # YY = Y.shift(periods=-(0))\n",
    "\n",
    "        X_ts = np.array(Xt_list); X_ts = np.transpose(X_ts, (1,0,2)); X_ts = X_ts[:-(time_seq+predict_t-1),:,:]\n",
    "        Y_ts = np.array(Yt_list); Y_ts = np.transpose(Y_ts, (1,0,2)); Y_ts = Y_ts[:-(time_seq+predict_t-1),:,:]\n",
    "        split_time.append(len(X_ts))\n",
    "\n",
    "        if i == 0:\n",
    "            X_final = X_ts\n",
    "            Y_final = Y_ts\n",
    "        else:\n",
    "            X_final = np.concatenate((X_final,X_ts), axis=0)\n",
    "            Y_final = np.concatenate((Y_final,Y_ts), axis=0)\n",
    "\n",
    "    split_time = [(sum(split_time[:i]), sum(split_time[:i])+x) for i, x in enumerate(split_time)]\n",
    "    \n",
    "    return X_final, np.array(HO(Y_final)), np.array(reamin_HO_time(Y_final)), split_time # forecast HO\n",
    "\n",
    "class RNN_Dataset_simple(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset take all csv file specified in dir_list in directory dirname.\n",
    "    Transfer csvs to (features, label) pair\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "\n",
    "        self.inputs = torch.FloatTensor(X)\n",
    "        self.labels = torch.FloatTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data = self.inputs[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_in_file(file, dates):\n",
    "    \n",
    "    for date in dates:\n",
    "        if date in file: return True \n",
    "    return False\n",
    "\n",
    "def train_valid_split(L, valid_size=0.2):\n",
    "    \n",
    "    length = len(L)\n",
    "    v_num = int(length*valid_size)\n",
    "    v_files = random.sample(L, v_num)\n",
    "    t_files = list(set(L) - set(v_files))\n",
    "    \n",
    "    return t_files, v_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:2\"\n",
    "    \n",
    "    print(f\"Random seed set as {seed}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time sequence length and prediction time length\n",
    "seed = 55688\n",
    "time_seq = 20\n",
    "predict_t = 10\n",
    "valid_ratio = 0.2\n",
    "task = 'classification'\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 55688\n",
      "GPU 0: NVIDIA GeForce RTX 4070 Ti\n",
      "GPU 1: NVIDIA GeForce RTX 4070 Ti\n",
      "Loading training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c80a0817ba4325a65d16c22c1565c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading training data...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     51\u001b[0m X_train, y_train1, y_train2, split_time_train \u001b[38;5;241m=\u001b[39m ts_array_create(dirname, train_dir_list, time_seq)\n\u001b[0;32m---> 53\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mRNN_Dataset_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m train_dataloader1 \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     56\u001b[0m cond \u001b[38;5;241m=\u001b[39m y_train2 \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 82\u001b[0m, in \u001b[0;36mRNN_Dataset_simple.__init__\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(y)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "# Setup seed\n",
    "set_seed(seed)\n",
    "\n",
    "# Get GPU\n",
    "device_count = torch.cuda.device_count()\n",
    "num_of_gpus = device_count\n",
    "\n",
    "for i in range(device_count):\n",
    "    print(\"GPU {}: {}\".format(i, torch.cuda.get_device_name(i)))\n",
    "    gpu_id = i\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Save best model to \n",
    "save_path = \"../model\"\n",
    "\n",
    "# Define DataSet\n",
    "dirname = \"../data/single\"\n",
    "dir_list = os.listdir(dirname)\n",
    "dir_list = [f for f in dir_list if ( f.endswith('.csv') and (not 'sm' in f) ) ]\n",
    "\n",
    "train_dates = ['03-26', '04-01']\n",
    "test_dates = ['04-10']\n",
    "    \n",
    "# train_dir_list = [f for f in dir_list if ( f.endswith('.csv') and ('All' in f) and days_in_file(f, train_dates) )]\n",
    "# test_dir_list = [f for f in dir_list if ( f.endswith('.csv') and ('All' in f) and days_in_file(f, test_dates) )]\n",
    "\n",
    "train_dir_list, test_dir_list = train_valid_split(dir_list, valid_ratio)\n",
    "train_dir_list += [f for f in os.listdir(dirname) if 'sm' in f]\n",
    "\n",
    "# features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n",
    "#         'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2',\n",
    "#         'nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1', 'nr-RSRP2', 'nr-RSRQ2' ]\n",
    "features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n",
    "        'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1','nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1']\n",
    "# features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n",
    "#         'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2']\n",
    "\n",
    "num_of_features = len(features)\n",
    "\n",
    "# target = ['LTE_HO', 'MN_HO'] # For eNB HO.\n",
    "# target = ['eNB_to_ENDC'] # Setup gNB\n",
    "target = ['gNB_Rel', 'gNB_HO'] # For gNB HO.\n",
    "# target = ['RLF'] # For RLF\n",
    "# target = ['SCG_RLF'] # For scg failure\n",
    "# target = ['dl-loss'] # For DL loss\n",
    "# target = ['ul-loss'] # For UL loss\n",
    "\n",
    "# Data\n",
    "print('Loading training data...')\n",
    "X_train, y_train1, y_train2, split_time_train = ts_array_create(dirname, train_dir_list, time_seq)\n",
    "\n",
    "train_dataset = RNN_Dataset_simple(X_train, y_train1)\n",
    "train_dataloader1 = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "cond = y_train2 > 0\n",
    "X_train_fore = X_train[cond]\n",
    "y_train2_fore = y_train2[cond]\n",
    "train_dataset = RNN_Dataset_simple(X_train_fore, y_train2_fore)\n",
    "train_dataloader2 = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('Loading testing data...')\n",
    "X_test, y_test1, y_test2, split_time_test = ts_array_create(dirname, test_dir_list, time_seq)\n",
    "\n",
    "test_dataset = RNN_Dataset_simple(X_test, y_test1)\n",
    "test_dataloader1 = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "cond = y_test2 > 0\n",
    "X_test_fore = X_test[cond]\n",
    "y_test2_fore = y_test2[cond]\n",
    "test_dataset = RNN_Dataset_simple(X_test_fore, y_test2_fore)\n",
    "test_dataloader2 = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = next(iter(train_dataloader1))\n",
    "input_dim, out_dim = a.shape[2], 1\n",
    "a.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Cls(nn.Module):\n",
    "    '''\n",
    "    Using LSTM or GRU.\n",
    "    '''\n",
    "    def __init__(self, input_dim, out_dim, hidden_dim, num_layer, dropout, rnn):\n",
    "\n",
    "        super().__init__()\n",
    "        self.in_dim = input_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dim = hidden_dim\n",
    "        self.num_layer = num_layer\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # input_size: num of features; hidden_size: num of hidden state h\n",
    "        # num_layers: number of recurrent layer; seq; batch_first: batch first than seq\n",
    "        if rnn == 'LSTM':\n",
    "            self.rnn= nn.LSTM(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "        elif rnn == 'GRU':\n",
    "            self.rnn= nn.GRU(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_dim, out_dim) # For binary classification\n",
    "\n",
    "    def forward(self,batch_input):\n",
    "\n",
    "        out,_ = self.rnn(batch_input)\n",
    "        out = self.linear(out[:,-1, :])  #Extract out of last time step (N, L, Hout) -> (Batch, time_seq, output)\n",
    "        \n",
    "        out = torch.sigmoid(out) # Binary Classifier\n",
    "\n",
    "        return out\n",
    "\n",
    "class RNN_Fst(nn.Module):\n",
    "    '''\n",
    "    Using LSTM or GRU.\n",
    "    '''\n",
    "    def __init__(self, input_dim, out_dim, hidden_dim, num_layer, dropout, rnn):\n",
    "\n",
    "        super().__init__()\n",
    "        self.in_dim = input_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dim = hidden_dim\n",
    "        self.num_layer = num_layer\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # input_size: num of features; hidden_size: num of hidden state h\n",
    "        # num_layers: number of recurrent layer; seq; batch_first: batch first than seq\n",
    "        if rnn == 'LSTM':\n",
    "            self.rnn= nn.LSTM(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "        elif rnn == 'GRU':\n",
    "            self.rnn= nn.GRU(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_dim, out_dim) # For binary classification\n",
    "\n",
    "    def forward(self,batch_input):\n",
    "\n",
    "        out,_ = self.rnn(batch_input)\n",
    "        out = self.linear(out[:,-1, :])  #Extract out of last time step (N, L, Hout) -> (Batch, time_seq, output)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_epochs = 600\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "hidden_dim = 128\n",
    "num_layer = 2\n",
    "dropout = 0\n",
    "\n",
    "rnn = 'GRU' # 'LSTM' or 'GRU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "# Define model and optimizer\n",
    "\n",
    "classifier = RNN_Cls(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[600, 1000], gamma=0.4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cls(n_epochs, train_dataloader, test_dataloader, best_model_path, early_stopping_patience=30):\n",
    "    \n",
    "    # 初始化變數\n",
    "    best_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = early_stopping_patience\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        classifier.train()\n",
    "\n",
    "        train_losses = []\n",
    "        \n",
    "        trues = np.array([])\n",
    "        preds = np.array([])\n",
    "\n",
    "        for i, (features, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = classifier(features)\n",
    "            \n",
    "            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n",
    "            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                    \n",
    "            # metrics calculate\n",
    "      \n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(trues, preds)\n",
    "        aucpr = auc(recall, precision)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(trues, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        train_losses_for_epochs.append(train_loss) # Record Loss\n",
    "\n",
    "        print(f'Epoch {epoch} train loss: {train_loss}, auc: {roc_auc}, aucpr: {aucpr}', end = '; ')\n",
    "        \n",
    "        # Validate\n",
    "        classifier.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = np.array([])\n",
    "        preds = np.array([])\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = classifier(features)\n",
    "\n",
    "            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n",
    "            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(trues, preds)\n",
    "        aucpr = auc(recall, precision)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(trues, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        valid_losses_for_epochs.append(valid_loss) # Record Loss\n",
    "        \n",
    "        print(f'Epoch {epoch} valid loss: {valid_loss}, auc: {roc_auc}, aucpr: {aucpr}')\n",
    "        \n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(classifier.state_dict(), best_model_path)\n",
    "            # best_model.load_state_dict(copy.deepcopy(classifier.state_dict()))\n",
    "            print(f'Best model found! Loss: {valid_loss}')\n",
    "            \n",
    "        else:\n",
    "            # 驗證損失沒有改善，計數器加1\n",
    "            early_stopping_counter += 1\n",
    "            \n",
    "            # 如果計數器達到早期停止的耐心值，則停止訓練\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print('Early stopping triggered.')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For record loss\n",
    "train_losses_for_epochs = []\n",
    "validation_losses_for_epochs = []\n",
    "valid_losses_for_epochs = []\n",
    "\n",
    "# Save best model to ... \n",
    "best_model_path = os.path.join(save_path, 'lte_HO_cls_RNN.pt')\n",
    "\n",
    "early_stopping_patience = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visulized on many sample on validation data\n",
    "# sample_value = 2\n",
    "# # samples = random.sample(split_time_test, sample_value)\n",
    "# samples = [split_time_test[8], split_time_test[9]]\n",
    "\n",
    "# fig, axs = plt.subplots(1, sample_value, figsize=(14, 2.5))\n",
    "\n",
    "# # y_test\n",
    "# # preds\n",
    "\n",
    "# for i in range(sample_value):\n",
    "#     true = [y_test1[i] for i in range(samples[i][0], samples[i][1])]\n",
    "#     axs[i].plot(true, label='true')\n",
    "#     prediction = [preds[i] for i in range(samples[i][0], samples[i][1])]\n",
    "#     # prediction = [1 if preds[i] > 0.5 else 0  for i in range(samples[i][0], samples[i][1])]\n",
    "#     axs[i].plot(prediction, label='pred')\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cls(n_epochs, train_dataloader1, test_dataloader1, best_model_path, early_stopping_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "def test(test_dataloader):\n",
    "    best_model = RNN_Cls(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "    best_model.load_state_dict(torch.load(best_model_path))\n",
    "    best_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        best_model.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = np.array([])\n",
    "        preds = np.array([])\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = best_model(features)\n",
    "\n",
    "            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n",
    "            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(trues, preds)\n",
    "        aucpr = auc(recall, precision)\n",
    "        threshold = 0.5\n",
    "        p = precision_score(trues, [1 if pred > threshold else 0 for pred in preds])\n",
    "        r = recall_score(trues, [1 if pred > threshold else 0 for pred in preds])\n",
    "        f1 = f1_score(trues, [1 if pred > threshold else 0 for pred in preds])\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(trues, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "\n",
    "        print(f'valid loss {valid_loss}, roc_auc {roc_auc}, aucpr {aucpr}')\n",
    "        \n",
    "        return valid_loss, roc_auc, aucpr, p, r, f1\n",
    "        \n",
    "test(test_dataloader1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import itertools\n",
    "\n",
    "n_epochs = 600\n",
    "lrs = [0.001, 0.01, 0.1]\n",
    "hidden_dims = [32, 64, 128]\n",
    "num_layers = [1, 2]\n",
    "dropout = 0\n",
    "\n",
    "early_stopping_patience = 50\n",
    "rnn = 'GRU'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_out = 'lte_ho_cls_rnn.csv'\n",
    "f_out = open(f_out, 'w')\n",
    "cols_out = ['lr','hidden_dim','num_layer', 'valid_loss','auc','aucpr', 'p', 'r', 'f1']\n",
    "f_out.write(','.join(cols_out)+'\\n')\n",
    "\n",
    "for lr, hidden_dim, num_layer in itertools.product(lrs, hidden_dims, num_layers):\n",
    "    \n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Model and optimizer\n",
    "    classifier = RNN_Cls(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=lr)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # For record loss\n",
    "    train_losses_for_epochs = []\n",
    "    validation_losses_for_epochs = []\n",
    "    valid_losses_for_epochs = []\n",
    "\n",
    "    # Save best model to ... \n",
    "    best_model_path = os.path.join(save_path, 'lte_HO_cls_RNN.pt')\n",
    "    \n",
    "    train_cls(n_epochs, train_dataloader1, test_dataloader1, best_model_path, early_stopping_patience)\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    print(f'For learning_rate = {lr}, hidden_dim = {hidden_dim}, num_layer = {num_layer}.')\n",
    "    valid_loss, roc_auc, aucpr, p, r, f1 = test(test_dataloader1)\n",
    "    \n",
    "    cols_out = [lr, hidden_dim, num_layer, valid_loss, roc_auc, aucpr, p, r, f1]\n",
    "    cols_out = [str(n) for n in cols_out]\n",
    "    f_out.write(','.join(cols_out)+'\\n')\n",
    "\n",
    "f_out.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_epochs = 600\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "hidden_dim = 128\n",
    "num_layer = 2\n",
    "dropout = 0\n",
    "\n",
    "rnn = 'GRU' # 'LSTM' or 'GRU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "forecaster = RNN_Fst(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "optimizer = optim.Adam(forecaster.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fst(n_epochs, train_dataloader, test_dataloader, best_model_path, early_stopping_patience=30):\n",
    "    \n",
    "    def rmse(predictions, targets):\n",
    "        return torch.sqrt(F.mse_loss(predictions, targets))\n",
    "\n",
    "    def mae(predictions, targets):\n",
    "        return torch.mean(torch.abs(predictions - targets))\n",
    "    \n",
    "    # 初始化變數\n",
    "    best_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = early_stopping_patience\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        forecaster.train()\n",
    "\n",
    "        train_losses = []\n",
    "        \n",
    "        trues = torch.tensor([]).to(device)\n",
    "        preds = torch.tensor([]).to(device)\n",
    "\n",
    "        for i, (features, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = forecaster(features)\n",
    "            \n",
    "            trues = torch.cat((trues, labels), axis=0)\n",
    "            preds = torch.cat((preds, out.squeeze().detach()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                    \n",
    "            # metrics calculate\n",
    "      \n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        train_losses_for_epochs.append(train_loss) # Record Loss\n",
    "\n",
    "        rmse_error = rmse(preds, trues)\n",
    "        mae_error = mae(preds, trues)\n",
    "        \n",
    "        print(f'Epoch {epoch} train loss: {train_loss}, rmse: {rmse_error}, mae: {mae_error}', end = '; ')\n",
    "        \n",
    "        # Validate\n",
    "        forecaster.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = torch.tensor([]).to(device)\n",
    "        preds = torch.tensor([]).to(device)\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = forecaster(features)\n",
    "\n",
    "            trues = torch.cat((trues, labels), axis=0)\n",
    "            preds = torch.cat((preds, out.squeeze().detach()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        valid_losses_for_epochs.append(valid_loss) # Record Loss\n",
    "        \n",
    "        rmse_error = rmse(preds, trues)\n",
    "        mae_error = mae(preds, trues)\n",
    "\n",
    "        print(f'Epoch {epoch} valid loss: {valid_loss}, rmse: {rmse_error}, mae: {mae_error}')\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(forecaster.state_dict(), best_model_path)\n",
    "            # best_model.load_state_dict(copy.deepcopy(classifier.state_dict()))\n",
    "            print(f'Best model found! Loss: {valid_loss}')\n",
    "            \n",
    "        else:\n",
    "            # 驗證損失沒有改善，計數器加1\n",
    "            early_stopping_counter += 1\n",
    "            \n",
    "            # 如果計數器達到早期停止的耐心值，則停止訓練\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print('Early stopping triggered.')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For record loss\n",
    "train_losses_for_epochs = []\n",
    "validation_losses_for_epochs = []\n",
    "valid_losses_for_epochs = []\n",
    "\n",
    "# Save best model to ... \n",
    "best_model_path = os.path.join(save_path, 'lte_HO_fst_RNN.pt')\n",
    "\n",
    "early_stopping_patience = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fst(n_epochs, train_dataloader2, test_dataloader2, best_model_path, early_stopping_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "def rmse(predictions, targets):\n",
    "    return torch.sqrt(F.mse_loss(predictions, targets))\n",
    "\n",
    "def mae(predictions, targets):\n",
    "    return torch.mean(torch.abs(predictions - targets))\n",
    "\n",
    "def test2(test_dataloader):\n",
    "    best_model = RNN_Fst(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "    best_model.load_state_dict(torch.load(best_model_path))\n",
    "    best_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        best_model.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = torch.tensor([]).to(device)\n",
    "        preds = torch.tensor([]).to(device)\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = best_model(features)\n",
    "\n",
    "            trues = torch.cat((trues, labels), axis=0)\n",
    "            preds = torch.cat((preds, out.squeeze().detach()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        rmse_error = rmse(preds, trues)\n",
    "        mae_error = mae(preds, trues)\n",
    "\n",
    "        print(f'valid loss {valid_loss}, rmse {rmse_error}, mae {mae_error}')\n",
    "        \n",
    "        return valid_loss, rmse_error.item(), mae_error.item()\n",
    "        \n",
    "test2(test_dataloader2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "save_path = \"/home/wmnlab/Documents/sheng-ru/model\"\n",
    "best_model_path = os.path.join(save_path, 'lte_HO_cls_RNN.pt')\n",
    "torch.save(classifier.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# m_path = os.path.join('/home/wmnlab/Documents/sheng-ru/model', 'lte_HO_cls_RNN.pt')\n",
    "# classifier = RNN(input_dim, out_dim, hidden_dim, num_layers, dropout, rnn)\n",
    "# classifier.load_state_dict(torch.load(m_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheng-ru",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7771dd1fbefba0f9e49b3f12d6cb05ea3fc9d8cb4bbb591d0ecb9d07210ade7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
