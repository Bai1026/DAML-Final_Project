{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "import random\n",
    "import copy\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 26 11:33:27 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 Ti      On | 00000000:65:00.0 Off |                  N/A |\n",
      "|  0%   36C    P8               10W / 285W|     68MiB / 12282MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4070 Ti      On | 00000000:B3:00.0 Off |                  N/A |\n",
      "|  0%   38C    P8                9W / 285W|      6MiB / 12282MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1236      G   /usr/lib/xorg/Xorg                           56MiB |\n",
      "|    0   N/A  N/A      1400      G   /usr/bin/gnome-shell                          9MiB |\n",
      "|    1   N/A  N/A      1236      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_array_create(dirname, dir_list, time_seq):\n",
    "    \n",
    "    columns = ['RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2',\n",
    "               'nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1', 'nr-RSRP2', 'nr-RSRQ2']\n",
    "    \n",
    "    def reamin_HO_time(y_train):\n",
    "        def f(L):    \n",
    "            for i, e in enumerate(L):\n",
    "                if e: return i+1\n",
    "            return 0\n",
    "\n",
    "        out = []\n",
    "        for a2 in y_train:\n",
    "            a1_out = []\n",
    "            for a1 in a2:\n",
    "                a1_out.append(a1.any())\n",
    "      \n",
    "            out.append(f(a1_out))\n",
    "        return out\n",
    "    \n",
    "    def HO(y_train):\n",
    "        out = []\n",
    "        for a2 in y_train:\n",
    "            if sum(a2.reshape(-1)) == 0: ho = 0\n",
    "            elif sum(a2.reshape(-1)) > 0: ho = 1\n",
    "            out.append(ho)\n",
    "        return out\n",
    "\n",
    "    split_time = []\n",
    "    for i, f in enumerate(tqdm(dir_list)):\n",
    "    \n",
    "        f = os.path.join(dirname, f)\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "        # preprocess data with ffill method\n",
    "        del df['Timestamp'], df['lat'], df['long'], df['gpsspeed']\n",
    "        # df[columns] = df[columns].replace(0, np.nan)\n",
    "        # df[columns] = df[columns].fillna(method='ffill')\n",
    "        # df.dropna(inplace=True)\n",
    "        \n",
    "        df.replace(np.nan,0,inplace=True); df.replace('-',0,inplace=True)\n",
    "        \n",
    "        X = df[features]\n",
    "        Y = df[target]\n",
    "\n",
    "        Xt_list = []\n",
    "        Yt_list = []\n",
    "\n",
    "        for j in range(time_seq):\n",
    "            X_t = X.shift(periods=-j)\n",
    "            Xt_list.append(X_t)\n",
    "    \n",
    "        for j in range(time_seq,time_seq+predict_t):\n",
    "            Y_t = Y.shift(periods=-(j))\n",
    "            Yt_list.append(Y_t)\n",
    "\n",
    "        # YY = Y.shift(periods=-(0))\n",
    "\n",
    "        X_ts = np.array(Xt_list); X_ts = np.transpose(X_ts, (1,0,2)); X_ts = X_ts[:-(time_seq+predict_t-1),:,:]\n",
    "        Y_ts = np.array(Yt_list); Y_ts = np.transpose(Y_ts, (1,0,2)); Y_ts = Y_ts[:-(time_seq+predict_t-1),:,:]\n",
    "        split_time.append(len(X_ts))\n",
    "\n",
    "        if i == 0:\n",
    "            X_final = X_ts\n",
    "            Y_final = Y_ts\n",
    "        else:\n",
    "            X_final = np.concatenate((X_final,X_ts), axis=0)\n",
    "            Y_final = np.concatenate((Y_final,Y_ts), axis=0)\n",
    "\n",
    "    split_time = [(sum(split_time[:i]), sum(split_time[:i])+x) for i, x in enumerate(split_time)]\n",
    "    \n",
    "    return X_final, np.array(HO(Y_final)), np.array(reamin_HO_time(Y_final)), split_time # forecast HO\n",
    "\n",
    "class RNN_Dataset_simple(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset take all csv file specified in dir_list in directory dirname.\n",
    "    Transfer csvs to (features, label) pair\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "\n",
    "        # self.inputs = torch.FloatTensor(X)\n",
    "        # self.labels = torch.FloatTensor(y)\n",
    "        self.inputs = torch.FloatTensor(X.astype(np.float32))\n",
    "        self.labels = torch.FloatTensor(y.astype(np.float32))\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data = self.inputs[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_in_file(file, dates):\n",
    "    \n",
    "    for date in dates:\n",
    "        if date in file: return True \n",
    "    return False\n",
    "\n",
    "def train_valid_split(L, valid_size=0.2):\n",
    "    \n",
    "    length = len(L)\n",
    "    v_num = int(length*valid_size)\n",
    "    v_files = random.sample(L, v_num)\n",
    "    t_files = list(set(L) - set(v_files))\n",
    "    \n",
    "    return t_files, v_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:2\"\n",
    "    \n",
    "    print(f\"Random seed set as {seed}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time sequence length and prediction time length\n",
    "seed = 55688\n",
    "time_seq = 20\n",
    "predict_t = 10\n",
    "valid_ratio = 0.2\n",
    "task = 'classification'\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 55688\n",
      "GPU 0: NVIDIA GeForce RTX 4070 Ti\n",
      "GPU 1: NVIDIA GeForce RTX 4070 Ti\n",
      "Loading training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14ac0a3a2234be98afda103931690da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading testing data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95575ad582dc4d648d02b98001c99d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup seed\n",
    "set_seed(seed)\n",
    "\n",
    "# Get GPU\n",
    "device_count = torch.cuda.device_count()\n",
    "num_of_gpus = device_count\n",
    "\n",
    "for i in range(device_count):\n",
    "    print(\"GPU {}: {}\".format(i, torch.cuda.get_device_name(i)))\n",
    "    gpu_id = i\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Save best model to \n",
    "save_path = \"../model\"\n",
    "\n",
    "# Define DataSet\n",
    "dirname = \"../data/single\"\n",
    "dir_list = os.listdir(dirname)\n",
    "dir_list = [f for f in dir_list if ( f.endswith('.csv') and (not 'sm' in f) ) ]\n",
    "\n",
    "train_dates = ['03-26', '04-01']\n",
    "test_dates = ['04-10']\n",
    "    \n",
    "# train_dir_list = [f for f in dir_list if ( f.endswith('.csv') and ('All' in f) and days_in_file(f, train_dates) )]\n",
    "# test_dir_list = [f for f in dir_list if ( f.endswith('.csv') and ('All' in f) and days_in_file(f, test_dates) )]\n",
    "\n",
    "train_dir_list, test_dir_list = train_valid_split(dir_list, valid_ratio)\n",
    "train_dir_list += [f for f in os.listdir(dirname) if 'sm' in f]\n",
    "\n",
    "# features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n",
    "#         'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2',\n",
    "#         'nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1', 'nr-RSRP2', 'nr-RSRQ2' ]\n",
    "features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n",
    "        'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1','nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1']\n",
    "# features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n",
    "#         'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2']\n",
    "\n",
    "num_of_features = len(features)\n",
    "\n",
    "# target = ['LTE_HO', 'MN_HO'] # For eNB HO.\n",
    "# target = ['eNB_to_ENDC'] # Setup gNB\n",
    "target = ['gNB_Rel', 'gNB_HO'] # For gNB HO.\n",
    "# target = ['RLF'] # For RLF\n",
    "# target = ['SCG_RLF'] # For scg failure\n",
    "# target = ['dl-loss'] # For DL loss\n",
    "# target = ['ul-loss'] # For UL loss\n",
    "\n",
    "# Data\n",
    "print('Loading training data...')\n",
    "X_train, y_train1, y_train2, split_time_train = ts_array_create(dirname, train_dir_list, time_seq)\n",
    "\n",
    "train_dataset = RNN_Dataset_simple(X_train, y_train1)\n",
    "train_dataloader1 = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "cond = y_train2 > 0\n",
    "X_train_fore = X_train[cond]\n",
    "y_train2_fore = y_train2[cond]\n",
    "train_dataset = RNN_Dataset_simple(X_train_fore, y_train2_fore)\n",
    "train_dataloader2 = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('Loading testing data...')\n",
    "X_test, y_test1, y_test2, split_time_test = ts_array_create(dirname, test_dir_list, time_seq)\n",
    "\n",
    "test_dataset = RNN_Dataset_simple(X_test, y_test1)\n",
    "test_dataloader1 = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "cond = y_test2 > 0\n",
    "X_test_fore = X_test[cond]\n",
    "y_test2_fore = y_test2[cond]\n",
    "test_dataset = RNN_Dataset_simple(X_test_fore, y_test2_fore)\n",
    "test_dataloader2 = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b = next(iter(train_dataloader1))\n",
    "input_dim, out_dim = a.shape[2], 1\n",
    "a.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Cls(nn.Module):\n",
    "    '''\n",
    "    Using LSTM or GRU.\n",
    "    '''\n",
    "    def __init__(self, input_dim, out_dim, hidden_dim, num_layer, dropout, rnn):\n",
    "\n",
    "        super().__init__()\n",
    "        self.in_dim = input_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dim = hidden_dim\n",
    "        self.num_layer = num_layer\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # input_size: num of features; hidden_size: num of hidden state h\n",
    "        # num_layers: number of recurrent layer; seq; batch_first: batch first than seq\n",
    "        if rnn == 'LSTM':\n",
    "            self.rnn= nn.LSTM(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "        elif rnn == 'GRU':\n",
    "            self.rnn= nn.GRU(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_dim, out_dim) # For binary classification\n",
    "\n",
    "    def forward(self,batch_input):\n",
    "\n",
    "        out,_ = self.rnn(batch_input)\n",
    "        out = self.linear(out[:,-1, :])  #Extract out of last time step (N, L, Hout) -> (Batch, time_seq, output)\n",
    "        \n",
    "        out = torch.sigmoid(out) # Binary Classifier\n",
    "\n",
    "        return out\n",
    "\n",
    "class RNN_Fst(nn.Module):\n",
    "    '''\n",
    "    Using LSTM or GRU.\n",
    "    '''\n",
    "    def __init__(self, input_dim, out_dim, hidden_dim, num_layer, dropout, rnn):\n",
    "\n",
    "        super().__init__()\n",
    "        self.in_dim = input_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dim = hidden_dim\n",
    "        self.num_layer = num_layer\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # input_size: num of features; hidden_size: num of hidden state h\n",
    "        # num_layers: number of recurrent layer; seq; batch_first: batch first than seq\n",
    "        if rnn == 'LSTM':\n",
    "            self.rnn= nn.LSTM(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "        elif rnn == 'GRU':\n",
    "            self.rnn= nn.GRU(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_dim, out_dim) # For binary classification\n",
    "\n",
    "    def forward(self,batch_input):\n",
    "\n",
    "        out,_ = self.rnn(batch_input)\n",
    "        out = self.linear(out[:,-1, :])  #Extract out of last time step (N, L, Hout) -> (Batch, time_seq, output)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_epochs = 600\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "hidden_dim = 128\n",
    "num_layer = 2\n",
    "dropout = 0\n",
    "\n",
    "rnn = 'GRU' # 'LSTM' or 'GRU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 55688\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "# Define model and optimizer\n",
    "\n",
    "classifier = RNN_Cls(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[600, 1000], gamma=0.4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cls(n_epochs, train_dataloader, test_dataloader, best_model_path, early_stopping_patience=30):\n",
    "    \n",
    "    # 初始化變數\n",
    "    best_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = early_stopping_patience\n",
    "    \n",
    "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "\n",
    "        classifier.train()\n",
    "\n",
    "        train_losses = []\n",
    "        \n",
    "        trues = np.array([])\n",
    "        preds = np.array([])\n",
    "\n",
    "        for i, (features, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = classifier(features)\n",
    "            \n",
    "            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n",
    "            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                    \n",
    "            # metrics calculate\n",
    "        \n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(trues, preds)\n",
    "        aucpr = auc(recall, precision)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(trues, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        train_losses_for_epochs.append(train_loss) # Record Loss\n",
    "\n",
    "        print(f'Epoch {epoch} train loss: {train_loss}, auc: {roc_auc}, aucpr: {aucpr}', end = '; ')\n",
    "        \n",
    "        # Validate\n",
    "        classifier.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = np.array([])\n",
    "        preds = np.array([])\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = classifier(features)\n",
    "\n",
    "            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n",
    "            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(trues, preds)\n",
    "        aucpr = auc(recall, precision)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(trues, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        valid_losses_for_epochs.append(valid_loss) # Record Loss\n",
    "        \n",
    "        print(f'Epoch {epoch} valid loss: {valid_loss}, auc: {roc_auc}, aucpr: {aucpr}')\n",
    "        \n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(classifier.state_dict(), best_model_path)\n",
    "            # best_model.load_state_dict(copy.deepcopy(classifier.state_dict()))\n",
    "            print(f'Best model found! Loss: {valid_loss}')\n",
    "            \n",
    "        else:\n",
    "            # 驗證損失沒有改善，計數器加1\n",
    "            early_stopping_counter += 1\n",
    "            \n",
    "            # 如果計數器達到早期停止的耐心值，則停止訓練\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print('Early stopping triggered.')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model/lte_HO_cls_RNN.pt\n"
     ]
    }
   ],
   "source": [
    "# For record loss\n",
    "train_losses_for_epochs = []\n",
    "validation_losses_for_epochs = []\n",
    "valid_losses_for_epochs = []\n",
    "\n",
    "# Save best model to ... \n",
    "best_model_path = os.path.join(save_path, 'lte_HO_cls_RNN.pt')\n",
    "print(best_model_path)\n",
    "\n",
    "early_stopping_patience = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visulized on many sample on validation data\n",
    "# sample_value = 2\n",
    "# # samples = random.sample(split_time_test, sample_value)\n",
    "# samples = [split_time_test[8], split_time_test[9]]\n",
    "\n",
    "# fig, axs = plt.subplots(1, sample_value, figsize=(14, 2.5))\n",
    "\n",
    "# # y_test\n",
    "# # preds\n",
    "\n",
    "# for i in range(sample_value):\n",
    "#     true = [y_test1[i] for i in range(samples[i][0], samples[i][1])]\n",
    "#     axs[i].plot(true, label='true')\n",
    "#     prediction = [preds[i] for i in range(samples[i][0], samples[i][1])]\n",
    "#     # prediction = [1 if preds[i] > 0.5 else 0  for i in range(samples[i][0], samples[i][1])]\n",
    "#     axs[i].plot(prediction, label='pred')\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1794ba6930334dfda52e91f840c73ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.42721959211322547, auc: 0.8600307032293801, aucpr: 0.7316010180480563; Epoch 1 valid loss: 0.44192511761170367, auc: 0.8586090868059404, aucpr: 0.7523857421497316\n",
      "Best model found! Loss: 0.44192511761170367\n",
      "Epoch 2 train loss: 0.42754019252136893, auc: 0.8601151078852293, aucpr: 0.7319148492839396; Epoch 2 valid loss: 0.47260054912111177, auc: 0.8338489021379389, aucpr: 0.7123895677662353\n",
      "Epoch 3 train loss: 0.42764542090089247, auc: 0.8604366342462717, aucpr: 0.7353597197278343; Epoch 3 valid loss: 0.4383377711278167, auc: 0.8622507880458641, aucpr: 0.7627497454371133\n",
      "Best model found! Loss: 0.4383377711278167\n",
      "Epoch 4 train loss: 0.4233865528169866, auc: 0.863762970158083, aucpr: 0.7406624792012199; Epoch 4 valid loss: 0.44026419846341014, auc: 0.8590519442719088, aucpr: 0.7504019643589122\n",
      "Epoch 5 train loss: 0.42780707217857844, auc: 0.8606195061690048, aucpr: 0.7339118725634509; Epoch 5 valid loss: 0.446163010870696, auc: 0.8552185854811969, aucpr: 0.7466143422520427\n",
      "Epoch 6 train loss: 0.42054722415438545, auc: 0.8658481680759114, aucpr: 0.7439349641294434; Epoch 6 valid loss: 0.4638663336359076, auc: 0.8393045974244573, aucpr: 0.7207306140337184\n",
      "Epoch 7 train loss: 0.4310051722332809, auc: 0.8578067079957288, aucpr: 0.7254647266925829; Epoch 7 valid loss: 0.4509679083712399, auc: 0.8530924540946476, aucpr: 0.7392129749161132\n",
      "Epoch 8 train loss: 0.4240903470534555, auc: 0.8630970169377621, aucpr: 0.7396411611813797; Epoch 8 valid loss: 0.4565726177529728, auc: 0.8495795139597756, aucpr: 0.7373195143425303\n",
      "Epoch 9 train loss: 0.42525178531752394, auc: 0.862002219213449, aucpr: 0.7374191977937492; Epoch 9 valid loss: 0.44293375910095845, auc: 0.8592068850207261, aucpr: 0.7436872654269415\n",
      "Epoch 10 train loss: 0.42119208685802983, auc: 0.8657159150772955, aucpr: 0.7430863228750711; Epoch 10 valid loss: 0.4581244248366646, auc: 0.8520753066654382, aucpr: 0.7417399196579378\n",
      "Epoch 11 train loss: 0.42442011571441635, auc: 0.8627889400756836, aucpr: 0.7394937698496641; Epoch 11 valid loss: 0.4531106978536266, auc: 0.8520163876258976, aucpr: 0.7471047903145804\n",
      "Epoch 12 train loss: 0.41872532358083514, auc: 0.8666463460581854, aucpr: 0.746433663854598; Epoch 12 valid loss: 0.44121660320751005, auc: 0.8608817588968788, aucpr: 0.7603720577646145\n",
      "Epoch 13 train loss: 0.4142233578862214, auc: 0.870345537877836, aucpr: 0.7520180805591428; Epoch 13 valid loss: 0.43578626335274134, auc: 0.8627120781181573, aucpr: 0.7653224021773215\n",
      "Best model found! Loss: 0.43578626335274134\n",
      "Epoch 14 train loss: 0.41644205549502217, auc: 0.8690061389328518, aucpr: 0.7518026049795197; Epoch 14 valid loss: 0.4684455231153005, auc: 0.8379388036282731, aucpr: 0.7201307051006647\n",
      "Epoch 15 train loss: 0.4373411185365557, auc: 0.8523331801964991, aucpr: 0.7196163020636241; Epoch 15 valid loss: 0.4587608659012097, auc: 0.8487916610295343, aucpr: 0.7452909246567512\n",
      "Epoch 16 train loss: 0.4182088374063319, auc: 0.867471994736686, aucpr: 0.7477762314702499; Epoch 16 valid loss: 0.4636355618168408, auc: 0.8416356839586282, aucpr: 0.7293987897221823\n",
      "Epoch 17 train loss: 0.42036707591656314, auc: 0.8663014325870854, aucpr: 0.7466301797969175; Epoch 17 valid loss: 0.4556274576817224, auc: 0.844813809601803, aucpr: 0.7381709626543099\n",
      "Epoch 18 train loss: 0.4172226171329096, auc: 0.8681235647657275, aucpr: 0.7501216181153387; Epoch 18 valid loss: 0.48187194603812716, auc: 0.8291942089678161, aucpr: 0.7184919433494933\n",
      "Epoch 19 train loss: 0.41495220050181336, auc: 0.8701138120106527, aucpr: 0.7531039814677097; Epoch 19 valid loss: 0.46551979371531405, auc: 0.8474523436984737, aucpr: 0.739785465823835\n",
      "Epoch 20 train loss: 0.42261271474628315, auc: 0.8643600982071131, aucpr: 0.7425669583725389; Epoch 20 valid loss: 0.4357301805914844, auc: 0.866557962770172, aucpr: 0.7685758456462058\n",
      "Best model found! Loss: 0.4357301805914844\n",
      "Epoch 21 train loss: 0.4198829058239153, auc: 0.8665225608582439, aucpr: 0.7465082583040785; Epoch 21 valid loss: 0.44068106325964135, auc: 0.8628568972586708, aucpr: 0.7638768145601006\n",
      "Epoch 22 train loss: 0.41635002665404097, auc: 0.869129281654593, aucpr: 0.751541119507981; Epoch 22 valid loss: 0.4299815523486989, auc: 0.871083063735441, aucpr: 0.7750449105022398\n",
      "Best model found! Loss: 0.4299815523486989\n",
      "Epoch 23 train loss: 0.4221183244290065, auc: 0.8645017909103359, aucpr: 0.742078564284335; Epoch 23 valid loss: 0.49486818527349374, auc: 0.8113730546402409, aucpr: 0.6671226966788371\n",
      "Epoch 24 train loss: 0.4220995945947789, auc: 0.8635649974563473, aucpr: 0.7391764398960072; Epoch 24 valid loss: 0.4310136057701562, auc: 0.8702324330880065, aucpr: 0.7829564806710383\n",
      "Epoch 25 train loss: 0.41807874197586015, auc: 0.867299480774833, aucpr: 0.7485448542228208; Epoch 25 valid loss: 0.4284731319753195, auc: 0.8698396197029781, aucpr: 0.7774424844074802\n",
      "Best model found! Loss: 0.4284731319753195\n",
      "Epoch 26 train loss: 0.4202910295150486, auc: 0.8657374442149431, aucpr: 0.7456038436789477; Epoch 26 valid loss: 0.44827758651645333, auc: 0.8584932671120926, aucpr: 0.7603934679321195\n",
      "Epoch 27 train loss: 0.4199208863852184, auc: 0.86599950962329, aucpr: 0.7447784022531135; Epoch 27 valid loss: 0.4476226657247066, auc: 0.8573774859456571, aucpr: 0.7586239451768553\n",
      "Epoch 28 train loss: 0.4271047133393239, auc: 0.8604139600178267, aucpr: 0.7355742515229613; Epoch 28 valid loss: 0.43001920517799114, auc: 0.8676956493528196, aucpr: 0.7689097673451168\n",
      "Epoch 29 train loss: 0.41706393697785726, auc: 0.8679501784792393, aucpr: 0.7494003274760854; Epoch 29 valid loss: 0.45650327550631004, auc: 0.8525258518045759, aucpr: 0.7478411218410528\n",
      "Epoch 30 train loss: 0.4162518378805106, auc: 0.8689894565242456, aucpr: 0.7525362714028615; Epoch 30 valid loss: 0.42585299736802723, auc: 0.8725630447080046, aucpr: 0.7832583866250775\n",
      "Best model found! Loss: 0.42585299736802723\n",
      "Epoch 31 train loss: 0.4162761891684392, auc: 0.8686916468892376, aucpr: 0.7491817173155645; Epoch 31 valid loss: 0.42119285444404636, auc: 0.8767340081111779, aucpr: 0.7864390412043821\n",
      "Best model found! Loss: 0.42119285444404636\n",
      "Epoch 32 train loss: 0.4166389046562078, auc: 0.8690657717527014, aucpr: 0.7532817140099721; Epoch 32 valid loss: 0.4228929111329413, auc: 0.8741494064552353, aucpr: 0.7849419779770783\n",
      "Epoch 33 train loss: 0.4413465570171139, auc: 0.8480896094257744, aucpr: 0.7126130186041277; Epoch 33 valid loss: 0.4645143025223347, auc: 0.8426031138103913, aucpr: 0.7220604356890559\n",
      "Epoch 34 train loss: 0.4361976235026611, auc: 0.8518164679040914, aucpr: 0.7202611331491763; Epoch 34 valid loss: 0.43484014107877117, auc: 0.8629759819874553, aucpr: 0.7601163845197149\n",
      "Epoch 35 train loss: 0.42443320443202, auc: 0.8629147690452761, aucpr: 0.7390882619465646; Epoch 35 valid loss: 0.4308843247440717, auc: 0.8666695676007379, aucpr: 0.7676227631912415\n",
      "Epoch 36 train loss: 0.42073994882391585, auc: 0.8650903577914599, aucpr: 0.7443359100059196; Epoch 36 valid loss: 0.4577653082122517, auc: 0.8452684211935437, aucpr: 0.7346619675345634\n",
      "Epoch 37 train loss: 0.4187991358801914, auc: 0.8673739572318093, aucpr: 0.7497816567164225; Epoch 37 valid loss: 0.4269977606351623, auc: 0.8701794801577594, aucpr: 0.7756069623223678\n",
      "Epoch 38 train loss: 0.4156596190608899, auc: 0.8690974248634169, aucpr: 0.7521502853562981; Epoch 38 valid loss: 0.41448031120134815, auc: 0.8789029708196704, aucpr: 0.795431973408938\n",
      "Best model found! Loss: 0.41448031120134815\n",
      "Epoch 39 train loss: 0.4178629949814569, auc: 0.8681246027454507, aucpr: 0.7523440924023419; Epoch 39 valid loss: 0.4240798996264387, auc: 0.8743782854043025, aucpr: 0.7812133747929256\n",
      "Epoch 40 train loss: 0.4283859385006999, auc: 0.8590448717279001, aucpr: 0.7317406536365272; Epoch 40 valid loss: 0.43682272464022986, auc: 0.863621123208824, aucpr: 0.7678600407587706\n",
      "Epoch 41 train loss: 0.41466864091056455, auc: 0.869295194449425, aucpr: 0.751550578134889; Epoch 41 valid loss: 0.46149436550473005, auc: 0.845535352640692, aucpr: 0.7416193062165768\n",
      "Epoch 42 train loss: 0.4214151444626215, auc: 0.8639368144432413, aucpr: 0.744426731694283; Epoch 42 valid loss: 0.4298696445295943, auc: 0.8686406691920625, aucpr: 0.7748363265245872\n",
      "Epoch 43 train loss: 0.42439882268592816, auc: 0.862392762002111, aucpr: 0.739406185242434; Epoch 43 valid loss: 0.42795860696032867, auc: 0.8700280122188293, aucpr: 0.7737851418564023\n",
      "Epoch 44 train loss: 0.42486485927176154, auc: 0.8618524984505131, aucpr: 0.737553360716918; Epoch 44 valid loss: 0.431284466918823, auc: 0.8672551070934428, aucpr: 0.7654061018701797\n",
      "Epoch 45 train loss: 0.41564330385710196, auc: 0.8684587377176399, aucpr: 0.7492609849992248; Epoch 45 valid loss: 0.45594605523046483, auc: 0.8523899669869349, aucpr: 0.749870466188672\n",
      "Epoch 46 train loss: 0.4123754330311557, auc: 0.8711426598089225, aucpr: 0.7543595837570842; Epoch 46 valid loss: 0.4345090862482172, auc: 0.8700758004574491, aucpr: 0.7796178062672847\n",
      "Epoch 47 train loss: 0.4217709312847135, auc: 0.8642348401828419, aucpr: 0.7420382287839312; Epoch 47 valid loss: 0.46165318543279776, auc: 0.8408419539655007, aucpr: 0.7321998999631045\n",
      "Epoch 48 train loss: 0.4305396545685853, auc: 0.8567255753102385, aucpr: 0.7277466563075582; Epoch 48 valid loss: 0.4424477414915178, auc: 0.8587449122593098, aucpr: 0.7592458503094035\n",
      "Epoch 49 train loss: 0.42112923445731487, auc: 0.8643342501777733, aucpr: 0.7406568812343135; Epoch 49 valid loss: 0.4858444691722871, auc: 0.8245194506739003, aucpr: 0.7037966620202263\n",
      "Epoch 50 train loss: 0.42631775527966237, auc: 0.8608300278084067, aucpr: 0.7370199400215074; Epoch 50 valid loss: 0.48017341973720473, auc: 0.8304280953525551, aucpr: 0.7056286925459421\n",
      "Epoch 51 train loss: 0.42075724721440333, auc: 0.8651002658481799, aucpr: 0.7435759883590651; Epoch 51 valid loss: 0.43532041504225705, auc: 0.8615048462913566, aucpr: 0.7726151059473481\n",
      "Epoch 52 train loss: 0.42478669615081777, auc: 0.8611327493050251, aucpr: 0.7405787289801369; Epoch 52 valid loss: 0.4270701715408751, auc: 0.8714872453784765, aucpr: 0.7862241690019504\n",
      "Epoch 53 train loss: 0.41966252421818334, auc: 0.8664315094802242, aucpr: 0.7458979500621374; Epoch 53 valid loss: 0.4434251565458288, auc: 0.8550523061565083, aucpr: 0.7550619757894332\n",
      "Epoch 54 train loss: 0.4243817156088161, auc: 0.8626396016371618, aucpr: 0.7394832366525731; Epoch 54 valid loss: 0.4600871630389704, auc: 0.8431106783323793, aucpr: 0.736181989030393\n",
      "Epoch 55 train loss: 0.41878129645453327, auc: 0.866103222885627, aucpr: 0.7456185404238623; Epoch 55 valid loss: 0.468504792168276, auc: 0.8349859060314634, aucpr: 0.7178189124862647\n",
      "Epoch 56 train loss: 0.42710175822433644, auc: 0.8608405973560129, aucpr: 0.734437423454688; Epoch 56 valid loss: 0.5363268385999244, auc: 0.7702663835149217, aucpr: 0.6332138318938821\n",
      "Epoch 57 train loss: 0.42478585357565946, auc: 0.8615277903823103, aucpr: 0.7396110061586308; Epoch 57 valid loss: 0.42831557330567294, auc: 0.8722239856708895, aucpr: 0.7835835276382399\n",
      "Epoch 58 train loss: 0.424511422924834, auc: 0.8631151040262186, aucpr: 0.7385649036523093; Epoch 58 valid loss: 0.43716031561382396, auc: 0.8630179228453249, aucpr: 0.7612994995776847\n",
      "Epoch 59 train loss: 0.4291880818152216, auc: 0.8589424332567648, aucpr: 0.7342287462526125; Epoch 59 valid loss: 0.430705920370184, auc: 0.8701017723262645, aucpr: 0.7771741121023444\n",
      "Epoch 60 train loss: 0.41595030488361473, auc: 0.8694761357374029, aucpr: 0.7506907227667444; Epoch 60 valid loss: 0.41992977699648576, auc: 0.875174360286157, aucpr: 0.7837254944009789\n",
      "Epoch 61 train loss: 0.4177523696425506, auc: 0.8672452902893646, aucpr: 0.746210002124545; Epoch 61 valid loss: 0.4338927290512228, auc: 0.8656237175462205, aucpr: 0.7708019751318085\n",
      "Epoch 62 train loss: 0.42283465897027, auc: 0.8629153207896665, aucpr: 0.7399924173278163; Epoch 62 valid loss: 0.4264717981121625, auc: 0.8709316254786468, aucpr: 0.7800723303819661\n",
      "Epoch 63 train loss: 0.4252230668937562, auc: 0.861457891464857, aucpr: 0.7378183489360723; Epoch 63 valid loss: 0.4301873813261198, auc: 0.8723292385243966, aucpr: 0.7849589139721762\n",
      "Epoch 64 train loss: 0.4411300853071581, auc: 0.8486015937713687, aucpr: 0.7156642893922606; Epoch 64 valid loss: 0.4582971090342752, auc: 0.8487735252445674, aucpr: 0.7376619055675969\n",
      "Epoch 65 train loss: 0.4380125871999692, auc: 0.8509968104806962, aucpr: 0.7157726488826407; Epoch 65 valid loss: 0.4308080664894296, auc: 0.8688698449624873, aucpr: 0.7823950488384311\n",
      "Epoch 66 train loss: 0.42985271225069166, auc: 0.8582264017502965, aucpr: 0.7305506533503842; Epoch 66 valid loss: 0.45468113817490835, auc: 0.8491012160236754, aucpr: 0.7426405502007545\n",
      "Epoch 67 train loss: 0.4440426125877343, auc: 0.8462320391058955, aucpr: 0.7099859667558479; Epoch 67 valid loss: 0.4316863771894256, auc: 0.8672777248809138, aucpr: 0.7689298795899678\n",
      "Epoch 68 train loss: 0.44964275759308764, auc: 0.8410305715524093, aucpr: 0.6989323878856358; Epoch 68 valid loss: 0.4966521437623458, auc: 0.8098743442399946, aucpr: 0.6696365197083116\n",
      "Epoch 69 train loss: 0.46874870523666284, auc: 0.8244507808143171, aucpr: 0.6690092632061222; Epoch 69 valid loss: 0.48581404524265825, auc: 0.8218084921659049, aucpr: 0.6898415742536035\n",
      "Epoch 70 train loss: 0.45510537278893776, auc: 0.8360081056164134, aucpr: 0.6873543385121521; Epoch 70 valid loss: 0.47262618740608964, auc: 0.8353659560981027, aucpr: 0.7117591116383597\n",
      "Epoch 71 train loss: 0.48911088292257926, auc: 0.797102478849563, aucpr: 0.6249743793645292; Epoch 71 valid loss: 0.5556119808801607, auc: 0.7360849702190227, aucpr: 0.5769259199718827\n",
      "Epoch 72 train loss: 0.48967504224276387, auc: 0.7980961083760767, aucpr: 0.6209298528917717; Epoch 72 valid loss: 0.5031114246282894, auc: 0.8011344096748566, aucpr: 0.6595955668752452\n",
      "Epoch 73 train loss: 0.487191174475162, auc: 0.8002793255390731, aucpr: 0.6257647446829251; Epoch 73 valid loss: 0.5032444859397937, auc: 0.8051535192713345, aucpr: 0.6678316950032509\n",
      "Epoch 74 train loss: 0.47144550452010475, auc: 0.8181043837889554, aucpr: 0.6537625477691033; Epoch 74 valid loss: 0.4753176723589444, auc: 0.8304013814303454, aucpr: 0.709359627262879\n",
      "Epoch 75 train loss: 0.46093205703628753, auc: 0.8297440333343217, aucpr: 0.6782679606175561; Epoch 75 valid loss: 0.474889501116556, auc: 0.8261736657835438, aucpr: 0.7092960158497206\n",
      "Epoch 76 train loss: 0.4696069777700509, auc: 0.8212348109108347, aucpr: 0.6608791316559322; Epoch 76 valid loss: 0.49081563032563597, auc: 0.8188594829288836, aucpr: 0.692181772762976\n",
      "Epoch 77 train loss: 0.4641938462678571, auc: 0.8258339684229401, aucpr: 0.6660690900611568; Epoch 77 valid loss: 0.4741288687691985, auc: 0.8338683142547447, aucpr: 0.7123638046239533\n",
      "Epoch 78 train loss: 0.469251339491068, auc: 0.8218376772144721, aucpr: 0.661359497657239; Epoch 78 valid loss: 0.4668870740823015, auc: 0.8373899512554028, aucpr: 0.722381641654755\n",
      "Epoch 79 train loss: 0.45526833473022776, auc: 0.8357946219510874, aucpr: 0.6865363335176393; Epoch 79 valid loss: 0.457074460311887, auc: 0.8493351409358265, aucpr: 0.7407532579884761\n",
      "Epoch 80 train loss: 0.45540329279765157, auc: 0.8351078443802218, aucpr: 0.68487546260325; Epoch 80 valid loss: 0.5032452753385745, auc: 0.8084961729931863, aucpr: 0.6793336249893868\n",
      "Epoch 81 train loss: 0.47023862603996375, auc: 0.8202104544361445, aucpr: 0.6603971823795487; Epoch 81 valid loss: 0.46901272159865065, auc: 0.837258548440266, aucpr: 0.727687940259828\n",
      "Epoch 82 train loss: 0.4749557894278465, auc: 0.815156926476907, aucpr: 0.6512156979203887; Epoch 82 valid loss: 0.5165516585373225, auc: 0.788493915963581, aucpr: 0.6599794794338218\n",
      "Epoch 83 train loss: 0.46992022773126296, auc: 0.8219376681316789, aucpr: 0.6625799360844357; Epoch 83 valid loss: 0.46605310982532716, auc: 0.841769045794727, aucpr: 0.7339549021061612\n",
      "Epoch 84 train loss: 0.46507643336513, auc: 0.8263464962301702, aucpr: 0.6747137864338145; Epoch 84 valid loss: 0.544919310993958, auc: 0.7524343655256478, aucpr: 0.5954306630588162\n",
      "Epoch 85 train loss: 0.4904215562270945, auc: 0.7998735786252228, aucpr: 0.6227603831069675; Epoch 85 valid loss: 0.503856091550813, auc: 0.8081351788577233, aucpr: 0.6591608410364738\n",
      "Epoch 86 train loss: 0.47846575342839454, auc: 0.81267516721987, aucpr: 0.64071429558549; Epoch 86 valid loss: 0.4948986934232857, auc: 0.8160637225589397, aucpr: 0.6833402367160318\n",
      "Epoch 87 train loss: 0.4897020976038411, auc: 0.7980514729890438, aucpr: 0.6217458508066117; Epoch 87 valid loss: 0.5531957921491804, auc: 0.7415227968597962, aucpr: 0.5650157384456322\n",
      "Epoch 88 train loss: 0.5005077135837713, auc: 0.7869247608707622, aucpr: 0.59758425429216; Epoch 88 valid loss: 0.5141357542172577, auc: 0.794782254523246, aucpr: 0.6511401759007787\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "train_cls(n_epochs, train_dataloader1, test_dataloader1, best_model_path, early_stopping_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.41448031120134815, roc_auc 0.8789029708196704, aucpr 0.795431973408938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.41448031120134815,\n",
       " 0.8789029708196704,\n",
       " 0.795431973408938,\n",
       " 0.7133520074696545,\n",
       " 0.7329708986248801,\n",
       " 0.7230283911671924)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "def test(test_dataloader):\n",
    "    best_model = RNN_Cls(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "    best_model.load_state_dict(torch.load(best_model_path))\n",
    "    best_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        best_model.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = np.array([])\n",
    "        preds = np.array([])\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = best_model(features)\n",
    "\n",
    "            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n",
    "            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(trues, preds)\n",
    "        aucpr = auc(recall, precision)\n",
    "        threshold = 0.5\n",
    "        p = precision_score(trues, [1 if pred > threshold else 0 for pred in preds])\n",
    "        r = recall_score(trues, [1 if pred > threshold else 0 for pred in preds])\n",
    "        f1 = f1_score(trues, [1 if pred > threshold else 0 for pred in preds])\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(trues, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "\n",
    "        print(f'valid loss {valid_loss}, roc_auc {roc_auc}, aucpr {aucpr}')\n",
    "        \n",
    "        return valid_loss, roc_auc, aucpr, p, r, f1\n",
    "        \n",
    "test(test_dataloader1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import itertools\n",
    "\n",
    "n_epochs = 600\n",
    "lrs = [0.001, 0.01, 0.1]\n",
    "hidden_dims = [32, 64, 128]\n",
    "num_layers = [1, 2]\n",
    "dropout = 0\n",
    "\n",
    "early_stopping_patience = 50\n",
    "rnn = 'GRU'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For learning_rate = 0.001, hidden_dim = 32, num_layer = 1.\n",
      "valid loss 0.549439296380997, roc_auc 0.7551104475240914, aucpr 0.6032514706158869\n",
      "Random seed set as 55688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab0ae9243a044e6b0359b1e69078e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.541000144682842, auc: 0.7118112640753881, aucpr: 0.48047472135696795; Epoch 1 valid loss: 0.5678944449090444, auc: 0.722749202159031, aucpr: 0.5656147478250108\n",
      "Best model found! Loss: 0.5678944449090444\n",
      "Epoch 2 train loss: 0.514731068915407, auc: 0.7666205275832194, aucpr: 0.5752750753710775; Epoch 2 valid loss: 0.5345588791103901, auc: 0.7709914587279697, aucpr: 0.6144170002258098\n",
      "Best model found! Loss: 0.5345588791103901\n",
      "Epoch 3 train loss: 0.49325764254889504, auc: 0.7984053932025881, aucpr: 0.6185327048338755; Epoch 3 valid loss: 0.5354040742046042, auc: 0.7819240720280954, aucpr: 0.6282306262857146\n",
      "Epoch 4 train loss: 0.4835167354683925, auc: 0.8093598748551106, aucpr: 0.6334439065249375; Epoch 4 valid loss: 0.5254436214123103, auc: 0.7894912654082193, aucpr: 0.6352728463651925\n",
      "Best model found! Loss: 0.5254436214123103\n",
      "Epoch 5 train loss: 0.4731254353979983, auc: 0.8202287578174378, aucpr: 0.6513563960219914; Epoch 5 valid loss: 0.540102713319302, auc: 0.7741410301565157, aucpr: 0.6084526143998034\n",
      "Epoch 6 train loss: 0.46488445853333055, auc: 0.82804087671073, aucpr: 0.6648383345462454; Epoch 6 valid loss: 0.48767312922364986, auc: 0.8200837226194914, aucpr: 0.6851879462880681\n",
      "Best model found! Loss: 0.48767312922364986\n",
      "Epoch 7 train loss: 0.46261526161493677, auc: 0.8308102205424546, aucpr: 0.6727976684674848; Epoch 7 valid loss: 0.5210159726350216, auc: 0.7872787293243889, aucpr: 0.639797419720193\n",
      "Epoch 8 train loss: 0.45744669509836716, auc: 0.8349994747039504, aucpr: 0.679486381147696; Epoch 8 valid loss: 0.47200754046272697, auc: 0.8328967289039712, aucpr: 0.7083779696235986\n",
      "Best model found! Loss: 0.47200754046272697\n",
      "Epoch 9 train loss: 0.4536587899066649, auc: 0.8385342702258038, aucpr: 0.6865248122187942; Epoch 9 valid loss: 0.5153228135409762, auc: 0.7938042578311417, aucpr: 0.6461516793617021\n",
      "Epoch 10 train loss: 0.4527065085870147, auc: 0.8397213743536747, aucpr: 0.6878769589747092; Epoch 10 valid loss: 0.4616347989067435, auc: 0.8410699424504942, aucpr: 0.7248010725538417\n",
      "Best model found! Loss: 0.4616347989067435\n",
      "Epoch 11 train loss: 0.4488945175686395, auc: 0.8424070147400087, aucpr: 0.6914228568659566; Epoch 11 valid loss: 0.4928772924642326, auc: 0.8186890184230488, aucpr: 0.6911570676854353\n",
      "Epoch 12 train loss: 0.4435578221761036, auc: 0.8469123534928988, aucpr: 0.7041096932547362; Epoch 12 valid loss: 0.4859507603619652, auc: 0.8279377938420136, aucpr: 0.7081081179165741\n",
      "Epoch 13 train loss: 0.4403677786823027, auc: 0.8505907399746906, aucpr: 0.7115789837845087; Epoch 13 valid loss: 0.48348727906366973, auc: 0.8257250499832326, aucpr: 0.7006236271963855\n",
      "Epoch 14 train loss: 0.4389885087669199, auc: 0.8511579303843904, aucpr: 0.716519401813306; Epoch 14 valid loss: 0.49422052699884167, auc: 0.8220677953041556, aucpr: 0.6988768733780689\n",
      "Epoch 15 train loss: 0.4375762604655436, auc: 0.8520901799946069, aucpr: 0.7170002504311463; Epoch 15 valid loss: 0.4650730121120364, auc: 0.846028432280415, aucpr: 0.7407387321976131\n",
      "Epoch 16 train loss: 0.43163132349540106, auc: 0.8573313243888998, aucpr: 0.7287292287441207; Epoch 16 valid loss: 0.45200159449186106, auc: 0.8541991822096675, aucpr: 0.7534646080125827\n",
      "Best model found! Loss: 0.45200159449186106\n",
      "Epoch 17 train loss: 0.42646133775108014, auc: 0.8610000123300094, aucpr: 0.73257026854915; Epoch 17 valid loss: 0.47552647757647415, auc: 0.8392484388235449, aucpr: 0.7284251556969611\n",
      "Epoch 18 train loss: 0.42778419179665045, auc: 0.8599998691701292, aucpr: 0.7313874689791406; Epoch 18 valid loss: 0.4403454261293028, auc: 0.8608001330234597, aucpr: 0.7650260332820613\n",
      "Best model found! Loss: 0.4403454261293028\n",
      "Epoch 19 train loss: 0.42868469303430473, auc: 0.8589191745294008, aucpr: 0.7315599391413927; Epoch 19 valid loss: 0.43801759725806305, auc: 0.8629081879893137, aucpr: 0.7666632287619026\n",
      "Best model found! Loss: 0.43801759725806305\n",
      "Epoch 20 train loss: 0.42711494862537186, auc: 0.8612583712057573, aucpr: 0.7355824900506966; Epoch 20 valid loss: 0.45904107737853733, auc: 0.8488593362991328, aucpr: 0.739820182701134\n",
      "Epoch 21 train loss: 0.42353859254464443, auc: 0.8633789374261435, aucpr: 0.7388034140912612; Epoch 21 valid loss: 0.4580620421264111, auc: 0.8538678702099934, aucpr: 0.7488788369174935\n",
      "Epoch 22 train loss: 0.4198219791376244, auc: 0.8658848659487965, aucpr: 0.745832473200067; Epoch 22 valid loss: 0.4297411366634434, auc: 0.8691493616352102, aucpr: 0.7768974794742867\n",
      "Best model found! Loss: 0.4297411366634434\n",
      "Epoch 23 train loss: 0.4169997052250876, auc: 0.8683483252129398, aucpr: 0.7499974630230755; Epoch 23 valid loss: 0.44644659745929, auc: 0.8542409449747225, aucpr: 0.7562187849453397\n",
      "Epoch 24 train loss: 0.4243504866987295, auc: 0.8625906808659433, aucpr: 0.738561318719051; Epoch 24 valid loss: 0.4338130144653528, auc: 0.8658731958975251, aucpr: 0.7729806649408552\n",
      "Epoch 25 train loss: 0.4188534615654675, auc: 0.8664370640082789, aucpr: 0.7452609030801896; Epoch 25 valid loss: 0.43006570968508356, auc: 0.8680025329547396, aucpr: 0.77739053927442\n",
      "Epoch 26 train loss: 0.41342002332927996, auc: 0.8710118466918633, aucpr: 0.7562050465904011; Epoch 26 valid loss: 0.4228847871869384, auc: 0.8727840875732236, aucpr: 0.7836459386738976\n",
      "Best model found! Loss: 0.4228847871869384\n",
      "Epoch 27 train loss: 0.4131088347549162, auc: 0.8711164108189714, aucpr: 0.7543432823684353; Epoch 27 valid loss: 0.42759430598230397, auc: 0.8692450271588573, aucpr: 0.7791979048545683\n",
      "Epoch 28 train loss: 0.40976402151962565, auc: 0.8731303441060909, aucpr: 0.7577853207168413; Epoch 28 valid loss: 0.42586166591973007, auc: 0.8709966887202957, aucpr: 0.7802570821555926\n",
      "Epoch 29 train loss: 0.40383795969594205, auc: 0.877894846667862, aucpr: 0.7662279925486625; Epoch 29 valid loss: 0.42826485075143755, auc: 0.8692989892817213, aucpr: 0.7790991980522003\n",
      "Epoch 30 train loss: 0.40384326485024796, auc: 0.8776792365935855, aucpr: 0.7669997262466673; Epoch 30 valid loss: 0.42513232460517564, auc: 0.8719568167666548, aucpr: 0.7764356275198465\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Save best model to ... \u001b[39;00m\n\u001b[1;32m     22\u001b[0m best_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlte_HO_cls_RNN.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtrain_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFor learning_rate = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, hidden_dim = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, num_layer = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_layer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 31\u001b[0m, in \u001b[0;36mtrain_cls\u001b[0;34m(n_epochs, train_dataloader, test_dataloader, best_model_path, early_stopping_patience)\u001b[0m\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out\u001b[38;5;241m.\u001b[39msqueeze(), labels)\n\u001b[1;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 31\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# metrics calculate\u001b[39;00m\n\u001b[1;32m     35\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/anaconda3/envs/wmnlab/lib/python3.9/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/wmnlab/lib/python3.9/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/anaconda3/envs/wmnlab/lib/python3.9/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/wmnlab/lib/python3.9/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/wmnlab/lib/python3.9/site-packages/torch/optim/adam.py:363\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    360\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f_out = 'lte_ho_cls_rnn.csv'\n",
    "f_out = open(f_out, 'w')\n",
    "cols_out = ['lr','hidden_dim','num_layer', 'valid_loss','auc','aucpr', 'p', 'r', 'f1']\n",
    "f_out.write(','.join(cols_out)+'\\n')\n",
    "\n",
    "for lr, hidden_dim, num_layer in itertools.product(lrs, hidden_dims, num_layers):\n",
    "    \n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Model and optimizer\n",
    "    classifier = RNN_Cls(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=lr)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # For record loss\n",
    "    train_losses_for_epochs = []\n",
    "    validation_losses_for_epochs = []\n",
    "    valid_losses_for_epochs = []\n",
    "\n",
    "    # Save best model to ... \n",
    "    best_model_path = os.path.join(save_path, 'lte_HO_cls_RNN.pt')\n",
    "    \n",
    "    train_cls(n_epochs, train_dataloader1, test_dataloader1, best_model_path, early_stopping_patience)\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    print(f'For learning_rate = {lr}, hidden_dim = {hidden_dim}, num_layer = {num_layer}.')\n",
    "    valid_loss, roc_auc, aucpr, p, r, f1 = test(test_dataloader1)\n",
    "    \n",
    "    cols_out = [lr, hidden_dim, num_layer, valid_loss, roc_auc, aucpr, p, r, f1]\n",
    "    cols_out = [str(n) for n in cols_out]\n",
    "    f_out.write(','.join(cols_out)+'\\n')\n",
    "\n",
    "f_out.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_epochs = 600\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "hidden_dim = 128\n",
    "num_layer = 2\n",
    "dropout = 0\n",
    "\n",
    "rnn = 'GRU' # 'LSTM' or 'GRU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 55688\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "forecaster = RNN_Fst(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "optimizer = optim.Adam(forecaster.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fst(n_epochs, train_dataloader, test_dataloader, best_model_path, early_stopping_patience=30):\n",
    "    \n",
    "    def rmse(predictions, targets):\n",
    "        return torch.sqrt(F.mse_loss(predictions, targets))\n",
    "\n",
    "    def mae(predictions, targets):\n",
    "        return torch.mean(torch.abs(predictions - targets))\n",
    "    \n",
    "    # 初始化變數\n",
    "    best_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = early_stopping_patience\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        forecaster.train()\n",
    "\n",
    "        train_losses = []\n",
    "        \n",
    "        trues = torch.tensor([]).to(device)\n",
    "        preds = torch.tensor([]).to(device)\n",
    "\n",
    "        for i, (features, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = forecaster(features)\n",
    "            \n",
    "            trues = torch.cat((trues, labels), axis=0)\n",
    "            preds = torch.cat((preds, out.squeeze().detach()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                    \n",
    "            # metrics calculate\n",
    "      \n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        train_losses_for_epochs.append(train_loss) # Record Loss\n",
    "\n",
    "        rmse_error = rmse(preds, trues)\n",
    "        mae_error = mae(preds, trues)\n",
    "        \n",
    "        print(f'Epoch {epoch} train loss: {train_loss}, rmse: {rmse_error}, mae: {mae_error}', end = '; ')\n",
    "        \n",
    "        # Validate\n",
    "        forecaster.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = torch.tensor([]).to(device)\n",
    "        preds = torch.tensor([]).to(device)\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = forecaster(features)\n",
    "\n",
    "            trues = torch.cat((trues, labels), axis=0)\n",
    "            preds = torch.cat((preds, out.squeeze().detach()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        valid_losses_for_epochs.append(valid_loss) # Record Loss\n",
    "        \n",
    "        rmse_error = rmse(preds, trues)\n",
    "        mae_error = mae(preds, trues)\n",
    "\n",
    "        print(f'Epoch {epoch} valid loss: {valid_loss}, rmse: {rmse_error}, mae: {mae_error}')\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(forecaster.state_dict(), best_model_path)\n",
    "            # best_model.load_state_dict(copy.deepcopy(classifier.state_dict()))\n",
    "            print(f'Best model found! Loss: {valid_loss}')\n",
    "            \n",
    "        else:\n",
    "            # 驗證損失沒有改善，計數器加1\n",
    "            early_stopping_counter += 1\n",
    "            \n",
    "            # 如果計數器達到早期停止的耐心值，則停止訓練\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print('Early stopping triggered.')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For record loss\n",
    "train_losses_for_epochs = []\n",
    "validation_losses_for_epochs = []\n",
    "valid_losses_for_epochs = []\n",
    "\n",
    "# Save best model to ... \n",
    "best_model_path = os.path.join(save_path, 'lte_HO_fst_RNN.pt')\n",
    "\n",
    "early_stopping_patience = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 7.177642323063538, rmse: 2.6792397499084473, mae: 2.269796133041382; Epoch 1 valid loss: 6.251005211654975, rmse: 2.50063419342041, mae: 2.120218515396118\n",
      "Best model found! Loss: 6.251005211654975\n",
      "Epoch 2 train loss: 6.235113834912798, rmse: 2.4970955848693848, mae: 2.0826523303985596; Epoch 2 valid loss: 5.995686071259635, rmse: 2.4488203525543213, mae: 2.076934576034546\n",
      "Best model found! Loss: 5.995686071259635\n",
      "Epoch 3 train loss: 6.041770482485274, rmse: 2.4580719470977783, mae: 2.032453775405884; Epoch 3 valid loss: 5.85856412381542, rmse: 2.420503854751587, mae: 2.0303750038146973\n",
      "Best model found! Loss: 5.85856412381542\n",
      "Epoch 4 train loss: 5.9665968686078505, rmse: 2.4427034854888916, mae: 2.0079736709594727; Epoch 4 valid loss: 5.812350878910142, rmse: 2.410991668701172, mae: 2.02079701423645\n",
      "Best model found! Loss: 5.812350878910142\n",
      "Epoch 5 train loss: 5.937625103503202, rmse: 2.436760425567627, mae: 2.0041685104370117; Epoch 5 valid loss: 5.8309624462711565, rmse: 2.414891004562378, mae: 2.0111961364746094\n",
      "Epoch 6 train loss: 5.887356036110262, rmse: 2.4264676570892334, mae: 1.9918245077133179; Epoch 6 valid loss: 5.845810031404301, rmse: 2.417786121368408, mae: 2.0106773376464844\n",
      "Epoch 7 train loss: 5.898553825901673, rmse: 2.4287467002868652, mae: 1.9954277276992798; Epoch 7 valid loss: 5.766581615623163, rmse: 2.4017906188964844, mae: 2.0020530223846436\n",
      "Best model found! Loss: 5.766581615623163\n",
      "Epoch 8 train loss: 5.9324295141000665, rmse: 2.4356985092163086, mae: 1.9998656511306763; Epoch 8 valid loss: 5.77507543807127, rmse: 2.403625249862671, mae: 1.9861944913864136\n",
      "Epoch 9 train loss: 5.874866164258096, rmse: 2.423844814300537, mae: 1.9886709451675415; Epoch 9 valid loss: 5.992690624022971, rmse: 2.448265790939331, mae: 1.9931846857070923\n",
      "Epoch 10 train loss: 5.843480322635279, rmse: 2.417355537414551, mae: 1.9799675941467285; Epoch 10 valid loss: 5.664578498626242, rmse: 2.380171298980713, mae: 1.9664798974990845\n",
      "Best model found! Loss: 5.664578498626242\n",
      "Epoch 11 train loss: 5.7970495122723875, rmse: 2.4077389240264893, mae: 1.9713393449783325; Epoch 11 valid loss: 5.667027857838844, rmse: 2.3807790279388428, mae: 1.971176266670227\n",
      "Epoch 12 train loss: 5.805677485255013, rmse: 2.409543991088867, mae: 1.97300124168396; Epoch 12 valid loss: 5.68980733715758, rmse: 2.38545560836792, mae: 1.966158390045166\n",
      "Epoch 13 train loss: 5.856381542070777, rmse: 2.4200148582458496, mae: 1.9828343391418457; Epoch 13 valid loss: 5.975192734173366, rmse: 2.445129156112671, mae: 1.9518733024597168\n",
      "Epoch 14 train loss: 5.760627260039338, rmse: 2.40016770362854, mae: 1.9585793018341064; Epoch 14 valid loss: 5.590773156711033, rmse: 2.3650150299072266, mae: 1.9201580286026\n",
      "Best model found! Loss: 5.590773156711033\n",
      "Epoch 15 train loss: 5.742260121666225, rmse: 2.3963472843170166, mae: 1.9578332901000977; Epoch 15 valid loss: 5.633085752020077, rmse: 2.373892307281494, mae: 1.9310554265975952\n",
      "Epoch 16 train loss: 5.71174426796162, rmse: 2.3899481296539307, mae: 1.9477673768997192; Epoch 16 valid loss: 5.620034643581936, rmse: 2.371180534362793, mae: 1.9086997509002686\n",
      "Epoch 17 train loss: 5.686694891895868, rmse: 2.3846960067749023, mae: 1.9424667358398438; Epoch 17 valid loss: 5.58494418981124, rmse: 2.363651752471924, mae: 1.9114372730255127\n",
      "Best model found! Loss: 5.58494418981124\n",
      "Epoch 18 train loss: 5.670823307691422, rmse: 2.381378650665283, mae: 1.9400668144226074; Epoch 18 valid loss: 5.571526987212045, rmse: 2.3609838485717773, mae: 1.8906440734863281\n",
      "Best model found! Loss: 5.571526987212045\n",
      "Epoch 19 train loss: 5.6588205965219345, rmse: 2.3788933753967285, mae: 1.9350745677947998; Epoch 19 valid loss: 5.460497121421659, rmse: 2.337120294570923, mae: 1.907657504081726\n",
      "Best model found! Loss: 5.460497121421659\n",
      "Epoch 20 train loss: 5.6301978066959215, rmse: 2.37280535697937, mae: 1.9300698041915894; Epoch 20 valid loss: 5.914474178333672, rmse: 2.4326558113098145, mae: 1.959164023399353\n",
      "Epoch 21 train loss: 5.596541390587798, rmse: 2.3657236099243164, mae: 1.922722578048706; Epoch 21 valid loss: 5.55241503764172, rmse: 2.3568832874298096, mae: 1.9079723358154297\n",
      "Epoch 22 train loss: 5.580860404314193, rmse: 2.3624284267425537, mae: 1.9192980527877808; Epoch 22 valid loss: 5.513344171095867, rmse: 2.3487319946289062, mae: 1.8828778266906738\n",
      "Epoch 23 train loss: 5.561263294979534, rmse: 2.3582417964935303, mae: 1.913390874862671; Epoch 23 valid loss: 5.615526948656354, rmse: 2.3707289695739746, mae: 1.8971858024597168\n",
      "Epoch 24 train loss: 5.538404432229236, rmse: 2.3534185886383057, mae: 1.9079290628433228; Epoch 24 valid loss: 5.593515381521108, rmse: 2.3661515712738037, mae: 1.8814880847930908\n",
      "Epoch 25 train loss: 5.510878609134033, rmse: 2.347576379776001, mae: 1.9016033411026; Epoch 25 valid loss: 5.579961562643246, rmse: 2.3630831241607666, mae: 1.8891977071762085\n",
      "Epoch 26 train loss: 5.484922290586792, rmse: 2.3420441150665283, mae: 1.8944942951202393; Epoch 26 valid loss: 5.746499683175768, rmse: 2.3983561992645264, mae: 1.885026216506958\n",
      "Epoch 27 train loss: 5.483082630571011, rmse: 2.3416225910186768, mae: 1.8975324630737305; Epoch 27 valid loss: 5.482449830794821, rmse: 2.3425145149230957, mae: 1.8844062089920044\n",
      "Epoch 28 train loss: 5.481111445891119, rmse: 2.3411991596221924, mae: 1.894838809967041; Epoch 28 valid loss: 5.65271731298797, rmse: 2.378593921661377, mae: 1.907228946685791\n",
      "Epoch 29 train loss: 5.449527368397839, rmse: 2.3344626426696777, mae: 1.8894070386886597; Epoch 29 valid loss: 5.515175953203318, rmse: 2.3491404056549072, mae: 1.8824914693832397\n",
      "Epoch 30 train loss: 5.421251563266315, rmse: 2.328406572341919, mae: 1.878890037536621; Epoch 30 valid loss: 5.502031917474707, rmse: 2.346547842025757, mae: 1.8799325227737427\n",
      "Epoch 31 train loss: 5.429663828094449, rmse: 2.330198049545288, mae: 1.884876012802124; Epoch 31 valid loss: 5.638532099675159, rmse: 2.375387668609619, mae: 1.9180842638015747\n",
      "Epoch 32 train loss: 5.424349422053953, rmse: 2.328996181488037, mae: 1.8815743923187256; Epoch 32 valid loss: 5.99789745710334, rmse: 2.4503560066223145, mae: 1.9165018796920776\n",
      "Epoch 33 train loss: 5.3676573500169065, rmse: 2.3168282508850098, mae: 1.8662205934524536; Epoch 33 valid loss: 5.8015183575299325, rmse: 2.409656047821045, mae: 1.8784410953521729\n",
      "Epoch 34 train loss: 5.289277325689265, rmse: 2.299872636795044, mae: 1.8463712930679321; Epoch 34 valid loss: 5.623788806856895, rmse: 2.372603416442871, mae: 1.8575267791748047\n",
      "Epoch 35 train loss: 5.265836216496155, rmse: 2.2947723865509033, mae: 1.8428659439086914; Epoch 35 valid loss: 5.6792912288587925, rmse: 2.3843138217926025, mae: 1.872987985610962\n",
      "Epoch 36 train loss: 5.25732670357797, rmse: 2.292907476425171, mae: 1.84025239944458; Epoch 36 valid loss: 5.418429703128581, rmse: 2.328784942626953, mae: 1.8362172842025757\n",
      "Best model found! Loss: 5.418429703128581\n",
      "Epoch 37 train loss: 5.210197897505971, rmse: 2.28263258934021, mae: 1.8289299011230469; Epoch 37 valid loss: 5.312865301054352, rmse: 2.3061115741729736, mae: 1.8278865814208984\n",
      "Best model found! Loss: 5.312865301054352\n",
      "Epoch 38 train loss: 5.177985753738775, rmse: 2.275559425354004, mae: 1.8223319053649902; Epoch 38 valid loss: 5.683113253846461, rmse: 2.3850560188293457, mae: 1.8547734022140503\n",
      "Epoch 39 train loss: 5.109627983844386, rmse: 2.2605042457580566, mae: 1.8054147958755493; Epoch 39 valid loss: 5.498236787562468, rmse: 2.345797538757324, mae: 1.8457087278366089\n",
      "Epoch 40 train loss: 5.125164150453247, rmse: 2.2639107704162598, mae: 1.8063150644302368; Epoch 40 valid loss: 5.228084736940812, rmse: 2.287256956100464, mae: 1.8154584169387817\n",
      "Best model found! Loss: 5.228084736940812\n",
      "Epoch 41 train loss: 5.0981970060188155, rmse: 2.2579174041748047, mae: 1.8000953197479248; Epoch 41 valid loss: 5.390680850768576, rmse: 2.3228485584259033, mae: 1.8293812274932861\n",
      "Epoch 42 train loss: 5.083081555577506, rmse: 2.2546627521514893, mae: 1.797478199005127; Epoch 42 valid loss: 5.578088234881966, rmse: 2.3630120754241943, mae: 1.8314340114593506\n",
      "Epoch 43 train loss: 5.020550851273326, rmse: 2.240741014480591, mae: 1.7834498882293701; Epoch 43 valid loss: 5.542303800582886, rmse: 2.3553338050842285, mae: 1.8473920822143555\n",
      "Epoch 44 train loss: 5.054173450132387, rmse: 2.2481884956359863, mae: 1.7898238897323608; Epoch 44 valid loss: 5.19545146518824, rmse: 2.280529737472534, mae: 1.8086814880371094\n",
      "Best model found! Loss: 5.19545146518824\n",
      "Epoch 45 train loss: 4.966719508382072, rmse: 2.228644847869873, mae: 1.769689917564392; Epoch 45 valid loss: 5.202413425153615, rmse: 2.2821104526519775, mae: 1.8151510953903198\n",
      "Epoch 46 train loss: 4.952397542189708, rmse: 2.225435256958008, mae: 1.7653393745422363; Epoch 46 valid loss: 5.170142305140593, rmse: 2.2744812965393066, mae: 1.780599594116211\n",
      "Best model found! Loss: 5.170142305140593\n",
      "Epoch 47 train loss: 4.903471782988151, rmse: 2.214416742324829, mae: 1.7546255588531494; Epoch 47 valid loss: 5.41146644889092, rmse: 2.3273091316223145, mae: 1.8011904954910278\n",
      "Epoch 48 train loss: 4.886662028743102, rmse: 2.210602283477783, mae: 1.7523702383041382; Epoch 48 valid loss: 5.595371294994743, rmse: 2.3667306900024414, mae: 1.8284580707550049\n",
      "Epoch 49 train loss: 4.895012055878091, rmse: 2.212468385696411, mae: 1.7545844316482544; Epoch 49 valid loss: 5.424379409575949, rmse: 2.3303048610687256, mae: 1.8046643733978271\n",
      "Epoch 50 train loss: 4.84371538985092, rmse: 2.200895071029663, mae: 1.7421599626541138; Epoch 50 valid loss: 5.572958960825083, rmse: 2.362081527709961, mae: 1.811216115951538\n",
      "Epoch 51 train loss: 4.787177119212868, rmse: 2.1880390644073486, mae: 1.727722406387329; Epoch 51 valid loss: 5.560657661788317, rmse: 2.359513521194458, mae: 1.8009828329086304\n",
      "Epoch 52 train loss: 4.82674859445707, rmse: 2.1970019340515137, mae: 1.738289475440979; Epoch 52 valid loss: 5.320175195226864, rmse: 2.307589530944824, mae: 1.785744071006775\n",
      "Epoch 53 train loss: 4.748758643865585, rmse: 2.1791255474090576, mae: 1.7194839715957642; Epoch 53 valid loss: 5.840684793433365, rmse: 2.4183309078216553, mae: 1.8595861196517944\n",
      "Epoch 54 train loss: 4.799142417971011, rmse: 2.190713882446289, mae: 1.7307713031768799; Epoch 54 valid loss: 6.024377479845164, rmse: 2.4560511112213135, mae: 1.8903425931930542\n",
      "Epoch 55 train loss: 4.843175121324252, rmse: 2.200732707977295, mae: 1.7419989109039307; Epoch 55 valid loss: 5.875269262158141, rmse: 2.425187587738037, mae: 1.8658461570739746\n",
      "Epoch 56 train loss: 4.854643914446367, rmse: 2.2033209800720215, mae: 1.7438997030258179; Epoch 56 valid loss: 5.883396657145753, rmse: 2.4272756576538086, mae: 1.8612531423568726\n",
      "Epoch 57 train loss: 4.795981052386022, rmse: 2.189969778060913, mae: 1.7298378944396973; Epoch 57 valid loss: 5.370525110741051, rmse: 2.3183753490448, mae: 1.7871038913726807\n",
      "Epoch 58 train loss: 4.738394799063691, rmse: 2.176791191101074, mae: 1.7187223434448242; Epoch 58 valid loss: 6.109771048536106, rmse: 2.4735453128814697, mae: 1.8967492580413818\n",
      "Epoch 59 train loss: 4.758443841206289, rmse: 2.1814236640930176, mae: 1.720717191696167; Epoch 59 valid loss: 5.700496077537537, rmse: 2.3889877796173096, mae: 1.8346434831619263\n",
      "Epoch 60 train loss: 4.719822158855674, rmse: 2.1725027561187744, mae: 1.7139720916748047; Epoch 60 valid loss: 6.190152231527835, rmse: 2.4898900985717773, mae: 1.914730191230774\n",
      "Epoch 61 train loss: 4.729774057548658, rmse: 2.1748130321502686, mae: 1.7125020027160645; Epoch 61 valid loss: 5.341087185606664, rmse: 2.312469720840454, mae: 1.7782906293869019\n",
      "Epoch 62 train loss: 4.703425980563712, rmse: 2.1686923503875732, mae: 1.708516240119934; Epoch 62 valid loss: 5.315766344265062, rmse: 2.306617021560669, mae: 1.7929058074951172\n",
      "Epoch 63 train loss: 4.6863223129669125, rmse: 2.1648166179656982, mae: 1.7051656246185303; Epoch 63 valid loss: 5.3286630912702915, rmse: 2.30997896194458, mae: 1.7661349773406982\n",
      "Epoch 64 train loss: 4.62329076866133, rmse: 2.15018367767334, mae: 1.6889967918395996; Epoch 64 valid loss: 5.820789342023889, rmse: 2.4136159420013428, mae: 1.8549834489822388\n",
      "Epoch 65 train loss: 4.658817033008137, rmse: 2.1584115028381348, mae: 1.7020142078399658; Epoch 65 valid loss: 5.4261431900822386, rmse: 2.330064296722412, mae: 1.8006460666656494\n",
      "Epoch 66 train loss: 4.6256643860741, rmse: 2.150703191757202, mae: 1.6915696859359741; Epoch 66 valid loss: 5.488996651707863, rmse: 2.3444178104400635, mae: 1.809834361076355\n",
      "Epoch 67 train loss: 4.583178968767149, rmse: 2.1408638954162598, mae: 1.6790310144424438; Epoch 67 valid loss: 5.44808554649353, rmse: 2.3358097076416016, mae: 1.7906639575958252\n",
      "Epoch 68 train loss: 4.586642483061394, rmse: 2.141629934310913, mae: 1.6787320375442505; Epoch 68 valid loss: 5.622646513033886, rmse: 2.3729445934295654, mae: 1.809532642364502\n",
      "Epoch 69 train loss: 4.525526977666711, rmse: 2.127296209335327, mae: 1.6671464443206787; Epoch 69 valid loss: 5.461719028803767, rmse: 2.3384461402893066, mae: 1.7931097745895386\n",
      "Epoch 70 train loss: 4.535345847311273, rmse: 2.129638433456421, mae: 1.6688367128372192; Epoch 70 valid loss: 5.5758195215342, rmse: 2.3628647327423096, mae: 1.8424986600875854\n",
      "Epoch 71 train loss: 4.544358146929108, rmse: 2.1317691802978516, mae: 1.6704014539718628; Epoch 71 valid loss: 5.171866268527751, rmse: 2.275360584259033, mae: 1.7479907274246216\n",
      "Epoch 72 train loss: 4.530630625085493, rmse: 2.1285276412963867, mae: 1.6664602756500244; Epoch 72 valid loss: 6.371578666628624, rmse: 2.5254244804382324, mae: 1.9003890752792358\n",
      "Epoch 73 train loss: 4.507507162600492, rmse: 2.123063325881958, mae: 1.6602925062179565; Epoch 73 valid loss: 5.701133119816682, rmse: 2.389261245727539, mae: 1.809790015220642\n",
      "Epoch 74 train loss: 4.487130797278565, rmse: 2.1183202266693115, mae: 1.6567038297653198; Epoch 74 valid loss: 5.874895696737329, rmse: 2.425145149230957, mae: 1.8364429473876953\n",
      "Epoch 75 train loss: 4.461726366418652, rmse: 2.11226224899292, mae: 1.6518640518188477; Epoch 75 valid loss: 5.527857678277152, rmse: 2.3524153232574463, mae: 1.7860684394836426\n",
      "Epoch 76 train loss: 4.4035884264296135, rmse: 2.0984373092651367, mae: 1.6390111446380615; Epoch 76 valid loss: 5.745335758948813, rmse: 2.3979716300964355, mae: 1.8432062864303589\n",
      "Epoch 77 train loss: 4.393493412975716, rmse: 2.096021890640259, mae: 1.6381185054779053; Epoch 77 valid loss: 5.489066907337734, rmse: 2.3444440364837646, mae: 1.8019044399261475\n",
      "Epoch 78 train loss: 4.345482028796609, rmse: 2.084561347961426, mae: 1.626481056213379; Epoch 78 valid loss: 5.636919067830456, rmse: 2.3753209114074707, mae: 1.7986750602722168\n",
      "Epoch 79 train loss: 4.405497899572406, rmse: 2.098776340484619, mae: 1.6359965801239014; Epoch 79 valid loss: 5.909225529553939, rmse: 2.4314329624176025, mae: 1.8385330438613892\n",
      "Epoch 80 train loss: 4.372677084777208, rmse: 2.091036558151245, mae: 1.633982539176941; Epoch 80 valid loss: 5.807745639158755, rmse: 2.410832166671753, mae: 1.8364604711532593\n",
      "Epoch 81 train loss: 4.302538176184207, rmse: 2.074258804321289, mae: 1.612940788269043; Epoch 81 valid loss: 5.307889133083577, rmse: 2.3039591312408447, mae: 1.7637709379196167\n",
      "Epoch 82 train loss: 4.280000912088209, rmse: 2.0688095092773438, mae: 1.6069577932357788; Epoch 82 valid loss: 5.512008204752085, rmse: 2.3480968475341797, mae: 1.7767210006713867\n",
      "Epoch 83 train loss: 4.324067765368825, rmse: 2.0794546604156494, mae: 1.6205167770385742; Epoch 83 valid loss: 5.910947914026221, rmse: 2.431873083114624, mae: 1.8387941122055054\n",
      "Epoch 84 train loss: 4.278930283436733, rmse: 2.0686028003692627, mae: 1.6088595390319824; Epoch 84 valid loss: 5.984011978519206, rmse: 2.4471023082733154, mae: 1.8438255786895752\n",
      "Epoch 85 train loss: 4.278016454297885, rmse: 2.0682268142700195, mae: 1.6057199239730835; Epoch 85 valid loss: 5.492267202357857, rmse: 2.3444831371307373, mae: 1.7825788259506226\n",
      "Epoch 86 train loss: 4.274107131599325, rmse: 2.0673725605010986, mae: 1.6078263521194458; Epoch 86 valid loss: 5.732951434291139, rmse: 2.395064115524292, mae: 1.8248600959777832\n",
      "Epoch 87 train loss: 4.251160230415057, rmse: 2.061833143234253, mae: 1.6032555103302002; Epoch 87 valid loss: 5.215773954683421, rmse: 2.2841174602508545, mae: 1.7666865587234497\n",
      "Epoch 88 train loss: 4.22855325103861, rmse: 2.056421995162964, mae: 1.5991910696029663; Epoch 88 valid loss: 5.608022018354767, rmse: 2.368166446685791, mae: 1.8032276630401611\n",
      "Epoch 89 train loss: 4.237895296790959, rmse: 2.0586659908294678, mae: 1.5976883172988892; Epoch 89 valid loss: 5.536567885048536, rmse: 2.353308916091919, mae: 1.7925705909729004\n",
      "Epoch 90 train loss: 4.200881783360929, rmse: 2.0496840476989746, mae: 1.5922410488128662; Epoch 90 valid loss: 5.478992634890031, rmse: 2.340999126434326, mae: 1.781551718711853\n",
      "Epoch 91 train loss: 4.168629638283654, rmse: 2.0417816638946533, mae: 1.586014747619629; Epoch 91 valid loss: 5.6290631123951504, rmse: 2.3724684715270996, mae: 1.7928239107131958\n",
      "Epoch 92 train loss: 4.149226367157118, rmse: 2.0370466709136963, mae: 1.5789270401000977; Epoch 92 valid loss: 5.655807718938711, rmse: 2.37823748588562, mae: 1.8089909553527832\n",
      "Epoch 93 train loss: 4.201266027024362, rmse: 2.049736261367798, mae: 1.5902262926101685; Epoch 93 valid loss: 5.852409070851851, rmse: 2.4205167293548584, mae: 1.83092200756073\n",
      "Epoch 94 train loss: 4.118979973603139, rmse: 2.029628038406372, mae: 1.5693645477294922; Epoch 94 valid loss: 5.5283123425074985, rmse: 2.35132098197937, mae: 1.7969025373458862\n",
      "Epoch 95 train loss: 4.166424349345992, rmse: 2.0412516593933105, mae: 1.5833009481430054; Epoch 95 valid loss: 5.347978416754275, rmse: 2.3131766319274902, mae: 1.796465277671814\n",
      "Epoch 96 train loss: 4.10129744679527, rmse: 2.0251364707946777, mae: 1.5689313411712646; Epoch 96 valid loss: 5.430064723199727, rmse: 2.3303372859954834, mae: 1.7973684072494507\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "train_fst(n_epochs, train_dataloader2, test_dataloader2, best_model_path, early_stopping_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 5.170142305140593, rmse 2.2744812965393066, mae 1.780599594116211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.170142305140593, 2.2744812965393066, 1.780599594116211)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "def rmse(predictions, targets):\n",
    "    return torch.sqrt(F.mse_loss(predictions, targets))\n",
    "\n",
    "def mae(predictions, targets):\n",
    "    return torch.mean(torch.abs(predictions - targets))\n",
    "\n",
    "def test2(test_dataloader):\n",
    "    best_model = RNN_Fst(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n",
    "    best_model.load_state_dict(torch.load(best_model_path))\n",
    "    best_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        best_model.eval()\n",
    "        valid_losses = []\n",
    "\n",
    "        trues = torch.tensor([]).to(device)\n",
    "        preds = torch.tensor([]).to(device)\n",
    "        \n",
    "        for i, (features, labels) in enumerate(test_dataloader):\n",
    "            \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = best_model(features)\n",
    "\n",
    "            trues = torch.cat((trues, labels), axis=0)\n",
    "            preds = torch.cat((preds, out.squeeze().detach()), axis=0)\n",
    "            \n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        rmse_error = rmse(preds, trues)\n",
    "        mae_error = mae(preds, trues)\n",
    "\n",
    "        print(f'valid loss {valid_loss}, rmse {rmse_error}, mae {mae_error}')\n",
    "        \n",
    "        return valid_loss, rmse_error.item(), mae_error.item()\n",
    "        \n",
    "test2(test_dataloader2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "save_path = \"../model\"\n",
    "best_model_path = os.path.join(save_path, 'lte_HO_cls_RNN.pt')\n",
    "torch.save(classifier.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# m_path = os.path.join('/home/wmnlab/Documents/sheng-ru/model', 'lte_HO_cls_RNN.pt')\n",
    "# classifier = RNN(input_dim, out_dim, hidden_dim, num_layers, dropout, rnn)\n",
    "# classifier.load_state_dict(torch.load(m_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheng-ru",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7771dd1fbefba0f9e49b3f12d6cb05ea3fc9d8cb4bbb591d0ecb9d07210ade7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
