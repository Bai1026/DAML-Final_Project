{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"c7771dd1fbefba0f9e49b3f12d6cb05ea3fc9d8cb4bbb591d0ecb9d07210ade7"}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8514003,"sourceType":"datasetVersion","datasetId":5082826},{"sourceId":8514030,"sourceType":"datasetVersion","datasetId":5082838},{"sourceId":8514360,"sourceType":"datasetVersion","datasetId":5083097},{"sourceId":8530765,"sourceType":"datasetVersion","datasetId":5094875},{"sourceId":8532827,"sourceType":"datasetVersion","datasetId":5096411},{"sourceId":8533167,"sourceType":"datasetVersion","datasetId":5096664}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\n# from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\nfrom sklearn.metrics import auc, precision_recall_curve\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n# from sklearn.metrics import mean_squared_error, mean_absolute_error\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom torch import nn, optim\n\nimport random\nimport copy\n\nplt.style.use('fivethirtyeight')","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:26:24.068907Z","iopub.execute_input":"2024-05-27T17:26:24.071336Z","iopub.status.idle":"2024-05-27T17:26:24.078227Z","shell.execute_reply.started":"2024-05-27T17:26:24.071303Z","shell.execute_reply":"2024-05-27T17:26:24.077359Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"!cat ~/user_state","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:26:24.079809Z","iopub.execute_input":"2024-05-27T17:26:24.080190Z","iopub.status.idle":"2024-05-27T17:26:25.050326Z","shell.execute_reply.started":"2024-05-27T17:26:24.080145Z","shell.execute_reply":"2024-05-27T17:26:25.049347Z"},"trusted":true},"execution_count":108,"outputs":[{"name":"stdout","text":"cat: /root/user_state: No such file or directory\n","output_type":"stream"}]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:26:25.051926Z","iopub.execute_input":"2024-05-27T17:26:25.052358Z","iopub.status.idle":"2024-05-27T17:26:26.107850Z","shell.execute_reply.started":"2024-05-27T17:26:25.052327Z","shell.execute_reply":"2024-05-27T17:26:26.106722Z"},"trusted":true},"execution_count":109,"outputs":[{"name":"stdout","text":"Mon May 27 17:26:25 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   45C    P0              26W /  70W |    275MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n| N/A   36C    P8               8W /  70W |      3MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"code","source":"def ts_array_create(dirname, dir_list, time_seq):\n    \n    columns = ['RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2',\n               'nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1', 'nr-RSRP2', 'nr-RSRQ2']\n    \n    def reamin_HO_time(y_train):\n        def f(L):    \n            for i, e in enumerate(L):\n                if e: return i+1\n            return 0\n\n        out = []\n        for a2 in y_train:\n            a1_out = []\n            for a1 in a2:\n                a1_out.append(a1.any())\n      \n            out.append(f(a1_out))\n        return out\n    \n    def HO(y_train):\n        out = []\n        for a2 in y_train:\n            if sum(a2.reshape(-1)) == 0: ho = 0\n            elif sum(a2.reshape(-1)) > 0: ho = 1\n            out.append(ho)\n        return out\n\n    split_time = []\n    X_final = np.array([])\n    Y_final = np.array([])\n    for i, f in enumerate(tqdm(dir_list)):\n    \n        f = os.path.join(dirname, f)\n        df = pd.read_csv(f)\n\n        # preprocess data with ffill method\n        del df['Timestamp'], df['lat'], df['long'], df['gpsspeed']\n        # df[columns] = df[columns].replace(0, np.nan)\n        # df[columns] = df[columns].fillna(method='ffill')\n        # df.dropna(inplace=True)\n        \n        df.replace(np.nan,0,inplace=True); df.replace('-',0,inplace=True)\n        \n        X = df[features]\n        Y = df[target]\n        \n        Xt_list = []\n        Yt_list = []\n\n        for j in range(time_seq):\n            X_t = X.shift(periods=-j)\n            Xt_list.append(X_t)\n    \n        for j in range(time_seq,time_seq+predict_t):\n            Y_t = Y.shift(periods=-(j))\n            Yt_list.append(Y_t)\n\n        # YY = Y.shift(periods=-(0))\n\n        X_ts = np.array(Xt_list); X_ts = np.transpose(X_ts, (1,0,2)); X_ts = X_ts[:-(time_seq+predict_t-1),:,:]\n        Y_ts = np.array(Yt_list); Y_ts = np.transpose(Y_ts, (1,0,2)); Y_ts = Y_ts[:-(time_seq+predict_t-1),:,:]\n        split_time.append(len(X_ts))\n\n        if i == 0 or X_final.size == 0:\n            X_final = X_ts\n            Y_final = Y_ts\n            print(X_final)\n        else:\n            X_final = np.concatenate((X_final,X_ts), axis=0)\n            Y_final = np.concatenate((Y_final,Y_ts), axis=0)\n\n    split_time = [(sum(split_time[:i]), sum(split_time[:i])+x) for i, x in enumerate(split_time)]\n    \n    return X_final, np.array(HO(Y_final)), np.array(reamin_HO_time(Y_final)), split_time # forecast HO\n#     return split_time, np.array(HO(Y_final)), np.array(reamin_HO_time(Y_final)), split_time\n\nclass RNN_Dataset_simple(Dataset):\n    \"\"\"\n    Dataset take all csv file specified in dir_list in directory dirname.\n    Transfer csvs to (features, label) pair\n\n    \"\"\"\n    def __init__(self, X, y):\n\n        self.inputs = torch.FloatTensor(X)\n        self.labels = torch.FloatTensor(y)\n        \n    def __len__(self):\n        \n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        \n        data = self.inputs[idx]\n        label = self.labels[idx]\n        \n        return data, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def days_in_file(file, dates):\n    \n    for date in dates:\n        if date in file: return True \n    return False\n\ndef train_valid_split(L, valid_size=0.2):\n    \n    length = len(L)\n    v_num = int(length*valid_size)\n    v_files = random.sample(L, v_num)\n    t_files = list(set(L) - set(v_files))\n    \n    return t_files, v_files","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:26:26.131693Z","iopub.execute_input":"2024-05-27T17:26:26.132141Z","iopub.status.idle":"2024-05-27T17:26:26.145168Z","shell.execute_reply.started":"2024-05-27T17:26:26.132101Z","shell.execute_reply":"2024-05-27T17:26:26.144342Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \n    # Set a fixed value for the hash seed\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:2\"\n    \n    print(f\"Random seed set as {seed}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:26:26.146273Z","iopub.execute_input":"2024-05-27T17:26:26.146615Z","iopub.status.idle":"2024-05-27T17:26:26.160347Z","shell.execute_reply.started":"2024-05-27T17:26:26.146590Z","shell.execute_reply":"2024-05-27T17:26:26.159544Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"# Time sequence length and prediction time length\nseed = 55688\ntime_seq = 20\npredict_t = 10\nvalid_ratio = 0.2\ntask = 'classification'\n\nbatch_size = 32","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:26:26.161469Z","iopub.execute_input":"2024-05-27T17:26:26.161810Z","iopub.status.idle":"2024-05-27T17:26:26.170773Z","shell.execute_reply.started":"2024-05-27T17:26:26.161777Z","shell.execute_reply":"2024-05-27T17:26:26.169971Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"# Setup seed\nset_seed(seed)\n\n# Get GPU\ndevice_count = torch.cuda.device_count()\nnum_of_gpus = device_count\n\nfor i in range(device_count):\n    print(\"GPU {}: {}\".format(i, torch.cuda.get_device_name(i)))\n    gpu_id = i\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Save best model to \nsave_path = \"/kaggle/working\"\n\n# Define DataSet\ndirname = \"/kaggle/input/111111\"\ndir_list = os.listdir(dirname)\ndir_list = [f for f in dir_list if ( f.endswith('.csv') and (not 'sm' in f) ) ]\nprint(dir_list)\n\n# train_dates = ['03-26','04-01','04-10','04-17']\n# test_dates = ['05-07']\ntrain_dates = ['03-26']\ntest_dates = ['04-01']\n    \n# train_dir_list = [f for f in dir_list if ( f.endswith('.csv') and ('All' in f) and days_in_file(f, train_dates) )]\n# test_dir_list = [f for f in dir_list if ( f.endswith('.csv') and ('All' in f) and days_in_file(f, test_dates) )]\n\ntrain_dir_list, test_dir_list = train_valid_split(dir_list, valid_ratio)\ntrain_dir_list += [f for f in os.listdir(dirname) if 'sm' in f]\n\n# features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n#         'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2',\n#         'nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1', 'nr-RSRP2', 'nr-RSRQ2' ]\nfeatures = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n        'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1','nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1']\n# features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n#         'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2']\n\nnum_of_features = len(features)\n\n# target = ['LTE_HO', 'MN_HO'] # For eNB HO.\n# target = ['eNB_to_ENDC'] # Setup gNB\ntarget = ['gNB_Rel', 'gNB_HO'] # For gNB HO.\n# target = ['RLF'] # For RLF\n# target = ['SCG_RLF'] # For scg failure\n# target = ['dl-loss'] # For DL loss\n# target = ['ul-loss'] # For UL loss\n\n# Data\nprint('Loading training data...')\nX_train, y_train1, y_train2, split_time_train = ts_array_create(dirname, train_dir_list, time_seq)\n\ntrain_dataset = RNN_Dataset_simple(X_train, y_train1)\ntrain_dataloader1 = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n\nprint(X_train.shape)\nprint(y_train2.shape)\nprint(X_train)\nprint(y_train2)\n\ncond = y_train2 > 0\nX_train_fore = X_train[cond]\ny_train2_fore = y_train2[cond]\ntrain_dataset = RNN_Dataset_simple(X_train_fore, y_train2_fore)\ntrain_dataloader2 = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n\nprint(X_train_fore.shape)\nprint(y_train2_fore.shape)\nprint(X_train_fore)\nprint(y_train2_fore)\n\nprint('Loading testing data...')\nX_test, y_test1, y_test2, split_time_test = ts_array_create(dirname, test_dir_list, time_seq)\n\ntest_dataset = RNN_Dataset_simple(X_test, y_test1)\ntest_dataloader1 = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\ncond = y_test2 > 0\nX_test_fore = X_test[cond]\ny_test2_fore = y_test2[cond]\ntest_dataset = RNN_Dataset_simple(X_test_fore, y_test2_fore)\ntest_dataloader2 = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:26:26.171979Z","iopub.execute_input":"2024-05-27T17:26:26.172295Z","iopub.status.idle":"2024-05-27T17:26:26.634302Z","shell.execute_reply.started":"2024-05-27T17:26:26.172265Z","shell.execute_reply":"2024-05-27T17:26:26.633382Z"},"trusted":true},"execution_count":114,"outputs":[{"name":"stdout","text":"Random seed set as 55688\nGPU 0: Tesla T4\nGPU 1: Tesla T4\n['2023-03-26_qc03_02_All.csv', '2023-03-26_qc00_01_All.csv', '2023-04-01_qc00_exp1_03_B1.csv', '2023-03-26_qc02_03_All.csv', '2023-04-01_qc00_exp1_01_B1.csv', '2023-03-26_qc02_01_All.csv', '2023-03-26_qc03_01_All.csv', '2023-04-01_qc00_exp1_04_B1.csv', '2023-04-01_qc00_exp1_02_B1.csv', '2023-03-26_qc03_03_All.csv', '2023-04-01_qc00_exp1_06_B1.csv', '2023-03-26_qc00_03_All.csv', '2023-03-26_qc00_02_All.csv', '2023-03-26_qc02_02_All.csv', '2023-04-01_qc00_exp1_05_B1.csv']\nLoading training data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85fec31c2b964ac99ff3fa60fe7d7764"}},"metadata":{}},{"name":"stdout","text":"[[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n ...\n\n [[1. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [1. 0. 0. ... 0. 0. 0.]]]\n(3144, 20, 16)\n(3144,)\n[[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n ...\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]]\n[0 0 0 ... 0 0 0]\n(645, 20, 16)\n(645,)\n[[[   0.            0.            0.         ...  -14.0885\n    -79.69533333  -15.49316667]\n  [   0.            0.            0.         ...  -15.83271429\n    -83.50342857  -15.83142857]\n  [   0.            0.            0.         ...  -16.9805\n    -81.04816667  -14.33583333]\n  ...\n  [   0.            0.            0.         ...  -15.02466667\n    -89.44666667  -15.543     ]\n  [   0.            0.            0.         ...  -15.125\n    -91.16742857  -15.75671429]\n  [   0.            0.            0.         ...  -14.9115\n    -91.0715      -14.92183333]]\n\n [[   0.            0.            0.         ...  -15.83271429\n    -83.50342857  -15.83142857]\n  [   0.            0.            0.         ...  -16.9805\n    -81.04816667  -14.33583333]\n  [   0.            0.            0.         ...  -17.71866667\n    -83.10416667  -16.17966667]\n  ...\n  [   0.            0.            0.         ...  -15.125\n    -91.16742857  -15.75671429]\n  [   0.            0.            0.         ...  -14.9115\n    -91.0715      -14.92183333]\n  [   0.            0.            0.         ...  -14.944\n    -91.479       -15.03666667]]\n\n [[   0.            0.            0.         ...  -16.9805\n    -81.04816667  -14.33583333]\n  [   0.            0.            0.         ...  -17.71866667\n    -83.10416667  -16.17966667]\n  [   0.            0.            0.         ...  -17.42833333\n    -81.34383333  -14.09883333]\n  ...\n  [   0.            0.            0.         ...  -14.9115\n    -91.0715      -14.92183333]\n  [   0.            0.            0.         ...  -14.944\n    -91.479       -15.03666667]\n  [   0.            0.            0.         ...  -14.077\n    -91.76166667  -15.70833333]]\n\n ...\n\n [[   0.            0.            0.         ...  -11.38666667\n   -103.49733333  -23.57066667]\n  [   0.            1.            0.         ...  -10.80616667\n   -116.89583333  -29.11333333]\n  [   0.            0.            0.         ...  -10.5956\n   -139.89066667  -37.90066667]\n  ...\n  [   0.            0.            0.         ...  -13.8385\n    -75.09616667  -11.81633333]\n  [   0.            0.            0.         ...  -13.51183333\n    -83.63916667  -13.07683333]\n  [   0.            0.            0.         ...  -13.32416667\n    -84.551       -14.82016667]]\n\n [[   0.            1.            0.         ...  -10.80616667\n   -116.89583333  -29.11333333]\n  [   0.            0.            0.         ...  -10.5956\n   -139.89066667  -37.90066667]\n  [   0.            1.            1.         ...  -10.8595\n    -73.008       -10.508     ]\n  ...\n  [   0.            0.            0.         ...  -13.51183333\n    -83.63916667  -13.07683333]\n  [   0.            0.            0.         ...  -13.32416667\n    -84.551       -14.82016667]\n  [   0.            0.            0.         ...  -12.53885714\n    -87.624       -15.47657143]]\n\n [[   0.            0.            0.         ...  -10.5956\n   -139.89066667  -37.90066667]\n  [   0.            1.            1.         ...  -10.8595\n    -73.008       -10.508     ]\n  [   0.            0.            0.         ...  -12.612\n    -77.4295      -14.448     ]\n  ...\n  [   0.            0.            0.         ...  -13.32416667\n    -84.551       -14.82016667]\n  [   0.            0.            0.         ...  -12.53885714\n    -87.624       -15.47657143]\n  [   0.            0.            0.         ...  -15.06133333\n    -83.41683333  -13.26816667]]]\n[10  9  8  7  6  5  4  3  2  1  5  4  3  2  1  9  8  7  6  5  4  3  2  1\n 10  9  8  7  6  5  4  3  2  1  2  1 10  9  8  7  6  5  4  3  2  1  2  1\n 10  9  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1 10  9  8  7\n  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1  8  7  6  5  4  3  2  1\n 10  9  8  7  6  5  4  3  2  1  3  2 10  9  8  7  6  5  4  3  2  1  1 10\n  9  8  7  6  5  4  3  2  1  7  6  5  4  3  2  1  5  4  3  2  1  1  2  1\n 10  9  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1 10  9  8  7\n  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1  5  4  3  2  1  9  8  7\n  6  5  4  3  2  1  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1 10\n  9  8  7  6  5  4  3  2  1  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3\n  2  1  3  2  1 10  9  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2\n  1  6  5  4  3  2  1  6  5  4  3  2  1  2  1 10  9  8  7  6  5  4  3  2\n  1 10  9  8  7  6  5  4  3  2  1  8  7  6  5  4  3  2  1 10  9  8  7  6\n  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2\n  1 10  9  8  7  6  5  4  3  2  1  4  3  2  1  5  4  3  2  1 10  9  8  7\n  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1  7  6  5  4  3  2  1  6\n  5  4  3  2  1  3  2  1 10  9  8  7  6  5  4  3  2  1 10  9  8  7  6  5\n  4  3  2  1  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1 10  9\n  8  7  6  5  4  3  2  1  3  2  1 10  9  8  7  6  5  4  3  2  1  6  5  4\n  3  2  1  9  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1  7  6\n  5  4  3  2  1  6  5  4  3  2  1  1 10  9  8  7  6  5  4  3  2  1 10  9\n  8  7  6  5  4  3  2  1  9  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4\n  3  2  1 10  9  8  7  6  5  4  3  2  1  2  1 10  9  8  7  6  5  4  3  2\n  1  6  5  4  3  2  1  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1\n  3  2 10  9  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1 10  9\n  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1  7  6  5  4  3  2\n  1 10  9  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1]\nLoading testing data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dc58fcf3c324b4aacfec8dccf94da1c"}},"metadata":{}},{"name":"stdout","text":"[[[  0.           0.           0.         ... -12.78266667 -92.61966667\n   -17.3985    ]\n  [  0.           0.           0.         ... -14.3985     -92.38533333\n   -15.45166667]\n  [  0.           0.           0.         ... -15.61171429 -92.62828571\n   -15.78242857]\n  ...\n  [  0.           1.           0.         ... -13.69816667 -85.5715\n   -15.702     ]\n  [  0.           0.           0.         ... -15.00266667 -84.72533333\n   -14.63666667]\n  [  0.           0.           0.         ... -18.19971429 -82.97657143\n   -13.09028571]]\n\n [[  0.           0.           0.         ... -14.3985     -92.38533333\n   -15.45166667]\n  [  0.           0.           0.         ... -15.61171429 -92.62828571\n   -15.78242857]\n  [  0.           0.           0.         ... -14.5145     -95.198\n   -16.47016667]\n  ...\n  [  0.           0.           0.         ... -15.00266667 -84.72533333\n   -14.63666667]\n  [  0.           0.           0.         ... -18.19971429 -82.97657143\n   -13.09028571]\n  [  0.           0.           0.         ... -17.21733333 -88.53366667\n   -13.61716667]]\n\n [[  0.           0.           0.         ... -15.61171429 -92.62828571\n   -15.78242857]\n  [  0.           0.           0.         ... -14.5145     -95.198\n   -16.47016667]\n  [  0.           0.           0.         ... -14.87633333 -93.19\n   -16.92316667]\n  ...\n  [  0.           0.           0.         ... -18.19971429 -82.97657143\n   -13.09028571]\n  [  0.           0.           0.         ... -17.21733333 -88.53366667\n   -13.61716667]\n  [  0.           0.           0.         ... -17.8815     -88.539\n   -12.9245    ]]\n\n ...\n\n [[  0.           0.           0.         ... -11.63533333 -75.03633333\n   -13.4975    ]\n  [  0.           0.           0.         ... -12.08216667 -84.01683333\n   -15.38166667]\n  [  0.           0.           0.         ... -11.324      -78.6615\n   -17.43883333]\n  ...\n  [  0.           0.           0.         ... -13.77233333 -96.59916667\n   -15.806     ]\n  [  0.           0.           0.         ... -17.93483333 -97.48283333\n   -15.67966667]\n  [  0.           0.           0.         ... -18.43757143 -97.06014286\n   -16.11171429]]\n\n [[  0.           0.           0.         ... -12.08216667 -84.01683333\n   -15.38166667]\n  [  0.           0.           0.         ... -11.324      -78.6615\n   -17.43883333]\n  [  0.           0.           0.         ... -12.26328571 -74.28914286\n   -13.10371429]\n  ...\n  [  0.           0.           0.         ... -17.93483333 -97.48283333\n   -15.67966667]\n  [  0.           0.           0.         ... -18.43757143 -97.06014286\n   -16.11171429]\n  [  0.           0.           0.         ... -15.99066667 -97.19133333\n   -15.73166667]]\n\n [[  0.           0.           0.         ... -11.324      -78.6615\n   -17.43883333]\n  [  0.           0.           0.         ... -12.26328571 -74.28914286\n   -13.10371429]\n  [  0.           0.           0.         ... -12.79183333 -80.62233333\n   -13.96083333]\n  ...\n  [  0.           0.           0.         ... -18.43757143 -97.06014286\n   -16.11171429]\n  [  0.           0.           0.         ... -15.99066667 -97.19133333\n   -15.73166667]\n  [  0.           0.           0.         ... -15.8385     -96.36866667\n   -15.6835    ]]]\n","output_type":"stream"}]},{"cell_type":"code","source":"a,b = next(iter(train_dataloader1))\ninput_dim, out_dim = a.shape[2], 1\na.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:26:26.635388Z","iopub.execute_input":"2024-05-27T17:26:26.635660Z","iopub.status.idle":"2024-05-27T17:26:26.643026Z","shell.execute_reply.started":"2024-05-27T17:26:26.635635Z","shell.execute_reply":"2024-05-27T17:26:26.642186Z"},"trusted":true},"execution_count":115,"outputs":[{"execution_count":115,"output_type":"execute_result","data":{"text/plain":"torch.Size([32, 20, 16])"},"metadata":{}}]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class RNN_Cls(nn.Module):\n    '''\n    Using LSTM or GRU.\n    '''\n    def __init__(self, input_dim, out_dim, hidden_dim, num_layer, dropout, rnn):\n\n        super().__init__()\n        self.in_dim = input_dim\n        self.out_dim = out_dim\n        self.hid_dim = hidden_dim\n        self.num_layer = num_layer\n        self.dropout = dropout\n\n        # input_size: num of features; hidden_size: num of hidden state h\n        # num_layers: number of recurrent layer; seq; batch_first: batch first than seq\n        if rnn == 'LSTM':\n            self.rnn= nn.LSTM(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n        elif rnn == 'GRU':\n            self.rnn= nn.GRU(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n\n        self.linear = nn.Linear(hidden_dim, out_dim) # For binary classification\n\n    def forward(self,batch_input):\n\n        out,_ = self.rnn(batch_input)\n        out = self.linear(out[:,-1, :])  #Extract out of last time step (N, L, Hout) -> (Batch, time_seq, output)\n        \n        out = torch.sigmoid(out) # Binary Classifier\n\n        return out\n\nclass RNN_Fst(nn.Module):\n    '''\n    Using LSTM or GRU.\n    '''\n    def __init__(self, input_dim, out_dim, hidden_dim, num_layer, dropout, rnn):\n\n        super().__init__()\n        self.in_dim = input_dim\n        self.out_dim = out_dim\n        self.hid_dim = hidden_dim\n        self.num_layer = num_layer\n        self.dropout = dropout\n\n        # input_size: num of features; hidden_size: num of hidden state h\n        # num_layers: number of recurrent layer; seq; batch_first: batch first than seq\n        if rnn == 'LSTM':\n            self.rnn= nn.LSTM(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n        elif rnn == 'GRU':\n            self.rnn= nn.GRU(input_dim, hidden_dim, num_layer, batch_first=True, dropout=dropout)\n\n        self.linear = nn.Linear(hidden_dim, out_dim) # For binary classification\n\n    def forward(self,batch_input):\n\n        out,_ = self.rnn(batch_input)\n        out = self.linear(out[:,-1, :])  #Extract out of last time step (N, L, Hout) -> (Batch, time_seq, output)\n        out = torch.sigmoid(out) # Binary Classifier\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:26:26.647917Z","iopub.execute_input":"2024-05-27T17:26:26.648231Z","iopub.status.idle":"2024-05-27T17:26:26.661000Z","shell.execute_reply.started":"2024-05-27T17:26:26.648206Z","shell.execute_reply":"2024-05-27T17:26:26.660095Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters\nn_epochs = 600\nlr = 0.001\nbatch_size = 32\nhidden_dim = 128\nnum_layer = 2\ndropout = 0\n\n# rnn = 'GRU' # 'LSTM' or 'GRU'\nrnn = 'LSTM'","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:26:26.662068Z","iopub.execute_input":"2024-05-27T17:26:26.662335Z","iopub.status.idle":"2024-05-27T17:26:26.675113Z","shell.execute_reply.started":"2024-05-27T17:26:26.662312Z","shell.execute_reply":"2024-05-27T17:26:26.674145Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"set_seed(seed)\n# Define model and optimizer\n\nclassifier = RNN_Cls(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\noptimizer = optim.Adam(classifier.parameters(), lr=lr)\n\ncriterion = nn.BCELoss()\n# criterion = nn.MSELoss()\n\n# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[600, 1000], gamma=0.4)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:26:26.676192Z","iopub.execute_input":"2024-05-27T17:26:26.676442Z","iopub.status.idle":"2024-05-27T17:26:26.690972Z","shell.execute_reply.started":"2024-05-27T17:26:26.676420Z","shell.execute_reply":"2024-05-27T17:26:26.689978Z"},"trusted":true},"execution_count":118,"outputs":[{"name":"stdout","text":"Random seed set as 55688\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"def train_cls(n_epochs, train_dataloader, test_dataloader, best_model_path, early_stopping_patience=30):\n    \n    # 初始化變數\n    best_loss = float('inf')\n    early_stopping_counter = 0\n    early_stopping_patience = early_stopping_patience\n    \n    for epoch in range(1, n_epochs + 1):\n\n        classifier.train()\n\n        train_losses = []\n        \n        trues = np.array([])\n        preds = np.array([])\n\n        for i, (features, labels) in enumerate(train_dataloader):\n            \n            features = features.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n\n            out = classifier(features)\n            \n            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n            \n            loss = criterion(out.squeeze(), labels)\n            loss.backward()\n            optimizer.step()\n                    \n            # metrics calculate\n      \n            train_losses.append(loss.item())\n\n        precision, recall, _ = precision_recall_curve(trues, preds)\n        aucpr = auc(recall, precision)\n\n        fpr, tpr, _ = roc_curve(trues, preds)\n        roc_auc = auc(fpr, tpr)\n        \n\n        train_loss = np.mean(train_losses)\n        train_losses_for_epochs.append(train_loss) # Record Loss\n\n        print(f'Epoch {epoch} train loss: {train_loss}, auc: {roc_auc}, aucpr: {aucpr}', end = '; ')\n        \n        # Validate\n        classifier.eval()\n        valid_losses = []\n\n        trues = np.array([])\n        preds = np.array([])\n        \n        for i, (features, labels) in enumerate(test_dataloader):\n            \n            features = features.to(device)\n            labels = labels.to(device)\n\n            out = classifier(features)\n\n            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n            \n            loss = criterion(out.squeeze(), labels)\n\n            valid_losses.append(loss.item())\n        \n        precision, recall, _ = precision_recall_curve(trues, preds)\n        aucpr = auc(recall, precision)\n        \n        fpr, tpr, _ = roc_curve(trues, preds)\n        roc_auc = auc(fpr, tpr)\n        \n        valid_loss = np.mean(valid_losses)\n        valid_losses_for_epochs.append(valid_loss) # Record Loss\n        \n        print(f'Epoch {epoch} valid loss: {valid_loss}, auc: {roc_auc}, aucpr: {aucpr}')\n        \n\n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            early_stopping_counter = 0\n            torch.save(classifier.state_dict(), best_model_path)\n            # best_model.load_state_dict(copy.deepcopy(classifier.state_dict()))\n            print(f'Best model found! Loss: {valid_loss}')\n            \n        else:\n            # 驗證損失沒有改善，計數器加1\n            early_stopping_counter += 1\n            \n            # 如果計數器達到早期停止的耐心值，則停止訓練\n            if early_stopping_counter >= early_stopping_patience:\n                print('Early stopping triggered.')\n                break\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:26:26.692268Z","iopub.execute_input":"2024-05-27T17:26:26.692545Z","iopub.status.idle":"2024-05-27T17:26:26.708743Z","shell.execute_reply.started":"2024-05-27T17:26:26.692511Z","shell.execute_reply":"2024-05-27T17:26:26.707812Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"# For record loss\ntrain_losses_for_epochs = []\nvalidation_losses_for_epochs = []\nvalid_losses_for_epochs = []\n\n# Save best model to ... \nbest_model_path = os.path.join(save_path, 'lte_HO_cls_RNN.pt')\n\nearly_stopping_patience = 50","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:26:26.710066Z","iopub.execute_input":"2024-05-27T17:26:26.710354Z","iopub.status.idle":"2024-05-27T17:26:26.723337Z","shell.execute_reply.started":"2024-05-27T17:26:26.710326Z","shell.execute_reply":"2024-05-27T17:26:26.722464Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"# # visulized on many sample on validation data\n# sample_value = 2\n# # samples = random.sample(split_time_test, sample_value)\n# samples = [split_time_test[8], split_time_test[9]]\n\n# fig, axs = plt.subplots(1, sample_value, figsize=(14, 2.5))\n\n# # y_test\n# # preds\n\n# for i in range(sample_value):\n#     true = [y_test1[i] for i in range(samples[i][0], samples[i][1])]\n#     axs[i].plot(true, label='true')\n#     prediction = [preds[i] for i in range(samples[i][0], samples[i][1])]\n#     # prediction = [1 if preds[i] > 0.5 else 0  for i in range(samples[i][0], samples[i][1])]\n#     axs[i].plot(prediction, label='pred')\n\n# plt.legend()\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:26:26.724614Z","iopub.execute_input":"2024-05-27T17:26:26.724919Z","iopub.status.idle":"2024-05-27T17:26:26.732775Z","shell.execute_reply.started":"2024-05-27T17:26:26.724896Z","shell.execute_reply":"2024-05-27T17:26:26.731910Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_cls(n_epochs, train_dataloader1, test_dataloader1, best_model_path, early_stopping_patience)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:26:26.734009Z","iopub.execute_input":"2024-05-27T17:26:26.734822Z","iopub.status.idle":"2024-05-27T17:27:25.600475Z","shell.execute_reply.started":"2024-05-27T17:26:26.734791Z","shell.execute_reply":"2024-05-27T17:27:25.599472Z"},"trusted":true},"execution_count":122,"outputs":[{"name":"stdout","text":"Epoch 1 train loss: 0.4941164331388368, auc: 0.7218366416333977, aucpr: 0.3642725473579976; Epoch 1 valid loss: 0.714144304394722, auc: 0.6037461667131307, aucpr: 0.4820846966913403\nBest model found! Loss: 0.714144304394722\nEpoch 2 train loss: 0.34777695962670024, auc: 0.8032980634114112, aucpr: 0.3912935576979412; Epoch 2 valid loss: 0.6931289074321588, auc: 0.6205324783942013, aucpr: 0.5254979997031937\nBest model found! Loss: 0.6931289074321588\nEpoch 3 train loss: 0.3430351990679599, auc: 0.8056251958147599, aucpr: 0.40547235924266134; Epoch 3 valid loss: 0.690814862648646, auc: 0.6176122107610817, aucpr: 0.5151505182134497\nBest model found! Loss: 0.690814862648646\nEpoch 4 train loss: 0.3416230887353345, auc: 0.805514764045153, aucpr: 0.418623138126471; Epoch 4 valid loss: 0.6899012501041094, auc: 0.6525717870086424, aucpr: 0.5823266962945571\nBest model found! Loss: 0.6899012501041094\nEpoch 5 train loss: 0.3411924783845968, auc: 0.8046002897282944, aucpr: 0.4007728149864139; Epoch 5 valid loss: 0.6873917169868946, auc: 0.6584855032060217, aucpr: 0.5925317632944922\nBest model found! Loss: 0.6873917169868946\nEpoch 6 train loss: 0.3402235730941586, auc: 0.8052107664771336, aucpr: 0.40052385621173203; Epoch 6 valid loss: 0.6873045066992441, auc: 0.6519236130471145, aucpr: 0.5819311022089158\nBest model found! Loss: 0.6873045066992441\nEpoch 7 train loss: 0.3399553200679201, auc: 0.8062424969987996, aucpr: 0.40355760161523985; Epoch 7 valid loss: 0.6850744684537252, auc: 0.6603742681906886, aucpr: 0.5912264935570761\nBest model found! Loss: 0.6850744684537252\nEpoch 8 train loss: 0.33893001620596597, auc: 0.8128563053128228, aucpr: 0.4157512882433802; Epoch 8 valid loss: 0.6966047883033752, auc: 0.6449574853638138, aucpr: 0.5827692988367583\nEpoch 9 train loss: 0.34133045825309993, auc: 0.805200840025933, aucpr: 0.40741061736794665; Epoch 9 valid loss: 0.6857222070296606, auc: 0.6421591859492612, aucpr: 0.5607333257843232\nEpoch 10 train loss: 0.33896722039736976, auc: 0.8129450229704285, aucpr: 0.4109222607200168; Epoch 10 valid loss: 0.6862217895686626, auc: 0.6432464454976303, aucpr: 0.5671192941734587\nEpoch 11 train loss: 0.33924902669292867, auc: 0.809296121549395, aucpr: 0.41881637651328607; Epoch 11 valid loss: 0.6844578633705775, auc: 0.6440514357401728, aucpr: 0.591068370506991\nBest model found! Loss: 0.6844578633705775\nEpoch 12 train loss: 0.338600299884965, auc: 0.8150016595785602, aucpr: 0.42982394752644776; Epoch 12 valid loss: 0.6842410775522391, auc: 0.6530596598829105, aucpr: 0.573334780171644\nBest model found! Loss: 0.6842410775522391\nEpoch 13 train loss: 0.33887381009484707, auc: 0.8113701294471277, aucpr: 0.42372604193305163; Epoch 13 valid loss: 0.6829102337360382, auc: 0.6505854474491218, aucpr: 0.589584621953866\nBest model found! Loss: 0.6829102337360382\nEpoch 14 train loss: 0.3381261897027596, auc: 0.8182280664203665, aucpr: 0.4315330839430632; Epoch 14 valid loss: 0.6834719441831112, auc: 0.6543490381934765, aucpr: 0.5683476609061366\nEpoch 15 train loss: 0.33901393398051055, auc: 0.809485964928607, aucpr: 0.41977343528879957; Epoch 15 valid loss: 0.6823724806308746, auc: 0.6415110119877334, aucpr: 0.5863379553093382\nBest model found! Loss: 0.6823724806308746\nEpoch 16 train loss: 0.3374083373446319, auc: 0.8230244035598737, aucpr: 0.4370441847748471; Epoch 16 valid loss: 0.6889901546140512, auc: 0.6471703373292446, aucpr: 0.5730471412190523\nEpoch 17 train loss: 0.3399758928501273, auc: 0.8068964019716414, aucpr: 0.41356969650592507; Epoch 17 valid loss: 0.6841365421811739, auc: 0.6374756063562866, aucpr: 0.5437195620917921\nEpoch 18 train loss: 0.33766478396637006, auc: 0.8280031392401923, aucpr: 0.44071420215478907; Epoch 18 valid loss: 0.686076125750939, auc: 0.6350083635349875, aucpr: 0.5756482483086662\nEpoch 19 train loss: 0.3383392995071122, auc: 0.8135483650824671, aucpr: 0.4323460065089139; Epoch 19 valid loss: 0.6813968606293201, auc: 0.6538960133816559, aucpr: 0.603416280384524\nBest model found! Loss: 0.6813968606293201\nEpoch 20 train loss: 0.33741749806307264, auc: 0.8244407840655642, aucpr: 0.44697895497663015; Epoch 20 valid loss: 0.6860090332726637, auc: 0.6408698076386952, aucpr: 0.5411401861099168\nEpoch 21 train loss: 0.33923253839197004, auc: 0.8097676279814251, aucpr: 0.41255317049839824; Epoch 21 valid loss: 0.6847047110398611, auc: 0.6481879007527181, aucpr: 0.5949429032437789\nEpoch 22 train loss: 0.3378042895245458, auc: 0.8270337592401302, aucpr: 0.42057544750144304; Epoch 22 valid loss: 0.6874400724967321, auc: 0.6540911625313633, aucpr: 0.5940677766942553\nEpoch 23 train loss: 0.3379198340815198, auc: 0.8173225879499086, aucpr: 0.41370636754139495; Epoch 23 valid loss: 0.6870680116117001, auc: 0.6439921940340118, aucpr: 0.5740657499308116\nEpoch 24 train loss: 0.33876803161128366, auc: 0.8105400299654746, aucpr: 0.41201224349440446; Epoch 24 valid loss: 0.6820530146360397, auc: 0.646640646780039, aucpr: 0.5645635600368148\nEpoch 25 train loss: 0.3376200315757123, auc: 0.821967546708606, aucpr: 0.42630402271311457; Epoch 25 valid loss: 0.6801901546617349, auc: 0.6513102871480345, aucpr: 0.5687305613457088\nBest model found! Loss: 0.6801901546617349\nEpoch 26 train loss: 0.3381232552631932, auc: 0.8099276920070353, aucpr: 0.4106665815706638; Epoch 26 valid loss: 0.6833610584338506, auc: 0.6300808475048787, aucpr: 0.5346511698962809\nEpoch 27 train loss: 0.33611428133522764, auc: 0.8397849062105462, aucpr: 0.44107701272049815; Epoch 27 valid loss: 0.6933619280656179, auc: 0.6178422079732366, aucpr: 0.5220360105162916\nEpoch 28 train loss: 0.3385309727931268, auc: 0.8114907978695354, aucpr: 0.4169142007842155; Epoch 28 valid loss: 0.6836513305703799, auc: 0.6530457206579315, aucpr: 0.5473101465460523\nEpoch 29 train loss: 0.3343628316525619, auc: 0.8476978388254527, aucpr: 0.46671921126703303; Epoch 29 valid loss: 0.7103928364813328, auc: 0.6030875383328688, aucpr: 0.4988210683677446\nEpoch 30 train loss: 0.3395268266020764, auc: 0.8129292026888275, aucpr: 0.41791892943904185; Epoch 30 valid loss: 0.6836318957308928, auc: 0.6541538890437691, aucpr: 0.5679088503055989\nEpoch 31 train loss: 0.33609693086144016, auc: 0.8369260882647633, aucpr: 0.43959218170033754; Epoch 31 valid loss: 0.7058303877711296, auc: 0.603296626707555, aucpr: 0.4995687319706287\nEpoch 32 train loss: 0.3359201275298167, auc: 0.8300970000403263, aucpr: 0.43951389650879663; Epoch 32 valid loss: 0.7337368056178093, auc: 0.5997560635628658, aucpr: 0.47931980454770295\nEpoch 33 train loss: 0.33842882168244964, auc: 0.8197893110732666, aucpr: 0.426738053707356; Epoch 33 valid loss: 0.6836989733080069, auc: 0.6370225815444662, aucpr: 0.5686813251208345\nEpoch 34 train loss: 0.3375908507146188, auc: 0.8159102400650183, aucpr: 0.4083533717622353; Epoch 34 valid loss: 0.6751451517144839, auc: 0.6759931697797602, aucpr: 0.5991058050585643\nBest model found! Loss: 0.6751451517144839\nEpoch 35 train loss: 0.33790130820037145, auc: 0.8103746925126639, aucpr: 0.4100087660816896; Epoch 35 valid loss: 0.6808038577437401, auc: 0.6642354335098968, aucpr: 0.5871388314045266\nEpoch 36 train loss: 0.33367894724021124, auc: 0.8542573618594723, aucpr: 0.4827406427305461; Epoch 36 valid loss: 0.7344127384324869, auc: 0.616517981600223, aucpr: 0.5247054077391491\nEpoch 37 train loss: 0.3374346012895679, auc: 0.8191906219852282, aucpr: 0.4266329568335997; Epoch 37 valid loss: 0.676654559870561, auc: 0.6707833844438249, aucpr: 0.5940737503523856\nEpoch 38 train loss: 0.3355867046407689, auc: 0.834262076923793, aucpr: 0.44414726022274503; Epoch 38 valid loss: 0.6848209711412588, auc: 0.6499512127125733, aucpr: 0.5695285175925106\nEpoch 39 train loss: 0.3362865524233914, auc: 0.8169196360714829, aucpr: 0.4236465507201008; Epoch 39 valid loss: 0.6770278190573057, auc: 0.6732785057150822, aucpr: 0.599947303852717\nEpoch 40 train loss: 0.3319752145040239, auc: 0.8615439974439388, aucpr: 0.5095566458102923; Epoch 40 valid loss: 0.6900854470829169, auc: 0.6127125731809312, aucpr: 0.5224072869673497\nEpoch 41 train loss: 0.33366764757598927, auc: 0.8424424033179164, aucpr: 0.46619155192178624; Epoch 41 valid loss: 0.6963442886869112, auc: 0.5941524951212711, aucpr: 0.5010132325626365\nEpoch 42 train loss: 0.33141910904220884, auc: 0.8543572467746788, aucpr: 0.4844785158289002; Epoch 42 valid loss: 0.6952365512649218, auc: 0.6337608028993589, aucpr: 0.5417128027346554\nEpoch 43 train loss: 0.3302059078281617, auc: 0.8514971880224959, aucpr: 0.4775709118462866; Epoch 43 valid loss: 0.6758723271389803, auc: 0.6416504042375243, aucpr: 0.5397309464468958\nEpoch 44 train loss: 0.3340216159717554, auc: 0.8358928067350971, aucpr: 0.44047000745302095; Epoch 44 valid loss: 0.7032069911559423, auc: 0.6365695567326457, aucpr: 0.539312816412896\nEpoch 45 train loss: 0.3264031497012661, auc: 0.8579205325541068, aucpr: 0.4790009193340894; Epoch 45 valid loss: 0.6658872937162718, auc: 0.6494075829383886, aucpr: 0.5426508214773174\nBest model found! Loss: 0.6658872937162718\nEpoch 46 train loss: 0.3317986338938911, auc: 0.8452987396508991, aucpr: 0.4770381571867448; Epoch 46 valid loss: 0.6809568479657173, auc: 0.6373850013939225, aucpr: 0.5427029202755497\nEpoch 47 train loss: 0.33064680412787834, auc: 0.8439009712412098, aucpr: 0.45789120029020397; Epoch 47 valid loss: 0.6929462750752767, auc: 0.6407722330638416, aucpr: 0.5446191306204718\nEpoch 48 train loss: 0.3272115950165225, auc: 0.8558530388899747, aucpr: 0.49060336984615865; Epoch 48 valid loss: 0.6714219450950623, auc: 0.6452467242821299, aucpr: 0.5508159552362846\nEpoch 49 train loss: 0.3295772580758506, auc: 0.85296754360659, aucpr: 0.4910491935146825; Epoch 49 valid loss: 0.6978042597572008, auc: 0.6249442431000837, aucpr: 0.5388353953841865\nEpoch 50 train loss: 0.3243380938001608, auc: 0.859609580266215, aucpr: 0.48774088597896875; Epoch 50 valid loss: 0.6626241269210974, auc: 0.6559311402286033, aucpr: 0.557150852465951\nBest model found! Loss: 0.6626241269210974\nEpoch 51 train loss: 0.3290789727947725, auc: 0.8532420720226075, aucpr: 0.4866438549984307; Epoch 51 valid loss: 0.6932695011297861, auc: 0.6326247560635629, aucpr: 0.5440946569659213\nEpoch 52 train loss: 0.32451312946550664, auc: 0.8605277770022737, aucpr: 0.5021498220082277; Epoch 52 valid loss: 0.6646444996198019, auc: 0.652313911346529, aucpr: 0.560546785243017\nEpoch 53 train loss: 0.3281122684418623, auc: 0.8534564213282212, aucpr: 0.47850760398910835; Epoch 53 valid loss: 0.6949739282329878, auc: 0.6462921661555618, aucpr: 0.5567556702331515\nEpoch 54 train loss: 0.32332166119289873, auc: 0.8602746524966576, aucpr: 0.4839686717271985; Epoch 54 valid loss: 0.6681879013776779, auc: 0.6481251742403122, aucpr: 0.5535719260798405\nEpoch 55 train loss: 0.32805470729472475, auc: 0.8527789410337778, aucpr: 0.4975331413652972; Epoch 55 valid loss: 0.6981083651383718, auc: 0.6437621968218568, aucpr: 0.5478769192145079\nEpoch 56 train loss: 0.32274781301403166, auc: 0.8608336357798934, aucpr: 0.49758464590570406; Epoch 56 valid loss: 0.6651027351617813, auc: 0.6501393922497909, aucpr: 0.5548785387223318\nEpoch 57 train loss: 0.32757442904108536, auc: 0.8546417016418971, aucpr: 0.5005862659760443; Epoch 57 valid loss: 0.6951246423025926, auc: 0.646257318093114, aucpr: 0.5493038263271318\nEpoch 58 train loss: 0.3211339688073397, auc: 0.8634883410728633, aucpr: 0.5052402981203321; Epoch 58 valid loss: 0.6609297394752502, auc: 0.65444661276833, aucpr: 0.559897528566931\nBest model found! Loss: 0.6609297394752502\nEpoch 59 train loss: 0.3279561189194209, auc: 0.8524429927009564, aucpr: 0.49064502774407365; Epoch 59 valid loss: 0.6828435342758894, auc: 0.6591162531363256, aucpr: 0.5693209036449552\nEpoch 60 train loss: 0.3209665481189671, auc: 0.8629684431912299, aucpr: 0.497307429595277; Epoch 60 valid loss: 0.6612985221048197, auc: 0.6602244215221633, aucpr: 0.5709487361384434\nEpoch 61 train loss: 0.32746126172774304, auc: 0.8527907286945786, aucpr: 0.4916024022084433; Epoch 61 valid loss: 0.6878341287374496, auc: 0.6612350153331475, aucpr: 0.5668895619642528\nEpoch 62 train loss: 0.32010752921420743, auc: 0.865060442781764, aucpr: 0.5086051133430403; Epoch 62 valid loss: 0.6592608715097109, auc: 0.6621828826317258, aucpr: 0.5767013654898008\nBest model found! Loss: 0.6592608715097109\nEpoch 63 train loss: 0.32475928723885655, auc: 0.8605749276454768, aucpr: 0.51479766914986; Epoch 63 valid loss: 0.670485174904267, auc: 0.6681279620853081, aucpr: 0.5692439262061042\nEpoch 64 train loss: 0.32550661395650576, auc: 0.8531549053730019, aucpr: 0.4938259036767794; Epoch 64 valid loss: 0.6951553498705229, auc: 0.6582659604126011, aucpr: 0.5660790105061859\nEpoch 65 train loss: 0.3203054429085743, auc: 0.862577589175205, aucpr: 0.5209096505959158; Epoch 65 valid loss: 0.675453698883454, auc: 0.6577850571508224, aucpr: 0.5765178793023551\nEpoch 66 train loss: 0.3245015674546717, auc: 0.85421858665947, aucpr: 0.5000714405249865; Epoch 66 valid loss: 0.7116362986465296, auc: 0.6639671034290494, aucpr: 0.5984715546411906\nEpoch 67 train loss: 0.31879901553135154, auc: 0.8677083236395333, aucpr: 0.5268430857804272; Epoch 67 valid loss: 0.6543773102263609, auc: 0.6673055478115417, aucpr: 0.5902585209016454\nBest model found! Loss: 0.6543773102263609\nEpoch 68 train loss: 0.3278454205334, auc: 0.855421858665947, aucpr: 0.5023508173050784; Epoch 68 valid loss: 0.6785665166874727, auc: 0.6647616392528575, aucpr: 0.5709985986371513\nEpoch 69 train loss: 0.32147537922458586, auc: 0.8640609732264999, aucpr: 0.5439254605370352; Epoch 69 valid loss: 0.6763287124534448, auc: 0.6497072762754391, aucpr: 0.5549546775314393\nEpoch 70 train loss: 0.3284424599170612, auc: 0.8514469353632925, aucpr: 0.48124741261993204; Epoch 70 valid loss: 0.6742103652407726, auc: 0.6602174519096737, aucpr: 0.5682388446277034\nEpoch 71 train loss: 0.32048132452206174, auc: 0.861735702032751, aucpr: 0.5091243755432475; Epoch 71 valid loss: 0.6984372325241566, auc: 0.6520769445218846, aucpr: 0.5685438571736846\nEpoch 72 train loss: 0.32150804319335846, auc: 0.8595217932134095, aucpr: 0.5273700505489985; Epoch 72 valid loss: 0.7006270637114843, auc: 0.6559311402286032, aucpr: 0.5794015184604081\nEpoch 73 train loss: 0.31200211834901426, auc: 0.8742293816751505, aucpr: 0.5717689664615759; Epoch 73 valid loss: 0.6709971359620491, auc: 0.6523836074714247, aucpr: 0.5468463764584061\nEpoch 74 train loss: 0.332600229607234, auc: 0.8453840450909046, aucpr: 0.45915733425862315; Epoch 74 valid loss: 0.6509973425418139, auc: 0.6756969612489546, aucpr: 0.5863825315350333\nBest model found! Loss: 0.6509973425418139\nEpoch 75 train loss: 0.32393779861944716, auc: 0.8517887775265146, aucpr: 0.49614028571213153; Epoch 75 valid loss: 0.6593438629060984, auc: 0.6719612489545581, aucpr: 0.5884713971847678\nEpoch 76 train loss: 0.3220443208686934, auc: 0.8546916440995003, aucpr: 0.510830524283412; Epoch 76 valid loss: 0.6690609318514665, auc: 0.6689434067465849, aucpr: 0.5872919554757003\nEpoch 77 train loss: 0.31785421360362104, auc: 0.8617766486439536, aucpr: 0.5337688617987956; Epoch 77 valid loss: 0.7131207485993704, auc: 0.6695009757457485, aucpr: 0.6057804080669429\nEpoch 78 train loss: 0.3211713720624212, auc: 0.8547338315171029, aucpr: 0.48943551713332606; Epoch 78 valid loss: 0.6533726857354244, auc: 0.6858238081962642, aucpr: 0.5941091674462918\nEpoch 79 train loss: 0.325108599724123, auc: 0.8541537545250657, aucpr: 0.518046252738487; Epoch 79 valid loss: 0.6520383246243, auc: 0.6744807638695288, aucpr: 0.5907146804694985\nEpoch 80 train loss: 0.32037570689357325, auc: 0.8613963414823294, aucpr: 0.5155600980014428; Epoch 80 valid loss: 0.8623769000793496, auc: 0.6748187900752718, aucpr: 0.5907166321584835\nEpoch 81 train loss: 0.32005232766889297, auc: 0.8666772135210674, aucpr: 0.5053352682438796; Epoch 81 valid loss: 0.6774702245990435, auc: 0.6799135768051296, aucpr: 0.5862765343874694\nEpoch 82 train loss: 0.32192907622193345, auc: 0.8559268668707793, aucpr: 0.5028976668904215; Epoch 82 valid loss: 0.669574674218893, auc: 0.667667967660998, aucpr: 0.5889731932970124\nEpoch 83 train loss: 0.3248336748830766, auc: 0.8499343923615958, aucpr: 0.5033726960433847; Epoch 83 valid loss: 0.653680145740509, auc: 0.6816350710900475, aucpr: 0.5873654026238478\nEpoch 84 train loss: 0.3246532230283136, auc: 0.8529278378017874, aucpr: 0.5114777408972073; Epoch 84 valid loss: 0.6556539423763752, auc: 0.6780526902704209, aucpr: 0.5819536124535063\nEpoch 85 train loss: 0.3241884623260541, auc: 0.8528868911905847, aucpr: 0.5196369378294075; Epoch 85 valid loss: 0.6623028454681238, auc: 0.664434067465849, aucpr: 0.5767181739145762\nEpoch 86 train loss: 0.322826996606641, auc: 0.8550434127139228, aucpr: 0.4898079163836885; Epoch 86 valid loss: 0.6646503030012051, auc: 0.676609980485085, aucpr: 0.5803627448771316\nEpoch 87 train loss: 0.31185002755586483, auc: 0.87593921289446, aucpr: 0.5596453305863859; Epoch 87 valid loss: 0.7579157399013638, auc: 0.6600641204349038, aucpr: 0.5660654479574613\nEpoch 88 train loss: 0.3071190798622011, auc: 0.8824565485108773, aucpr: 0.5897848646993853; Epoch 88 valid loss: 0.7774684832741817, auc: 0.6446264287705603, aucpr: 0.6021215898503811\nEpoch 89 train loss: 0.31825696335302683, auc: 0.8673038207531074, aucpr: 0.5657689817466601; Epoch 89 valid loss: 0.6667156952122847, auc: 0.6752369668246444, aucpr: 0.5769834468878083\nEpoch 90 train loss: 0.32009559148322886, auc: 0.8675507412267232, aucpr: 0.5585678519482212; Epoch 90 valid loss: 0.7026277544597784, auc: 0.6603777529969334, aucpr: 0.6098225726719462\nEpoch 91 train loss: 0.3223537694572834, auc: 0.8595965517990142, aucpr: 0.4882345918080412; Epoch 91 valid loss: 0.6524795033037663, auc: 0.6870574296069138, aucpr: 0.6062042473870273\nEpoch 92 train loss: 0.31906030228226495, auc: 0.8650654060073641, aucpr: 0.5426370840373691; Epoch 92 valid loss: 0.6662624639769396, auc: 0.6784917758572623, aucpr: 0.5839401777951422\nEpoch 93 train loss: 0.31485082381669427, auc: 0.8694113304236423, aucpr: 0.5620474116971703; Epoch 93 valid loss: 0.6765168588608503, auc: 0.6731321438528017, aucpr: 0.5842579502843979\nEpoch 94 train loss: 0.31221841016875057, auc: 0.8726430106926492, aucpr: 0.5724021141386957; Epoch 94 valid loss: 0.7312116604298353, auc: 0.6639183161416227, aucpr: 0.5728933545561181\nEpoch 95 train loss: 0.3125999612027614, auc: 0.8702724500652974, aucpr: 0.5685262274810927; Epoch 95 valid loss: 0.6932172862191995, auc: 0.6725536660161696, aucpr: 0.5859108491547039\nEpoch 96 train loss: 0.30502017117121777, auc: 0.8811989912243968, aucpr: 0.6037380959693478; Epoch 96 valid loss: 0.9032292379997671, auc: 0.6305478115416784, aucpr: 0.5570159581877652\nEpoch 97 train loss: 0.30743209053314263, auc: 0.8794277400882834, aucpr: 0.5950661279541714; Epoch 97 valid loss: 0.7839289450397094, auc: 0.6493518260384723, aucpr: 0.5810187829938951\nEpoch 98 train loss: 0.30414411578698897, auc: 0.8824491036724769, aucpr: 0.6029883174089641; Epoch 98 valid loss: 0.7763254856690764, auc: 0.665061332589908, aucpr: 0.5837943006127656\nEpoch 99 train loss: 0.29723158928908044, auc: 0.8903176774585803, aucpr: 0.6240605722888771; Epoch 99 valid loss: 0.7194500978415211, auc: 0.661548647895177, aucpr: 0.5891561808375956\nEpoch 100 train loss: 0.32723762207128765, auc: 0.8573252556836687, aucpr: 0.5027137475759209; Epoch 100 valid loss: 0.674655685822169, auc: 0.6489824365765263, aucpr: 0.5594463440282538\nEpoch 101 train loss: 0.3167164795578964, auc: 0.8666765931178672, aucpr: 0.5421569229432283; Epoch 101 valid loss: 0.6408087325592836, auc: 0.7024393643713409, aucpr: 0.6236260512389203\nBest model found! Loss: 0.6408087325592836\nEpoch 102 train loss: 0.3219259816133451, auc: 0.8614335656743317, aucpr: 0.5441878447558035; Epoch 102 valid loss: 0.7201997749507427, auc: 0.6468427655422359, aucpr: 0.5818954562288124\nEpoch 103 train loss: 0.32140827110580367, auc: 0.856312757661204, aucpr: 0.5311849358418839; Epoch 103 valid loss: 0.6491305989523729, auc: 0.690601477557848, aucpr: 0.6072983402485839\nEpoch 104 train loss: 0.314839860766351, auc: 0.867846053149942, aucpr: 0.5594387163964182; Epoch 104 valid loss: 0.6362378634512424, auc: 0.7093532199609702, aucpr: 0.6375544315161048\nBest model found! Loss: 0.6362378634512424\nEpoch 105 train loss: 0.3316796141861315, auc: 0.84859246023991, aucpr: 0.46341904867503136; Epoch 105 valid loss: 0.6535940517981847, auc: 0.6755157513242264, aucpr: 0.584677846510544\nEpoch 106 train loss: 0.3159197108931737, auc: 0.8732776831662897, aucpr: 0.5329646008829201; Epoch 106 valid loss: 0.6319788185258707, auc: 0.7026205742960692, aucpr: 0.6328562747695978\nBest model found! Loss: 0.6319788185258707\nEpoch 107 train loss: 0.3130387859764435, auc: 0.878375846462616, aucpr: 0.5458803758634925; Epoch 107 valid loss: 0.6508955123523871, auc: 0.6980485085029272, aucpr: 0.6194322518432921\nEpoch 108 train loss: 0.3200422179696199, auc: 0.868452807479581, aucpr: 0.5735008389703236; Epoch 108 valid loss: 0.6608198744555315, auc: 0.6917967660998049, aucpr: 0.6058939368923618\nEpoch 109 train loss: 0.30913306936645774, auc: 0.8726157129518476, aucpr: 0.5681113416037229; Epoch 109 valid loss: 0.7113739469399055, auc: 0.6615347086701979, aucpr: 0.5731226415875398\nEpoch 110 train loss: 0.29944534236386, auc: 0.8883888439096569, aucpr: 0.6205746065810552; Epoch 110 valid loss: 0.780159680172801, auc: 0.644277948146083, aucpr: 0.567351465916307\nEpoch 111 train loss: 0.3407438667029671, auc: 0.8466326065309844, aucpr: 0.45576205027685956; Epoch 111 valid loss: 0.6500881053507328, auc: 0.6830986897128519, aucpr: 0.6014391742845024\nEpoch 112 train loss: 0.31494478861231706, auc: 0.872641149483049, aucpr: 0.5161617282516402; Epoch 112 valid loss: 0.6482898145914078, auc: 0.6915388904376917, aucpr: 0.6195099699632276\nEpoch 113 train loss: 0.3121010749888237, auc: 0.8794575194418852, aucpr: 0.5683163557359978; Epoch 113 valid loss: 0.6616195067763329, auc: 0.696550041817675, aucpr: 0.628337066002086\nEpoch 114 train loss: 0.31245011214152346, auc: 0.8738366664495255, aucpr: 0.5740845843045539; Epoch 114 valid loss: 0.6563795345524946, auc: 0.6952955115695567, aucpr: 0.6091779509359063\nEpoch 115 train loss: 0.3080633574771614, auc: 0.88202909070605, aucpr: 0.5959625946709204; Epoch 115 valid loss: 0.6538912455240885, auc: 0.7055269027042096, aucpr: 0.6433545577525429\nEpoch 116 train loss: 0.3127597310338783, auc: 0.8727100142382533, aucpr: 0.578646579465758; Epoch 116 valid loss: 0.6451510762174925, auc: 0.6997003066629495, aucpr: 0.6166951748048937\nEpoch 117 train loss: 0.3085834725496129, auc: 0.8805122048819528, aucpr: 0.5997277436417857; Epoch 117 valid loss: 0.6352150775492191, auc: 0.7169849456370225, aucpr: 0.6642250187562639\nEpoch 118 train loss: 0.3249107261112128, auc: 0.8545197924130894, aucpr: 0.5174913006489883; Epoch 118 valid loss: 0.6265191783507665, auc: 0.7068511290772234, aucpr: 0.6438497870840193\nBest model found! Loss: 0.6265191783507665\nEpoch 119 train loss: 0.3207979875431059, auc: 0.8642216576553101, aucpr: 0.4889369384196315; Epoch 119 valid loss: 0.6382082216441631, auc: 0.7006342347365485, aucpr: 0.6229299649164319\nEpoch 120 train loss: 0.302795288941419, auc: 0.8867274041399504, aucpr: 0.6070320140614753; Epoch 120 valid loss: 0.6652232532699903, auc: 0.6856704767214943, aucpr: 0.6116515934116771\nEpoch 121 train loss: 0.30911947171598925, auc: 0.8791960194930687, aucpr: 0.6046829912639333; Epoch 121 valid loss: 0.6510068122297525, auc: 0.7046487315305269, aucpr: 0.6219859475279153\nEpoch 122 train loss: 0.2993532393880268, auc: 0.8875066305592004, aucpr: 0.6188096910180304; Epoch 122 valid loss: 0.6598479362825552, auc: 0.6821508224142737, aucpr: 0.6094318801283388\nEpoch 123 train loss: 0.3453293631746258, auc: 0.8419808233370868, aucpr: 0.45709967636970744; Epoch 123 valid loss: 0.6499985530972481, auc: 0.6810705324783941, aucpr: 0.6059741385095827\nEpoch 124 train loss: 0.3148569379880042, auc: 0.8773357404977495, aucpr: 0.5414306349600607; Epoch 124 valid loss: 0.6598778006931146, auc: 0.686778645107332, aucpr: 0.602605650982486\nEpoch 125 train loss: 0.3113234348652119, auc: 0.88046877665795, aucpr: 0.5757639298019864; Epoch 125 valid loss: 0.6513133036593596, auc: 0.6961597435182604, aucpr: 0.6200023860635073\nEpoch 126 train loss: 0.30557515291006937, auc: 0.8857099428918855, aucpr: 0.627067058153294; Epoch 126 valid loss: 0.6694267795731624, auc: 0.6985851686646223, aucpr: 0.6280432759993055\nEpoch 127 train loss: 0.3149161236827359, auc: 0.8713150376429641, aucpr: 0.5719556702782107; Epoch 127 valid loss: 0.6546077206730843, auc: 0.6877962085308057, aucpr: 0.6194729271697255\nEpoch 128 train loss: 0.30889425830242784, auc: 0.8771123953457353, aucpr: 0.5959461771594026; Epoch 128 valid loss: 0.6857265966633955, auc: 0.6783802620574297, aucpr: 0.5983431101127135\nEpoch 129 train loss: 0.3018636877322428, auc: 0.8836452410421534, aucpr: 0.6147977992724725; Epoch 129 valid loss: 0.6711914446204901, auc: 0.6809590186785613, aucpr: 0.6055722014329002\nEpoch 130 train loss: 0.3298298417874486, auc: 0.8515077348768965, aucpr: 0.4980843196778091; Epoch 130 valid loss: 0.6424008309841156, auc: 0.684325341511012, aucpr: 0.6159519725478085\nEpoch 131 train loss: 0.3129901049355404, auc: 0.875329356548821, aucpr: 0.5392621843243478; Epoch 131 valid loss: 0.6588537010053793, auc: 0.6881446891552829, aucpr: 0.6131403389225135\nEpoch 132 train loss: 0.30138550157865057, auc: 0.8882076861752453, aucpr: 0.6297785744038413; Epoch 132 valid loss: 0.6483717896044254, auc: 0.6915876777251185, aucpr: 0.6247209079805628\nEpoch 133 train loss: 0.3228704885540046, auc: 0.8569244752164432, aucpr: 0.5251723138663134; Epoch 133 valid loss: 0.6761475031574568, auc: 0.6554502369668247, aucpr: 0.5891441481821835\nEpoch 134 train loss: 0.3320440875140063, auc: 0.8574747728548784, aucpr: 0.5047660349091985; Epoch 134 valid loss: 0.6492372862994671, auc: 0.6735154725397267, aucpr: 0.5995892839069643\nEpoch 135 train loss: 0.31878417295696765, auc: 0.8703127762733001, aucpr: 0.5197249852671258; Epoch 135 valid loss: 0.6345194727182388, auc: 0.6944521884583217, aucpr: 0.6274299868122486\nEpoch 136 train loss: 0.3184530248137058, auc: 0.8727149774638537, aucpr: 0.5357342804748778; Epoch 136 valid loss: 0.6407527675231298, auc: 0.6898731530526903, aucpr: 0.6191298537318786\nEpoch 137 train loss: 0.3130747217895794, auc: 0.8773376017073496, aucpr: 0.552196052193263; Epoch 137 valid loss: 0.6564744090040525, auc: 0.6789935879565097, aucpr: 0.6039716312910435\nEpoch 138 train loss: 0.30556990590882765, auc: 0.8872950730679869, aucpr: 0.5963314341917516; Epoch 138 valid loss: 0.6594902339080969, auc: 0.6624337886813494, aucpr: 0.5927344563710507\nEpoch 139 train loss: 0.32061021164157993, auc: 0.8661331199146325, aucpr: 0.5368448242190345; Epoch 139 valid loss: 0.683868228768309, auc: 0.6538960133816559, aucpr: 0.5855389931389934\nEpoch 140 train loss: 0.3182651140673695, auc: 0.8686215571499919, aucpr: 0.5611553797976357; Epoch 140 valid loss: 0.66543676952521, auc: 0.6596947309729578, aucpr: 0.5849004966332484\nEpoch 141 train loss: 0.3410346874194799, auc: 0.8373507542551905, aucpr: 0.4716599025730536; Epoch 141 valid loss: 0.6600304171442986, auc: 0.67530666294954, aucpr: 0.5980568585398156\nEpoch 142 train loss: 0.32181316220392503, auc: 0.8585176706341451, aucpr: 0.5145420002379303; Epoch 142 valid loss: 0.6592443908254305, auc: 0.6719960970170059, aucpr: 0.5792941088205877\nEpoch 143 train loss: 0.3207505220199577, auc: 0.8612945953575228, aucpr: 0.5366039121035857; Epoch 143 valid loss: 0.6531199080248674, auc: 0.6745121271257318, aucpr: 0.5916549163996827\nEpoch 144 train loss: 0.31929877451431243, auc: 0.8640212674216974, aucpr: 0.544041816110641; Epoch 144 valid loss: 0.6609118146200975, auc: 0.6631237803178144, aucpr: 0.5866957689913379\nEpoch 145 train loss: 0.31784231527447016, auc: 0.8669123463338824, aucpr: 0.5522006290290227; Epoch 145 valid loss: 0.6551573711136977, auc: 0.6663646501254531, aucpr: 0.5837739139155194\nEpoch 146 train loss: 0.32739244062116823, auc: 0.8573993938660734, aucpr: 0.5015056951156446; Epoch 146 valid loss: 0.6432798430323601, auc: 0.6848132143852802, aucpr: 0.6106054010798766\nEpoch 147 train loss: 0.3099010640007243, auc: 0.8819186589364427, aucpr: 0.5581715863491847; Epoch 147 valid loss: 0.6694046954313914, auc: 0.657039308614441, aucpr: 0.5835233218489664\nEpoch 148 train loss: 0.3242667875860806, auc: 0.8565832534564213, aucpr: 0.5350895268381618; Epoch 148 valid loss: 0.647674024105072, auc: 0.6819835517145247, aucpr: 0.6035922875619193\nEpoch 149 train loss: 0.32963726467083443, auc: 0.8524805270945588, aucpr: 0.484417912029304; Epoch 149 valid loss: 0.641566431770722, auc: 0.6882980206300531, aucpr: 0.6119653648829143\nEpoch 150 train loss: 0.3131641122921438, auc: 0.8768983562417215, aucpr: 0.5315865234110129; Epoch 150 valid loss: 0.6499892404923836, auc: 0.6726721494284917, aucpr: 0.5931682850674218\nEpoch 151 train loss: 0.31006032495787567, auc: 0.8770354653489303, aucpr: 0.5763085133815053; Epoch 151 valid loss: 0.6517662641902765, auc: 0.6747212155004182, aucpr: 0.607187757049196\nEpoch 152 train loss: 0.32532805708735574, auc: 0.8546128528930953, aucpr: 0.5344488839006971; Epoch 152 valid loss: 0.6332634625335535, auc: 0.6997421243378867, aucpr: 0.6462122808189199\nEpoch 153 train loss: 0.32000391247628834, auc: 0.8663794199850482, aucpr: 0.4878849134320027; Epoch 153 valid loss: 0.6533078613380591, auc: 0.6775369389461947, aucpr: 0.5770984938837024\nEpoch 154 train loss: 0.30766023036583856, auc: 0.8840106585269767, aucpr: 0.5884361862764855; Epoch 154 valid loss: 0.6769722842921814, auc: 0.668344020072484, aucpr: 0.586911299410782\nEpoch 155 train loss: 0.30023743022758426, auc: 0.8892282494393106, aucpr: 0.6185526423773496; Epoch 155 valid loss: 0.647512768705686, auc: 0.6695846110956232, aucpr: 0.6063354947876183\nEpoch 156 train loss: 0.3137931493777866, auc: 0.8758244383024527, aucpr: 0.5504162797720925; Epoch 156 valid loss: 0.6740207013984522, auc: 0.6613883468079175, aucpr: 0.5783686796923408\nEpoch 157 train loss: 0.29831987134493276, auc: 0.8895260429753296, aucpr: 0.6196008739282028; Epoch 157 valid loss: 0.6760499477386475, auc: 0.6523208809590186, aucpr: 0.5845874679115495\nEpoch 158 train loss: 0.29596491590813334, auc: 0.8902097273017734, aucpr: 0.6174319987431953; Epoch 158 valid loss: 0.7151922965422273, auc: 0.6530666294954002, aucpr: 0.5802603702886115\nEpoch 159 train loss: 0.2936000698944383, auc: 0.8917489476410718, aucpr: 0.6191012344215112; Epoch 159 valid loss: 0.66206402455767, auc: 0.674003345413995, aucpr: 0.6022661376650562\nEpoch 160 train loss: 0.311090950710771, auc: 0.8777973204785791, aucpr: 0.5699995955397517; Epoch 160 valid loss: 0.6726597802092632, auc: 0.6667131307499303, aucpr: 0.5969921416132381\nEpoch 161 train loss: 0.2961458090103874, auc: 0.8895285245881297, aucpr: 0.6103255732764389; Epoch 161 valid loss: 0.6809432115405798, auc: 0.6616810705324784, aucpr: 0.5836803933648298\nEpoch 162 train loss: 0.28974354241938954, auc: 0.8970564970174115, aucpr: 0.6401757095447675; Epoch 162 valid loss: 0.686666759972771, auc: 0.6654725397267912, aucpr: 0.5903615148410936\nEpoch 163 train loss: 0.3033453438085508, auc: 0.8831129350965192, aucpr: 0.5952871777365911; Epoch 163 valid loss: 0.7469551755736271, auc: 0.6459785335935322, aucpr: 0.5729940881207014\nEpoch 164 train loss: 0.30709349297633176, auc: 0.8810817350195894, aucpr: 0.5906444320421551; Epoch 164 valid loss: 0.658749678482612, auc: 0.6891483133537776, aucpr: 0.6235102905138348\nEpoch 165 train loss: 0.31001913450200397, auc: 0.8715597867053799, aucpr: 0.5716504910290434; Epoch 165 valid loss: 0.6367539459218582, auc: 0.6887510454418735, aucpr: 0.6158125612505319\nEpoch 166 train loss: 0.2892607295082235, auc: 0.8973226499902286, aucpr: 0.6221722985883231; Epoch 166 valid loss: 0.694222285101811, auc: 0.672651240591023, aucpr: 0.5901737378078887\nEpoch 167 train loss: 0.2866533981013877, auc: 0.9014756290112945, aucpr: 0.6487680262516897; Epoch 167 valid loss: 0.6772924227019151, auc: 0.6737315305269027, aucpr: 0.6067044182893473\nEpoch 168 train loss: 0.3101234549836117, auc: 0.8749949592239996, aucpr: 0.5866143882870112; Epoch 168 valid loss: 0.6818356141448021, auc: 0.6384652913298021, aucpr: 0.5628331439767962\nEarly stopping triggered.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test\ndef test(test_dataloader):\n    best_model = RNN_Cls(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n    best_model.load_state_dict(torch.load(best_model_path))\n    best_model.eval()\n\n    with torch.no_grad():\n        \n        best_model.eval()\n        valid_losses = []\n\n        trues = np.array([])\n        preds = np.array([])\n        \n        for i, (features, labels) in enumerate(test_dataloader):\n            \n            features = features.to(device)\n            labels = labels.to(device)\n\n            out = best_model(features)\n\n            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n            \n            loss = criterion(out.squeeze(), labels)\n\n            valid_losses.append(loss.item())\n        \n        precision, recall, _ = precision_recall_curve(trues, preds)\n        aucpr = auc(recall, precision)\n        threshold = 0.5\n        p = precision_score(trues, [1 if pred > threshold else 0 for pred in preds])\n        r = recall_score(trues, [1 if pred > threshold else 0 for pred in preds])\n        f1 = f1_score(trues, [1 if pred > threshold else 0 for pred in preds])\n        \n        fpr, tpr, _ = roc_curve(trues, preds)\n        roc_auc = auc(fpr, tpr)\n        \n        valid_loss = np.mean(valid_losses)\n\n        print(f'valid loss {valid_loss}, roc_auc {roc_auc}, aucpr {aucpr}')\n        \n        return valid_loss, roc_auc, aucpr, p, r, f1\n        \ntest(test_dataloader1)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:27:25.602183Z","iopub.execute_input":"2024-05-27T17:27:25.602558Z","iopub.status.idle":"2024-05-27T17:27:25.666930Z","shell.execute_reply.started":"2024-05-27T17:27:25.602522Z","shell.execute_reply":"2024-05-27T17:27:25.666076Z"},"trusted":true},"execution_count":123,"outputs":[{"name":"stdout","text":"valid loss 0.6265191783507665, roc_auc 0.7068511290772234, aucpr 0.6438497870840193\n","output_type":"stream"},{"execution_count":123,"output_type":"execute_result","data":{"text/plain":"(0.6265191783507665,\n 0.7068511290772234,\n 0.6438497870840193,\n 0.6410256410256411,\n 0.29411764705882354,\n 0.40322580645161293)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Grid Search","metadata":{}},{"cell_type":"code","source":"# from IPython.display import display, clear_output\n# import itertools\n\n# n_epochs = 600\n# lrs = [0.001, 0.01, 0.1]\n# hidden_dims = [32, 64, 128]\n# num_layers = [1, 2]\n# dropout = 0\n\n# early_stopping_patience = 50\n# rnn = 'GRU'\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:27:25.668375Z","iopub.execute_input":"2024-05-27T17:27:25.669035Z","iopub.status.idle":"2024-05-27T17:27:25.673068Z","shell.execute_reply.started":"2024-05-27T17:27:25.668998Z","shell.execute_reply":"2024-05-27T17:27:25.672142Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"# f_out = 'lte_ho_cls_rnn.csv'\n# f_out = open(f_out, 'w')\n# cols_out = ['lr','hidden_dim','num_layer', 'valid_loss','auc','aucpr', 'p', 'r', 'f1']\n# f_out.write(','.join(cols_out)+'\\n')\n\n# for lr, hidden_dim, num_layer in itertools.product(lrs, hidden_dims, num_layers):\n    \n#     set_seed(seed)\n    \n#     # Model and optimizer\n#     classifier = RNN_Cls(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n#     optimizer = optim.Adam(classifier.parameters(), lr=lr)\n\n#     criterion = nn.BCELoss()\n    \n#     # For record loss\n#     train_losses_for_epochs = []\n#     validation_losses_for_epochs = []\n#     valid_losses_for_epochs = []\n\n#     # Save best model to ... \n#     best_model_path = os.path.join(save_path, 'lte_HO_cls_RNN.pt')\n    \n#     train_cls(n_epochs, train_dataloader1, test_dataloader1, best_model_path, early_stopping_patience)\n#     clear_output(wait=True)\n    \n#     print(f'For learning_rate = {lr}, hidden_dim = {hidden_dim}, num_layer = {num_layer}.')\n#     valid_loss, roc_auc, aucpr, p, r, f1 = test(test_dataloader1)\n    \n#     cols_out = [lr, hidden_dim, num_layer, valid_loss, roc_auc, aucpr, p, r, f1]\n#     cols_out = [str(n) for n in cols_out]\n#     f_out.write(','.join(cols_out)+'\\n')\n\n# f_out.close()","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:27:25.674206Z","iopub.execute_input":"2024-05-27T17:27:25.674495Z","iopub.status.idle":"2024-05-27T17:27:25.683913Z","shell.execute_reply.started":"2024-05-27T17:27:25.674472Z","shell.execute_reply":"2024-05-27T17:27:25.683134Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"markdown","source":"# Forecast","metadata":{}},{"cell_type":"code","source":"# Hyperparameters\nn_epochs = 600\nlr = 0.001\nbatch_size = 32\nhidden_dim = 128\nnum_layer = 2\ndropout = 0\n\nrnn = 'LSTM' # 'LSTM' or 'GRU'","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:27:25.684912Z","iopub.execute_input":"2024-05-27T17:27:25.685197Z","iopub.status.idle":"2024-05-27T17:27:25.708716Z","shell.execute_reply.started":"2024-05-27T17:27:25.685174Z","shell.execute_reply":"2024-05-27T17:27:25.707980Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"set_seed(seed)\nforecaster = RNN_Fst(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\noptimizer = optim.Adam(forecaster.parameters(), lr=lr)\n\n# criterion = nn.MSELoss()\ncriterion = nn.BCELoss()","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:27:25.709901Z","iopub.execute_input":"2024-05-27T17:27:25.710193Z","iopub.status.idle":"2024-05-27T17:27:25.724366Z","shell.execute_reply.started":"2024-05-27T17:27:25.710166Z","shell.execute_reply":"2024-05-27T17:27:25.723436Z"},"trusted":true},"execution_count":127,"outputs":[{"name":"stdout","text":"Random seed set as 55688\n","output_type":"stream"}]},{"cell_type":"code","source":"# def train_fst(n_epochs, train_dataloader, test_dataloader, best_model_path, early_stopping_patience=30):\n    \n#     def rmse(predictions, targets):\n#         return torch.sqrt(F.mse_loss(predictions, targets))\n\n#     def mae(predictions, targets):\n#         return torch.mean(torch.abs(predictions - targets))\n    \n#     # 初始化變數\n#     best_loss = float('inf')\n#     early_stopping_counter = 0\n#     early_stopping_patience = early_stopping_patience\n    \n#     for epoch in range(1, n_epochs + 1):\n\n#         forecaster.train()\n\n#         train_losses = []\n        \n#         trues = torch.tensor([]).to(device)\n#         preds = torch.tensor([]).to(device)\n\n#         for i, (features, labels) in enumerate(train_dataloader):\n            \n#             features = features.to(device)\n#             labels = labels.to(device)\n\n#             optimizer.zero_grad()\n\n#             out = forecaster(features)\n            \n#             trues = torch.cat((trues, labels), axis=0)\n#             preds = torch.cat((preds, out.squeeze().detach()), axis=0)\n            \n#             loss = criterion(out.squeeze(), labels)\n#             loss.backward()\n#             optimizer.step()\n                    \n#             # metrics calculate\n      \n#             train_losses.append(loss.item())\n\n#         train_loss = np.mean(train_losses)\n#         train_losses_for_epochs.append(train_loss) # Record Loss\n\n#         rmse_error = rmse(preds, trues)\n#         mae_error = mae(preds, trues)\n        \n#         print(f'Epoch {epoch} train loss: {train_loss}, rmse: {rmse_error}, mae: {mae_error}', end = '; ')\n        \n#         # Validate\n#         forecaster.eval()\n#         valid_losses = []\n\n#         trues = torch.tensor([]).to(device)\n#         preds = torch.tensor([]).to(device)\n        \n#         for i, (features, labels) in enumerate(test_dataloader):\n            \n#             features = features.to(device)\n#             labels = labels.to(device)\n\n#             out = forecaster(features)\n\n#             trues = torch.cat((trues, labels), axis=0)\n#             preds = torch.cat((preds, out.squeeze().detach()), axis=0)\n            \n#             loss = criterion(out.squeeze(), labels)\n\n#             valid_losses.append(loss.item())\n        \n#         valid_loss = np.mean(valid_losses)\n#         valid_losses_for_epochs.append(valid_loss) # Record Loss\n        \n#         rmse_error = rmse(preds, trues)\n#         mae_error = mae(preds, trues)\n\n#         print(f'Epoch {epoch} valid loss: {valid_loss}, rmse: {rmse_error}, mae: {mae_error}')\n        \n#         if valid_loss < best_loss:\n            \n#             best_loss = valid_loss\n#             early_stopping_counter = 0\n#             torch.save(forecaster.state_dict(), best_model_path)\n#             # best_model.load_state_dict(copy.deepcopy(classifier.state_dict()))\n#             print(f'Best model found! Loss: {valid_loss}')\n            \n#         else:\n#             # 驗證損失沒有改善，計數器加1\n#             early_stopping_counter += 1\n            \n#             # 如果計數器達到早期停止的耐心值，則停止訓練\n#             if early_stopping_counter >= early_stopping_patience:\n#                 print('Early stopping triggered.')\n#                 break\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:27:25.725443Z","iopub.execute_input":"2024-05-27T17:27:25.725695Z","iopub.status.idle":"2024-05-27T17:27:25.733067Z","shell.execute_reply.started":"2024-05-27T17:27:25.725661Z","shell.execute_reply":"2024-05-27T17:27:25.732196Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"def train_fst(n_epochs, train_dataloader, test_dataloader, best_model_path, early_stopping_patience=30):\n    \n    # 初始化變數\n    best_loss = float('inf')\n    early_stopping_counter = 0\n    early_stopping_patience = early_stopping_patience\n    \n    for epoch in range(1, n_epochs + 1):\n\n        classifier.train()\n\n        train_losses = []\n        \n        trues = np.array([])\n        preds = np.array([])\n\n        for i, (features, labels) in enumerate(train_dataloader):\n            \n            features = features.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n\n            out = classifier(features)\n            \n            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n            \n            loss = criterion(out.squeeze(), labels)\n            loss.backward()\n            optimizer.step()\n                    \n            # metrics calculate\n      \n            train_losses.append(loss.item())\n\n        precision, recall, _ = precision_recall_curve(trues, preds)\n        aucpr = auc(recall, precision)\n\n        fpr, tpr, _ = roc_curve(trues, preds)\n        roc_auc = auc(fpr, tpr)\n        \n\n        train_loss = np.mean(train_losses)\n        train_losses_for_epochs.append(train_loss) # Record Loss\n\n        print(f'Epoch {epoch} train loss: {train_loss}, auc: {roc_auc}, aucpr: {aucpr}', end = '; ')\n        \n        # Validate\n        classifier.eval()\n        valid_losses = []\n\n        trues = np.array([])\n        preds = np.array([])\n        \n        for i, (features, labels) in enumerate(test_dataloader):\n            \n            features = features.to(device)\n            labels = labels.to(device)\n\n            out = classifier(features)\n\n            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n            \n            loss = criterion(out.squeeze(), labels)\n\n            valid_losses.append(loss.item())\n        \n        precision, recall, _ = precision_recall_curve(trues, preds)\n        aucpr = auc(recall, precision)\n        \n        fpr, tpr, _ = roc_curve(trues, preds)\n        roc_auc = auc(fpr, tpr)\n        \n        valid_loss = np.mean(valid_losses)\n        valid_losses_for_epochs.append(valid_loss) # Record Loss\n        \n        print(f'Epoch {epoch} valid loss: {valid_loss}, auc: {roc_auc}, aucpr: {aucpr}')\n        \n\n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            early_stopping_counter = 0\n            torch.save(classifier.state_dict(), best_model_path)\n            # best_model.load_state_dict(copy.deepcopy(classifier.state_dict()))\n            print(f'Best model found! Loss: {valid_loss}')\n            \n        else:\n            # 驗證損失沒有改善，計數器加1\n            early_stopping_counter += 1\n            \n            # 如果計數器達到早期停止的耐心值，則停止訓練\n            if early_stopping_counter >= early_stopping_patience:\n                print('Early stopping triggered.')\n                break\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:27:25.734219Z","iopub.execute_input":"2024-05-27T17:27:25.734547Z","iopub.status.idle":"2024-05-27T17:27:25.751833Z","shell.execute_reply.started":"2024-05-27T17:27:25.734517Z","shell.execute_reply":"2024-05-27T17:27:25.750984Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"# Setup seed\nset_seed(seed)\n\n# Get GPU\ndevice_count = torch.cuda.device_count()\nnum_of_gpus = device_count\n\nfor i in range(device_count):\n    print(\"GPU {}: {}\".format(i, torch.cuda.get_device_name(i)))\n    gpu_id = i\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Save best model to \nsave_path = \"/kaggle/working\"\n\n# Define DataSet\ndirname = \"/kaggle/input/333333\"\ndir_list = os.listdir(dirname)\ndir_list = [f for f in dir_list if ( f.endswith('.csv') and (not 'sm' in f) ) ]\nprint(dir_list)\n\n# train_dates = ['03-26','04-01','04-10','04-17']\n# test_dates = ['05-07']\ntrain_dates = ['03-26', '04-01']\ntest_dates = ['04-10']\n    \n# train_dir_list = [f for f in dir_list if ( f.endswith('.csv') and ('All' in f) and days_in_file(f, train_dates) )]\n# test_dir_list = [f for f in dir_list if ( f.endswith('.csv') and ('All' in f) and days_in_file(f, test_dates) )]\n\ntrain_dir_list, test_dir_list = train_valid_split(dir_list, valid_ratio)\ntrain_dir_list += [f for f in os.listdir(dirname) if 'sm' in f]\n\n# features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n#         'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2',\n#         'nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1', 'nr-RSRP2', 'nr-RSRQ2' ]\nfeatures = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n        'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1','nr-RSRP', 'nr-RSRQ', 'nr-RSRP1', 'nr-RSRQ1']\n# features = ['LTE_HO', 'MN_HO', 'eNB_to_ENDC', 'gNB_Rel', 'gNB_HO', 'RLF', 'SCG_RLF',\n#         'num_of_neis', 'RSRP', 'RSRQ', 'RSRP1', 'RSRQ1', 'RSRP2', 'RSRQ2']\n\nnum_of_features = len(features)\n\n# target = ['LTE_HO', 'MN_HO'] # For eNB HO.\n# target = ['eNB_to_ENDC'] # Setup gNB\ntarget = ['gNB_Rel', 'gNB_HO'] # For gNB HO.\n# target = ['RLF'] # For RLF\n# target = ['SCG_RLF'] # For scg failure\n# target = ['dl-loss'] # For DL loss\n# target = ['ul-loss'] # For UL loss\n\n# Data\nprint('Loading training data...')\nX_train, y_train1, y_train2, split_time_train = ts_array_create(dirname, train_dir_list, time_seq)\n\ntrain_dataset = RNN_Dataset_simple(X_train, y_train1)\ntrain_dataloader2 = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n\nprint(X_train.shape)\nprint(y_train2.shape)\nprint(X_train)\nprint(y_train2)\n\ncond = y_train2 > 0\nX_train_fore = X_train[cond]\ny_train2_fore = y_train2[cond]\ntrain_dataset = RNN_Dataset_simple(X_train_fore, y_train2_fore)\ntrain_dataloader3 = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n\nprint(X_train_fore.shape)\nprint(y_train2_fore.shape)\nprint(X_train_fore)\nprint(y_train2_fore)\n\nprint('Loading testing data...')\nX_test, y_test1, y_test2, split_time_test = ts_array_create(dirname, test_dir_list, time_seq)\n\ntest_dataset = RNN_Dataset_simple(X_test, y_test1)\ntest_dataloader2 = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\ncond = y_test2 > 0\nX_test_fore = X_test[cond]\ny_test2_fore = y_test2[cond]\ntest_dataset = RNN_Dataset_simple(X_test_fore, y_test2_fore)\ntest_dataloader3 = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# cond = y_test2 > 0\n# X_test_fore = X_test[cond]\n# y_test2_fore = y_test[cond]\n# test_dataset = RNN_Dataset_simple(X_test_fore, y_test2_fore)\n# test_dataloader2 = DataLoader(test_dataset, batch_size=batch_size, shuffle=False","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:27:25.753152Z","iopub.execute_input":"2024-05-27T17:27:25.753392Z","iopub.status.idle":"2024-05-27T17:27:26.247826Z","shell.execute_reply.started":"2024-05-27T17:27:25.753371Z","shell.execute_reply":"2024-05-27T17:27:26.246870Z"},"trusted":true},"execution_count":130,"outputs":[{"name":"stdout","text":"Random seed set as 55688\nGPU 0: Tesla T4\nGPU 1: Tesla T4\n['2023-04-10_qc00_exp1_02_All.csv', '2023-04-10_qc01_exp1_01_All.csv', '2023-04-01_qc00_exp2_04_B1B3.csv', '2023-04-01_qc00_exp1_03_B1.csv', '2023-04-10_qc00_exp1_01_All.csv', '2023-04-01_qc00_exp2_01_B1B3.csv', '2023-04-01_qc00_exp1_01_B1.csv', '2023-04-10_qc00_exp2_02_B1.csv', '2023-04-10_qc00_exp3_01_LTE.csv', '2023-04-10_qc00_exp3_02_LTE.csv', '2023-04-01_qc00_exp1_04_B1.csv', '2023-04-01_qc00_exp2_03_B1B3.csv', '2023-04-01_qc00_exp1_02_B1.csv', '2023-04-01_qc00_exp1_06_B1.csv', '2023-04-01_qc00_exp2_02_B1B3.csv', '2023-04-10_qc00_exp2_01_B1.csv', '2023-04-01_qc00_exp1_05_B1.csv']\nLoading training data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/14 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"861d685422304a4c89e00738e4909da0"}},"metadata":{}},{"name":"stdout","text":"[[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [1. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n ...\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [1. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [1. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [1. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]]\n(3676, 20, 16)\n(3676,)\n[[[  0.           0.           0.         ...   0.           0.\n     0.        ]\n  [  0.           0.           0.         ...   0.           0.\n     0.        ]\n  [  0.           0.           0.         ...   0.           0.\n     0.        ]\n  ...\n  [  1.           0.           0.         ...   0.           0.\n     0.        ]\n  [  0.           0.           0.         ...   0.           0.\n     0.        ]\n  [  0.           0.           0.         ...   0.           0.\n     0.        ]]\n\n [[  0.           0.           0.         ...   0.           0.\n     0.        ]\n  [  0.           0.           0.         ...   0.           0.\n     0.        ]\n  [  0.           0.           0.         ...   0.           0.\n     0.        ]\n  ...\n  [  0.           0.           0.         ...   0.           0.\n     0.        ]\n  [  0.           0.           0.         ...   0.           0.\n     0.        ]\n  [  0.           0.           0.         ...   0.           0.\n     0.        ]]\n\n [[  0.           0.           0.         ...   0.           0.\n     0.        ]\n  [  0.           0.           0.         ...   0.           0.\n     0.        ]\n  [  0.           0.           0.         ...   0.           0.\n     0.        ]\n  ...\n  [  0.           0.           0.         ...   0.           0.\n     0.        ]\n  [  0.           0.           0.         ...   0.           0.\n     0.        ]\n  [  0.           0.           0.         ...   0.           0.\n     0.        ]]\n\n ...\n\n [[  0.           0.           0.         ... -10.82371429   0.\n     0.        ]\n  [  0.           0.           0.         ... -11.08983333 -87.719\n   -18.914     ]\n  [  0.           0.           0.         ... -12.099      -89.781\n   -18.914     ]\n  ...\n  [  0.           0.           0.         ... -17.297      -87.94383333\n   -14.42566667]\n  [  0.           0.           0.         ... -16.96616667 -83.78633333\n   -13.26566667]\n  [  0.           0.           0.         ... -15.23683333 -81.19266667\n   -13.004     ]]\n\n [[  0.           0.           0.         ... -11.08983333 -87.719\n   -18.914     ]\n  [  0.           0.           0.         ... -12.099      -89.781\n   -18.914     ]\n  [  0.           0.           0.         ... -12.841      -95.58333333\n   -20.099     ]\n  ...\n  [  0.           0.           0.         ... -16.96616667 -83.78633333\n   -13.26566667]\n  [  0.           0.           0.         ... -15.23683333 -81.19266667\n   -13.004     ]\n  [  0.           0.           0.         ... -12.25328571 -94.59042857\n   -19.98228571]]\n\n [[  0.           0.           0.         ... -12.099      -89.781\n   -18.914     ]\n  [  0.           0.           0.         ... -12.841      -95.58333333\n   -20.099     ]\n  [  0.           0.           0.         ... -12.06585714 -93.47542857\n   -19.14528571]\n  ...\n  [  0.           0.           0.         ... -15.23683333 -81.19266667\n   -13.004     ]\n  [  0.           0.           0.         ... -12.25328571 -94.59042857\n   -19.98228571]\n  [  0.           0.           0.         ... -12.93083333 -89.37516667\n   -18.12116667]]]\n[0 0 0 ... 0 0 0]\n(704, 20, 16)\n(704,)\n[[[   0.            0.            0.         ...  -15.21871429\n    -88.865       -17.13285714]\n  [   0.            0.            0.         ...  -16.68883333\n    -86.47766667  -14.27483333]\n  [   0.            0.            0.         ...  -16.34766667\n    -88.3945      -14.24233333]\n  ...\n  [   0.            0.            0.         ...  -10.8855\n   -115.62233333  -26.63      ]\n  [   0.            0.            0.         ...  -10.66033333\n   -124.96483333  -33.97      ]\n  [   0.            0.            0.         ...  -11.1055\n    -82.935       -14.026     ]]\n\n [[   0.            0.            0.         ...  -16.68883333\n    -86.47766667  -14.27483333]\n  [   0.            0.            0.         ...  -16.34766667\n    -88.3945      -14.24233333]\n  [   0.            1.            0.         ...  -14.25266667\n    -92.28383333  -15.71216667]\n  ...\n  [   0.            0.            0.         ...  -10.66033333\n   -124.96483333  -33.97      ]\n  [   0.            0.            0.         ...  -11.1055\n    -82.935       -14.026     ]\n  [   0.            0.            0.         ...  -11.91285714\n    -82.28228571  -16.12842857]]\n\n [[   0.            0.            0.         ...  -16.34766667\n    -88.3945      -14.24233333]\n  [   0.            1.            0.         ...  -14.25266667\n    -92.28383333  -15.71216667]\n  [   0.            0.            0.         ...  -13.58371429\n    -90.38185714  -16.91628571]\n  ...\n  [   0.            0.            0.         ...  -11.1055\n    -82.935       -14.026     ]\n  [   0.            0.            0.         ...  -11.91285714\n    -82.28228571  -16.12842857]\n  [   0.            0.            0.         ...  -11.396\n    -89.088       -18.164     ]]\n\n ...\n\n [[   0.            0.            0.         ...  -12.401\n   -100.50383333  -18.39583333]\n  [   0.            0.            0.         ...  -12.51433333\n   -100.53016667  -17.94533333]\n  [   0.            0.            0.         ...  -13.164\n    -99.29142857  -16.25228571]\n  ...\n  [   0.            1.            0.         ...  -12.90366667\n    -85.7755      -15.7285    ]\n  [   0.            0.            0.         ...  -15.395\n    -76.08583333  -13.17966667]\n  [   0.            0.            0.         ...  -12.8125\n    -84.8905      -18.2345    ]]\n\n [[   0.            0.            0.         ...  -12.51433333\n   -100.53016667  -17.94533333]\n  [   0.            0.            0.         ...  -13.164\n    -99.29142857  -16.25228571]\n  [   0.            0.            0.         ...  -14.7655\n    -93.4505      -13.45183333]\n  ...\n  [   0.            0.            0.         ...  -15.395\n    -76.08583333  -13.17966667]\n  [   0.            0.            0.         ...  -12.8125\n    -84.8905      -18.2345    ]\n  [   0.            0.            0.         ...  -13.845\n    -89.65366667  -18.48983333]]\n\n [[   0.            0.            0.         ...  -13.164\n    -99.29142857  -16.25228571]\n  [   0.            0.            0.         ...  -14.7655\n    -93.4505      -13.45183333]\n  [   0.            0.            0.         ...  -16.68483333\n    -96.427       -12.97266667]\n  ...\n  [   0.            0.            0.         ...  -12.8125\n    -84.8905      -18.2345    ]\n  [   0.            0.            0.         ...  -13.845\n    -89.65366667  -18.48983333]\n  [   0.            0.            0.         ...  -15.74328571\n    -83.38057143  -15.48985714]]]\n[10  9  8  7  6  5  4  3  2  1  2  1 10  9  8  7  6  5  4  3  2  1 10  9\n  8  7  6  5  4  3  2  1  3  2  1 10  9  8  7  6  5  4  3  2  1 10  9  8\n  7  6  5  4  3  2  1  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2\n  1  3  2  1 10  9  8  7  6  5  4  3  2  1  1  2  1 10  9  8  7  6  5  4\n  3  2  1 10  9  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1  3\n  2  1 10  9  8  7  6  5  4  3  2  1  1  6  5  4  3  2  1 10  9  8  7  6\n  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1  9  8  7  6  5  4  3  2  1\n 10  9  8  7  6  5  4  3  2  1  9  8  7  6  5  4  3  2  1  1  3  2  1 10\n  9  8  7  6  5  4  3  2  1  4  3  2  1  6  5  4  3  2  1  3  2  1 10  9\n  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1  2  1 10  9  8  7\n  6  5  4  3  2  1  4  3  2  1  7  6  5  4  3  2  1  1 10  9  8  7  6  5\n  4  3  2  1  1 10  9  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2\n  1 10  9  8  7  6  5  4  3  2  1  3  2  1 10  9  8  7  6  5  4  3  2  1\n 10  9  8  7  6  5  4  3  2  1  9  8  7  6  5  4  3  2  1  6  5  4  3  2\n  1  3  2  1 10  9  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1\n 10  9  8  7  6  5  4  3  2  1  2  1 10  9  8  7  6  5  4  3  2  1 10  9\n  8  7  6  5  4  3  2  1  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3\n  2  1 10  9  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1  6  5\n  4  3  2  1  3  2  1 10  9  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4\n  3  2  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1 10  9  8  7  6  5\n  4  3  2  1  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1 10  9  8  7\n  6  5  4  3  2  1  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1\n 10  9  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1 10  9  8  7\n  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3\n  2  1  3  2  1 10  9  8  7  6  5  4  3  2  1  2  1  4  3  2  1  3  2  1\n 10  9  8  7  6  5  4  3  2  1  1 10  9  8  7  6  5  4  3  2  1 10  9  8\n  7  6  5  4  3  2  1  3  2  1 10  9  8  7  6  5  4  3  2  1  8  7  6  5\n  4  3  2  1 10  9  8  7  6  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1\n  5  4  3  2  1 10  9  8  7  6  5  4  3  2  1  8  7  6  5  4  3  2  1  5\n  4  3  2  1  4  3  2  1]\nLoading testing data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfb0ce7eeb7041368bcb16efa733824c"}},"metadata":{}},{"name":"stdout","text":"[[[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n ...\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]\n\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]]]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For record loss\ntrain_losses_for_epochs = []\nvalidation_losses_for_epochs = []\nvalid_losses_for_epochs = []\n\n# Save best model to ... \nbest_model_path = os.path.join(save_path, 'lte_HO_fst_RNN.pt')\n\nearly_stopping_patience = 50","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:27:26.249085Z","iopub.execute_input":"2024-05-27T17:27:26.249367Z","iopub.status.idle":"2024-05-27T17:27:26.254959Z","shell.execute_reply.started":"2024-05-27T17:27:26.249343Z","shell.execute_reply":"2024-05-27T17:27:26.253986Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"train_fst(n_epochs, train_dataloader2, test_dataloader2, best_model_path, early_stopping_patience)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:27:26.256382Z","iopub.execute_input":"2024-05-27T17:27:26.256709Z","iopub.status.idle":"2024-05-27T17:27:44.501774Z","shell.execute_reply.started":"2024-05-27T17:27:26.256679Z","shell.execute_reply":"2024-05-27T17:27:44.500801Z"},"trusted":true},"execution_count":132,"outputs":[{"name":"stdout","text":"Epoch 1 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 1 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nBest model found! Loss: 0.24425184055284263\nEpoch 2 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 2 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 3 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 3 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 4 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 4 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 5 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 5 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 6 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 6 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 7 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 7 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 8 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 8 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 9 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 9 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 10 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 10 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 11 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 11 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 12 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 12 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 13 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 13 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 14 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 14 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 15 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 15 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 16 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 16 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 17 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 17 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 18 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 18 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 19 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 19 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 20 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 20 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 21 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 21 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 22 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 22 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 23 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 23 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 24 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 24 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 25 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 25 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 26 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 26 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 27 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 27 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 28 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 28 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 29 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 29 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 30 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 30 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 31 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 31 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 32 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 32 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 33 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 33 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 34 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 34 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 35 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 35 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 36 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 36 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 37 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 37 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 38 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 38 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 39 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 39 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 40 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 40 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 41 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 41 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 42 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 42 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 43 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 43 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 44 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 44 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 45 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 45 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 46 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 46 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 47 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 47 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 48 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 48 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 49 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 49 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 50 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 50 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEpoch 51 train loss: 0.32767931038079834, auc: 0.87524184051144, aucpr: 0.5108206833408775; Epoch 51 valid loss: 0.24425184055284263, auc: 0.9210165559422123, aucpr: 0.49782013248989504\nEarly stopping triggered.\n","output_type":"stream"}]},{"cell_type":"code","source":"  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test\ndef test2(test_dataloader):\n    best_model = RNN_Fst(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n    best_model.load_state_dict(torch.load(best_model_path))\n    best_model.eval()\n\n    with torch.no_grad():\n        \n        best_model.eval()\n        valid_losses = []\n\n        trues = np.array([])\n        preds = np.array([])\n        \n        for i, (features, labels) in enumerate(test_dataloader):\n            \n            features = features.to(device)\n            labels = labels.to(device)\n\n            out = best_model(features)\n\n            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n            \n            loss = criterion(out.squeeze(), labels)\n\n            valid_losses.append(loss.item())\n        \n        precision, recall, _ = precision_recall_curve(trues, preds)\n        aucpr = auc(recall, precision)\n        threshold = 0.5\n        p = precision_score(trues, [1 if pred > threshold else 0 for pred in preds])\n        r = recall_score(trues, [1 if pred > threshold else 0 for pred in preds])\n        f1 = f1_score(trues, [1 if pred > threshold else 0 for pred in preds])\n        \n        fpr, tpr, _ = roc_curve(trues, preds)\n        roc_auc = auc(fpr, tpr)\n        \n        valid_loss = np.mean(valid_losses)\n\n        print(f'valid loss {valid_loss}, roc_auc {roc_auc}, aucpr {aucpr}')\n        \n        return valid_loss, roc_auc, aucpr, p, r, f1\n        \ntest2(test_dataloader2)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:00:09.908287Z","iopub.execute_input":"2024-05-27T17:00:09.909104Z","iopub.status.idle":"2024-05-27T17:00:09.952598Z","shell.execute_reply.started":"2024-05-27T17:00:09.909068Z","shell.execute_reply":"2024-05-27T17:00:09.951666Z"}}},{"cell_type":"code","source":"# print(\"Model architecture when saving:\")\n# print(model)\n\n# print(\"Model architecture when loading:\")\n# print(best_model)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:27:44.503036Z","iopub.execute_input":"2024-05-27T17:27:44.503348Z","iopub.status.idle":"2024-05-27T17:27:44.507888Z","shell.execute_reply.started":"2024-05-27T17:27:44.503320Z","shell.execute_reply":"2024-05-27T17:27:44.506695Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"# Test\ndef test2(test_dataloader):\n    best_model = RNN_Fst(input_dim, out_dim, hidden_dim, num_layer, dropout, rnn).to(device)\n    best_model.load_state_dict(torch.load(best_model_path))\n    best_model.eval()\n#     print(\"Model architecture when saving:\")\n#     print(model)\n\n#     print(\"Model architecture when loading:\")\n#     print(best_model)\n    \n    with torch.no_grad():\n        \n        best_model.eval()\n        valid_losses = []\n\n        trues = np.array([])\n        preds = np.array([])\n        \n        for i, (features, labels) in enumerate(test_dataloader):\n            \n            features = features.to(device)\n            labels = labels.to(device)\n\n            out = best_model(features)\n\n            trues = np.concatenate((trues, labels.cpu().numpy()), axis=0)\n            preds = np.concatenate((preds, out.squeeze().detach().cpu().numpy()), axis=0)\n            \n            loss = criterion(out.squeeze(), labels)\n\n            valid_losses.append(loss.item())\n        \n        precision, recall, _ = precision_recall_curve(trues, preds)\n        aucpr = auc(recall, precision)\n        threshold = 0.5\n        p = precision_score(trues, [1 if pred > threshold else 0 for pred in preds])\n        r = recall_score(trues, [1 if pred > threshold else 0 for pred in preds])\n        f1 = f1_score(trues, [1 if pred > threshold else 0 for pred in preds])\n        \n        fpr, tpr, _ = roc_curve(trues, preds)\n        roc_auc = auc(fpr, tpr)\n        \n        valid_loss = np.mean(valid_losses)\n\n        print(f'valid loss {valid_loss}, roc_auc {roc_auc}, aucpr {aucpr}')\n        \n        return valid_loss, roc_auc, aucpr, p, r, f1\n        \ntest2(test_dataloader1)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:31:01.498912Z","iopub.execute_input":"2024-05-27T17:31:01.499790Z","iopub.status.idle":"2024-05-27T17:31:01.878384Z","shell.execute_reply.started":"2024-05-27T17:31:01.499757Z","shell.execute_reply":"2024-05-27T17:31:01.877105Z"},"trusted":true},"execution_count":139,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[139], line 50\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, roc_auc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroc_auc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, aucpr \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maucpr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m valid_loss, roc_auc, aucpr, p, r, f1\n\u001b[0;32m---> 50\u001b[0m \u001b[43mtest2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataloader1\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[139], line 3\u001b[0m, in \u001b[0;36mtest2\u001b[0;34m(test_dataloader)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest2\u001b[39m(test_dataloader):\n\u001b[0;32m----> 3\u001b[0m     best_model \u001b[38;5;241m=\u001b[39m \u001b[43mRNN_Fst\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     best_model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(best_model_path))\n\u001b[1;32m      5\u001b[0m     best_model\u001b[38;5;241m.\u001b[39meval()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:213\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 213\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecurse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# Resets _flat_weights\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# Note: be v. careful before removing this, as 3rd party device types\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# likely rely on this behavior to properly .to() modules like LSTM.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_flat_weights()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}