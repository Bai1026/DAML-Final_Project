{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from utils.F import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "seed = 55688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d88c82853e46b69cc8c8322f5c313e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincentpai/bai/DAML_Final_Project/HO-Prediction/main/utils/F.py:42: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[ffill_cols] = df[ffill_cols].ffill()\n"
     ]
    },
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m two_hot_vec_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE-UTRAN-eventA3\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meventA5\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNR-eventA3\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meventB1-NR-r15\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLTE_HO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMN_HO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMN_HO_to_eNB\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSN_setup\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSN_Rel\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSN_HO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRLF_II\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRLF_III\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSCG_RLF\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     21\u001b[0m merged_cols \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLTE_HO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMN_HO_to_eNB\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLTE_HO\u001b[39m\u001b[38;5;124m'\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRLF_II\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRLF_III\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRLF\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m---> 23\u001b[0m X_train, y_cls_train, y_fst_train, record_train \u001b[38;5;241m=\u001b[39m \u001b[43mts_array_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_seq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mffill_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtwo_hot_vec_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmerged_cols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m X_train_2d \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mreshape(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m X_test1, y_cls_test1, y_fst_test1, record_test1 \u001b[38;5;241m=\u001b[39m ts_array_create(test_data_list1, time_seq_len, pred_time, features, ffill_cols,two_hot_vec_cols,merged_cols)\n",
      "File \u001b[0;32m~/bai/DAML_Final_Project/HO-Prediction/main/utils/F.py:32\u001b[0m, in \u001b[0;36mts_array_create\u001b[0;34m(data_list, time_seq_len, pred_time, features, ffill_cols, two_hot_cols, merged_cols)\u001b[0m\n\u001b[1;32m     29\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m tqdm(data_list):\n\u001b[0;32m---> 32\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Hard to change to a feature, delete it now.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPCI\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEARFCN\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNR-PCI\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/wmnlab/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/wmnlab/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/wmnlab/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/wmnlab/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/wmnlab/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:581\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "# Read Data\n",
    "data_folder = '../data/vv2'\n",
    "data_list = [os.path.join(data_folder, file) for file in os.listdir(data_folder)]\n",
    "data_list.remove(os.path.join(data_folder, 'record.csv'))\n",
    "test_data_list1 = [x for x in data_list if ('2023-11-01' in x and '#02' in x)] # 同一天機捷 \n",
    "test_data_list2 = [x for x in data_list if '2023-11-02' in x] # 機捷\n",
    "test_data_list3 = [x for x in data_list if '2023-11-09' in x] # 棕線\n",
    "test_data_list4 = test_data_list1 + test_data_list2 + test_data_list3\n",
    "train_data_list = [x for x in data_list if x not in test_data_list1 + test_data_list2 + test_data_list3]\n",
    "\n",
    "time_seq_len = 10\n",
    "pred_time = 3\n",
    "\n",
    "features = ['num_of_neis', 'RSRP','RSRQ','RSRP1','RSRQ1','nr-RSRP','nr-RSRQ','nr-RSRP1','nr-RSRQ1',\n",
    "            'E-UTRAN-eventA3','eventA5','NR-eventA3','eventB1-NR-r15',\n",
    "            'LTE_HO','MN_HO','MN_HO_to_eNB','SN_setup','SN_Rel','SN_HO', \n",
    "            'RLF_II', 'RLF_III','SCG_RLF']\n",
    "ffill_cols = ['RSRP1', 'RSRQ1']\n",
    "two_hot_vec_cols = ['E-UTRAN-eventA3','eventA5','NR-eventA3','eventB1-NR-r15',\n",
    "            'LTE_HO','MN_HO','MN_HO_to_eNB','SN_setup','SN_Rel','SN_HO','RLF_II','RLF_III','SCG_RLF']\n",
    "merged_cols = [['LTE_HO', 'MN_HO_to_eNB', 'LTE_HO'], ['RLF_II', 'RLF_III', 'RLF']]\n",
    "\n",
    "X_train, y_cls_train, y_fst_train, record_train = ts_array_create(train_data_list, time_seq_len, pred_time, features, ffill_cols,two_hot_vec_cols,merged_cols)\n",
    "X_train_2d = X_train.reshape(X_train.shape[0], -1)\n",
    "\n",
    "X_test1, y_cls_test1, y_fst_test1, record_test1 = ts_array_create(test_data_list1, time_seq_len, pred_time, features, ffill_cols,two_hot_vec_cols,merged_cols)\n",
    "X_test1_2d = X_test1.reshape(X_test1.shape[0], -1)\n",
    "\n",
    "X_test2, y_cls_test2, y_fst_test2, record_test2 = ts_array_create(test_data_list2, time_seq_len, pred_time, features, ffill_cols,two_hot_vec_cols,merged_cols)\n",
    "X_test2_2d = X_test2.reshape(X_test2.shape[0], -1)\n",
    "\n",
    "X_test3, y_cls_test3, y_fst_test3, record_test3 = ts_array_create(test_data_list3, time_seq_len, pred_time, features, ffill_cols,two_hot_vec_cols,merged_cols)\n",
    "X_test3_2d = X_test3.reshape(X_test3.shape[0], -1)\n",
    "\n",
    "X_test4, y_cls_test4, y_fst_test4, record_test4 = ts_array_create(test_data_list4, time_seq_len, pred_time, features, ffill_cols,two_hot_vec_cols,merged_cols)\n",
    "X_test4_2d = X_test4.reshape(X_test4.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_cls_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count RLF number\n",
    "rlf_num_train = count_rlf(train_data_list)\n",
    "rlf_num_test1 = count_rlf(test_data_list1)\n",
    "rlf_num_test2 = count_rlf(test_data_list2)\n",
    "rlf_num_test3 = count_rlf(test_data_list3)\n",
    "rlf_num_test4 = count_rlf(test_data_list4)\n",
    "print(f'RLF # in training data: {rlf_num_train}\\nRLF # in testing data1: {rlf_num_test1}\\nRLF # in testing data2: {rlf_num_test2}\\nRLF # in testing data3: {rlf_num_test3}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將數據轉換為 DMatrix 格式\n",
    "dtrain = xgb.DMatrix(X_train_2d, label=y_cls_train)\n",
    "dtest1 = xgb.DMatrix(X_test1_2d, label=y_cls_test1)\n",
    "dtest2 = xgb.DMatrix(X_test2_2d, label=y_cls_test2)\n",
    "dtest3 = xgb.DMatrix(X_test3_2d, label=y_cls_test3)\n",
    "dtest4 = xgb.DMatrix(X_test4_2d, label=y_cls_test4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic', \n",
    "    'eval_metric': 'error',  \n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'alpha': 0.01,\n",
    "    'lambda':1.0,\n",
    "    'seed': seed,\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda:0'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Create\n",
    "num_rounds = 1000\n",
    "watchlist = [(dtrain, 'train'), (dtest1, 'test1'), (dtest2, 'test2'), (dtest3, 'test3')] \n",
    "model = xgb.train(params, dtrain, num_rounds, evals=watchlist, early_stopping_rounds=200,  verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric calculate\n",
    "performance(model, dtrain, y_cls_train)\n",
    "performance(model, dtest1, y_cls_test1)\n",
    "performance(model, dtest2, y_cls_test2)\n",
    "performance(model, dtest3, y_cls_test3)\n",
    "performance(model, dtest4, y_cls_test4)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "save_path = '../model/rlf_cls_xgb.json'\n",
    "config = model.save_model(save_path)\n",
    "\n",
    "# how to load\n",
    "# model2 = xgb.Booster()\n",
    "# model2.load_model(save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic', \n",
    "    'eval_metric': 'error',  \n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8, # on data\n",
    "    'colsample_bytree': 0.8, # on feature\n",
    "    'lambda': 0.01, # L2\n",
    "    'alpha': 0.01, # L1\n",
    "    'seed': seed,\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda:0'\n",
    "}\n",
    "\n",
    "# Model Create\n",
    "num_rounds = 200\n",
    "# watchlist = [(dtrain, 'train')]\n",
    "# watchlist = [(dtrain, 'train'), (dtest4, 'test4')] \n",
    "watchlist = [(dtrain, 'train'), (dtest1, 'test1'), (dtest2, 'test2'), (dtest3, 'test3')] \n",
    "model = xgb.train(params, dtrain, num_rounds, evals=watchlist,  early_stopping_rounds=20, verbose_eval=True)\n",
    "\n",
    "# Metric calculate\n",
    "performance(model, dtrain, y_cls_train)\n",
    "performance(model, dtest1, y_cls_test1)\n",
    "performance(model, dtest2, y_cls_test2)\n",
    "performance(model, dtest3, y_cls_test3)\n",
    "performance(model, dtest4, y_cls_test4)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dtest1, 'test1'), (dtest2, 'test2'), (dtest3, 'test3')] \n",
    "    \n",
    "# Define parameters range\n",
    "learning_rate_values = [0.05, 0.1, 0.2]\n",
    "max_depth_values = [4, 5, 6, 7, 8]\n",
    "subsample_values = [0.8, 0.9, 1.0]\n",
    "colsample_bytree_values = [0.8, 0.9, 1.0]\n",
    "alphas = [0.01,0.1,1]\n",
    "lambdas = [0.01,0.1,1]\n",
    "num_rounds_values = [50,100,200,300]\n",
    "r = product(learning_rate_values, max_depth_values, subsample_values, colsample_bytree_values, alphas, lambdas)\n",
    "\n",
    "savefile = '../info/xgb_record_no_early_stopping.csv'\n",
    "with open(savefile, 'w') as f:\n",
    "    print('lr, max_d, s_sample, cols_bytree, alpha, lambda, n,ACC(train), AUC(train), AUCPR(train), P(train), R(train), F1(train), ACC(train), AUC(test), AUCPR(test), P(test), R(test), F1(test)',file=f)\n",
    "    for lr, d, s, cbt, a, l in tqdm(r):\n",
    "        params = {'objective': 'binary:logistic', 'eval_metric': 'error',  'seed': seed,'tree_method': 'hist','device': 'cuda:0'}\n",
    "        params['learning_rate'] = lr\n",
    "        params['max_depth'] = d\n",
    "        params['subsample'] = s\n",
    "        params['colsample_bytree'] = cbt\n",
    "        params['alpha'] = a\n",
    "        params['lambda'] = l\n",
    "        for num_rounds in num_rounds_values:\n",
    "            model = xgb.train(params, dtrain, num_rounds, evals=watchlist,  early_stopping_rounds=20, verbose_eval=False)\n",
    "            \n",
    "            record = [lr, d, s, cbt, a, l, num_rounds]\n",
    "            record+=list(performance(model, dtrain, y_cls_train))\n",
    "            performance(model, dtest1, y_cls_test1)\n",
    "            performance(model, dtest2, y_cls_test2)\n",
    "            performance(model, dtest3, y_cls_test3)\n",
    "            record += list(performance(model, dtest4, y_cls_test4))\n",
    "            \n",
    "            params.clear()\n",
    "            record = [str(x) for x in record]\n",
    "            \n",
    "            print(','.join(record),end='\\n', file=f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary:logistic', \n",
    "    'eval_metric': 'error',  \n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8, # on data\n",
    "    'colsample_bytree': 0.8, # on feature\n",
    "    'lambda': 0, # L2\n",
    "    'alpha': 0, # L1\n",
    "    'seed': seed,\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda:0'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature columns\n",
    "cols = ['num_of_neis', 'RSRP','RSRQ','RSRP1','RSRQ1','nr-RSRP','nr-RSRQ','nr-RSRP1','nr-RSRQ1',\n",
    "        'E-UTRAN-eventA3','eventA5','NR-eventA3','eventB1-NR-r15',\n",
    "        'LTE_HO','MN_HO','SN_setup','SN_Rel','SN_HO', 'RLF','SCG_RLF']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP, FP, TN, FN = get_pred_result_ind(model, dtest4, y_cls_test4, X_test4)\n",
    "len(TP), len(FP), len(TN), len(FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP_cause = []\n",
    "for i in FP:\n",
    "    count = 1\n",
    "    while count<=pred_time:\n",
    "        df, ff = find_original_input(i+count, record_test4, time_seq_len, ffill_cols)\n",
    "        series = df.iloc[-1]\n",
    "        if (series['RLF_II'] or series['RLF_III']):\n",
    "            FP_cause.append(series['RLF_cause'])\n",
    "            break\n",
    "        count+=1        \n",
    "\n",
    "count = {'handoverFailure':0, 'otherFailure':0, 'reconfigurationFailure':0}\n",
    "for c in FP_cause:\n",
    "    for type in count.keys():\n",
    "        if c == type: count[type] += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify \n",
    "dtest = dtest4\n",
    "y_cls, y_fst, X_test = y_cls_test4, y_fst_test4, X_test4\n",
    "\n",
    "y_pred_proba = model.predict(dtest) \n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "FP_input = []\n",
    "FP_input_ind = []\n",
    "FP_time = []\n",
    "\n",
    "FN_input = []\n",
    "FN_input_ind = []\n",
    "\n",
    "TP_input = []\n",
    "TP_input_ind = []\n",
    "TP_time = []\n",
    "\n",
    "for i, (pred, label, t, x) in enumerate(zip(y_pred, y_cls, y_fst, X_test)):\n",
    "    if pred != label:\n",
    "        if label == 1: # FP analysis\n",
    "            FP_time.append(t)\n",
    "            x = pd.DataFrame(x, columns=features)\n",
    "            FP_input.append(x)\n",
    "            FP_input_ind.append(i)\n",
    "        else: # FN analysis\n",
    "            x = pd.DataFrame(x, columns=features)\n",
    "            FN_input.append(x)\n",
    "            FN_input_ind.append(i)\n",
    "    else: \n",
    "        if label == 1: # TP analysis\n",
    "            TP_time.append(t)\n",
    "            x = pd.DataFrame(x, columns=features)\n",
    "            TP_input.append(x)\n",
    "            TP_input_ind.append(i)\n",
    "            \n",
    "len(TP_input), len(FP_input), len(FN_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel filename\n",
    "file_record = record_test4\n",
    "excel_file = '../info/TP_data.xlsx'\n",
    "input_ind = TP_input_ind\n",
    "\n",
    "# ExcelWriter \n",
    "with pd.ExcelWriter(excel_file, engine='xlsxwriter') as writer:\n",
    "    for ind in tqdm(input_ind):\n",
    "        # x = X_test[ind] \n",
    "        # df = pd.DataFrame(x, columns=features)\n",
    "        df, tar_file = find_original_input(ind, file_record, time_seq_len)\n",
    "        df['filename'] = [tar_file] + [None]*(len(df) - 1)\n",
    "        df.to_excel(writer, sheet_name=f'{ind}', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, _ = find_original_input(316, file_record, time_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(TP_input_ind[:20])\n",
    "pprint(TP_time[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_test4_2d[621]\n",
    "x = np.expand_dims(x, axis=0)\n",
    "X = xgb.DMatrix(x)\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(FN_input)/1490"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failed CDF\n",
    "sorted_data = np.sort(FP_time)\n",
    "cumulative_distribution = np.arange(1, len(sorted_data) + 1) / (len(sorted_data)+len(FN_input))\n",
    "\n",
    "plt.ylim([0,1])\n",
    "plt.plot(sorted_data, cumulative_distribution, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Time away from RLF (second)')\n",
    "# plt.ylabel('Cumulative Distribution Function (CDF)')\n",
    "plt.title('CDF of the false prediced Data')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "importance = model.get_score(importance_type='gain')\n",
    "\n",
    "sorted_importance = {}\n",
    "for k, v in importance.items():\n",
    "    num = int(k[1:])\n",
    "    feature_name = features[num%len(features)]\n",
    "    sorted_importance[f'{feature_name} {time_seq_len-num//34}'] = v\n",
    "\n",
    "sorted_importance = sorted(sorted_importance.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Feature Importance\n",
    "top_features = 20\n",
    "\n",
    "data, labels = [], []\n",
    "for f, score in reversed(sorted_importance[:top_features]):\n",
    "    data.append(round(score))\n",
    "    labels.append(f)\n",
    "\n",
    "bars = plt.barh(labels, data)\n",
    "plt.bar_label(bars)\n",
    "\n",
    "# 設置標題和標籤\n",
    "plt.title('Feature importance')\n",
    "plt.xlabel('F score (gain)')\n",
    "plt.ylabel('Features')\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "# 顯示圖表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
