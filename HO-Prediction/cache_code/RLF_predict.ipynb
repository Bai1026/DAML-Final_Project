{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from utils.F import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "seed = 55688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "data_folder = '/home/wmnlab/Documents/sheng-ru/HO-Prediction/data/vv2'\n",
    "data_list = [os.path.join(data_folder, file) for file in os.listdir(data_folder)]\n",
    "data_list.remove(os.path.join(data_folder, 'record.csv'))\n",
    "test_data_list1 = [x for x in data_list if ('2023-11-01' in x and '#02' in x)] # 同一天機捷 \n",
    "test_data_list2 = [x for x in data_list if '2023-11-02' in x] # 機捷\n",
    "test_data_list3 = [x for x in data_list if '2023-11-09' in x] # 棕線\n",
    "test_data_list4 = test_data_list1 + test_data_list2 + test_data_list3\n",
    "train_data_list = [x for x in data_list if x not in test_data_list1 + test_data_list2 + test_data_list3]\n",
    "\n",
    "time_seq_len = 10\n",
    "pred_time = 3\n",
    "\n",
    "features = ['num_of_neis', 'RSRP','RSRQ','RSRP1','RSRQ1','nr-RSRP','nr-RSRQ','nr-RSRP1','nr-RSRQ1',\n",
    "            'E-UTRAN-eventA3','eventA5','NR-eventA3','eventB1-NR-r15',\n",
    "            'LTE_HO','MN_HO','MN_HO_to_eNB','SN_setup','SN_Rel','SN_HO', \n",
    "            'RLF_II', 'RLF_III','SCG_RLF']\n",
    "ffill_cols = ['RSRP1', 'RSRQ1']\n",
    "two_hot_vec_cols = ['E-UTRAN-eventA3','eventA5','NR-eventA3','eventB1-NR-r15',\n",
    "            'LTE_HO','MN_HO','MN_HO_to_eNB','SN_setup','SN_Rel','SN_HO','RLF_II','RLF_III','SCG_RLF']\n",
    "merged_cols = [['LTE_HO', 'MN_HO_to_eNB', 'LTE_HO'], ['RLF_II', 'RLF_III', 'RLF']]\n",
    "\n",
    "X_train, y_cls_train, y_fst_train, record_train = ts_array_create(train_data_list, time_seq_len, pred_time, features, ffill_cols,two_hot_vec_cols,merged_cols)\n",
    "X_train_2d = X_train.reshape(X_train.shape[0], -1)\n",
    "\n",
    "X_test1, y_cls_test1, y_fst_test1, record_test1 = ts_array_create(test_data_list1, time_seq_len, pred_time, features, ffill_cols,two_hot_vec_cols,merged_cols)\n",
    "X_test1_2d = X_test1.reshape(X_test1.shape[0], -1)\n",
    "\n",
    "X_test2, y_cls_test2, y_fst_test2, record_test2 = ts_array_create(test_data_list2, time_seq_len, pred_time, features, ffill_cols,two_hot_vec_cols,merged_cols)\n",
    "X_test2_2d = X_test2.reshape(X_test2.shape[0], -1)\n",
    "\n",
    "X_test3, y_cls_test3, y_fst_test3, record_test3 = ts_array_create(test_data_list3, time_seq_len, pred_time, features, ffill_cols,two_hot_vec_cols,merged_cols)\n",
    "X_test3_2d = X_test3.reshape(X_test3.shape[0], -1)\n",
    "\n",
    "X_test4, y_cls_test4, y_fst_test4, record_test4 = ts_array_create(test_data_list4, time_seq_len, pred_time, features, ffill_cols,two_hot_vec_cols,merged_cols)\n",
    "X_test4_2d = X_test4.reshape(X_test4.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_cls_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count RLF number\n",
    "rlf_num_train = count_rlf(train_data_list)\n",
    "rlf_num_test1 = count_rlf(test_data_list1)\n",
    "rlf_num_test2 = count_rlf(test_data_list2)\n",
    "rlf_num_test3 = count_rlf(test_data_list3)\n",
    "rlf_num_test4 = count_rlf(test_data_list4)\n",
    "print(f'RLF # in training data: {rlf_num_train}\\nRLF # in testing data1: {rlf_num_test1}\\nRLF # in testing data2: {rlf_num_test2}\\nRLF # in testing data3: {rlf_num_test3}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將數據轉換為 DMatrix 格式\n",
    "dtrain = xgb.DMatrix(X_train_2d, label=y_cls_train)\n",
    "dtest1 = xgb.DMatrix(X_test1_2d, label=y_cls_test1)\n",
    "dtest2 = xgb.DMatrix(X_test2_2d, label=y_cls_test2)\n",
    "dtest3 = xgb.DMatrix(X_test3_2d, label=y_cls_test3)\n",
    "dtest4 = xgb.DMatrix(X_test4_2d, label=y_cls_test4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic', \n",
    "    'eval_metric': 'error',  \n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'alpha': 0.01,\n",
    "    'lambda':1.0,\n",
    "    'seed': seed,\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda:0'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Create\n",
    "num_rounds = 1000\n",
    "watchlist = [(dtrain, 'train'), (dtest1, 'test1'), (dtest2, 'test2'), (dtest3, 'test3')] \n",
    "model = xgb.train(params, dtrain, num_rounds, evals=watchlist, early_stopping_rounds=200,  verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric calculate\n",
    "performance(model, dtrain, y_cls_train)\n",
    "performance(model, dtest1, y_cls_test1)\n",
    "performance(model, dtest2, y_cls_test2)\n",
    "performance(model, dtest3, y_cls_test3)\n",
    "performance(model, dtest4, y_cls_test4)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "save_path = '../model/rlf_cls_xgb.json'\n",
    "config = model.save_model(save_path)\n",
    "\n",
    "# how to load\n",
    "# model2 = xgb.Booster()\n",
    "# model2.load_model(save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic', \n",
    "    'eval_metric': 'error',  \n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8, # on data\n",
    "    'colsample_bytree': 0.8, # on feature\n",
    "    'lambda': 0.01, # L2\n",
    "    'alpha': 0.01, # L1\n",
    "    'seed': seed,\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda:0'\n",
    "}\n",
    "\n",
    "# Model Create\n",
    "num_rounds = 200\n",
    "# watchlist = [(dtrain, 'train')]\n",
    "# watchlist = [(dtrain, 'train'), (dtest4, 'test4')] \n",
    "watchlist = [(dtrain, 'train'), (dtest1, 'test1'), (dtest2, 'test2'), (dtest3, 'test3')] \n",
    "model = xgb.train(params, dtrain, num_rounds, evals=watchlist,  early_stopping_rounds=20, verbose_eval=True)\n",
    "\n",
    "# Metric calculate\n",
    "performance(model, dtrain, y_cls_train)\n",
    "performance(model, dtest1, y_cls_test1)\n",
    "performance(model, dtest2, y_cls_test2)\n",
    "performance(model, dtest3, y_cls_test3)\n",
    "performance(model, dtest4, y_cls_test4)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dtest1, 'test1'), (dtest2, 'test2'), (dtest3, 'test3')] \n",
    "    \n",
    "# Define parameters range\n",
    "learning_rate_values = [0.05, 0.1, 0.2]\n",
    "max_depth_values = [4, 5, 6, 7, 8]\n",
    "subsample_values = [0.8, 0.9, 1.0]\n",
    "colsample_bytree_values = [0.8, 0.9, 1.0]\n",
    "alphas = [0.01,0.1,1]\n",
    "lambdas = [0.01,0.1,1]\n",
    "num_rounds_values = [50,100,200,300]\n",
    "r = product(learning_rate_values, max_depth_values, subsample_values, colsample_bytree_values, alphas, lambdas)\n",
    "\n",
    "savefile = '../info/xgb_record_no_early_stopping.csv'\n",
    "with open(savefile, 'w') as f:\n",
    "    print('lr, max_d, s_sample, cols_bytree, alpha, lambda, n,ACC(train), AUC(train), AUCPR(train), P(train), R(train), F1(train), ACC(train), AUC(test), AUCPR(test), P(test), R(test), F1(test)',file=f)\n",
    "    for lr, d, s, cbt, a, l in tqdm(r):\n",
    "        params = {'objective': 'binary:logistic', 'eval_metric': 'error',  'seed': seed,'tree_method': 'hist','device': 'cuda:0'}\n",
    "        params['learning_rate'] = lr\n",
    "        params['max_depth'] = d\n",
    "        params['subsample'] = s\n",
    "        params['colsample_bytree'] = cbt\n",
    "        params['alpha'] = a\n",
    "        params['lambda'] = l\n",
    "        for num_rounds in num_rounds_values:\n",
    "            model = xgb.train(params, dtrain, num_rounds, evals=watchlist,  early_stopping_rounds=20, verbose_eval=False)\n",
    "            \n",
    "            record = [lr, d, s, cbt, a, l, num_rounds]\n",
    "            record+=list(performance(model, dtrain, y_cls_train))\n",
    "            performance(model, dtest1, y_cls_test1)\n",
    "            performance(model, dtest2, y_cls_test2)\n",
    "            performance(model, dtest3, y_cls_test3)\n",
    "            record += list(performance(model, dtest4, y_cls_test4))\n",
    "            \n",
    "            params.clear()\n",
    "            record = [str(x) for x in record]\n",
    "            \n",
    "            print(','.join(record),end='\\n', file=f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary:logistic', \n",
    "    'eval_metric': 'error',  \n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8, # on data\n",
    "    'colsample_bytree': 0.8, # on feature\n",
    "    'lambda': 0, # L2\n",
    "    'alpha': 0, # L1\n",
    "    'seed': seed,\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda:0'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature columns\n",
    "cols = ['num_of_neis', 'RSRP','RSRQ','RSRP1','RSRQ1','nr-RSRP','nr-RSRQ','nr-RSRP1','nr-RSRQ1',\n",
    "        'E-UTRAN-eventA3','eventA5','NR-eventA3','eventB1-NR-r15',\n",
    "        'LTE_HO','MN_HO','SN_setup','SN_Rel','SN_HO', 'RLF','SCG_RLF']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP, FP, TN, FN = get_pred_result_ind(model, dtest4, y_cls_test4, X_test4)\n",
    "len(TP), len(FP), len(TN), len(FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP_cause = []\n",
    "for i in FP:\n",
    "    count = 1\n",
    "    while count<=pred_time:\n",
    "        df, ff = find_original_input(i+count, record_test4, time_seq_len, ffill_cols)\n",
    "        series = df.iloc[-1]\n",
    "        if (series['RLF_II'] or series['RLF_III']):\n",
    "            FP_cause.append(series['RLF_cause'])\n",
    "            break\n",
    "        count+=1        \n",
    "\n",
    "count = {'handoverFailure':0, 'otherFailure':0, 'reconfigurationFailure':0}\n",
    "for c in FP_cause:\n",
    "    for type in count.keys():\n",
    "        if c == type: count[type] += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify \n",
    "dtest = dtest4\n",
    "y_cls, y_fst, X_test = y_cls_test4, y_fst_test4, X_test4\n",
    "\n",
    "y_pred_proba = model.predict(dtest) \n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "FP_input = []\n",
    "FP_input_ind = []\n",
    "FP_time = []\n",
    "\n",
    "FN_input = []\n",
    "FN_input_ind = []\n",
    "\n",
    "TP_input = []\n",
    "TP_input_ind = []\n",
    "TP_time = []\n",
    "\n",
    "for i, (pred, label, t, x) in enumerate(zip(y_pred, y_cls, y_fst, X_test)):\n",
    "    if pred != label:\n",
    "        if label == 1: # FP analysis\n",
    "            FP_time.append(t)\n",
    "            x = pd.DataFrame(x, columns=features)\n",
    "            FP_input.append(x)\n",
    "            FP_input_ind.append(i)\n",
    "        else: # FN analysis\n",
    "            x = pd.DataFrame(x, columns=features)\n",
    "            FN_input.append(x)\n",
    "            FN_input_ind.append(i)\n",
    "    else: \n",
    "        if label == 1: # TP analysis\n",
    "            TP_time.append(t)\n",
    "            x = pd.DataFrame(x, columns=features)\n",
    "            TP_input.append(x)\n",
    "            TP_input_ind.append(i)\n",
    "            \n",
    "len(TP_input), len(FP_input), len(FN_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel filename\n",
    "file_record = record_test4\n",
    "excel_file = '../info/TP_data.xlsx'\n",
    "input_ind = TP_input_ind\n",
    "\n",
    "# ExcelWriter \n",
    "with pd.ExcelWriter(excel_file, engine='xlsxwriter') as writer:\n",
    "    for ind in tqdm(input_ind):\n",
    "        # x = X_test[ind] \n",
    "        # df = pd.DataFrame(x, columns=features)\n",
    "        df, tar_file = find_original_input(ind, file_record, time_seq_len)\n",
    "        df['filename'] = [tar_file] + [None]*(len(df) - 1)\n",
    "        df.to_excel(writer, sheet_name=f'{ind}', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, _ = find_original_input(316, file_record, time_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(TP_input_ind[:20])\n",
    "pprint(TP_time[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_test4_2d[621]\n",
    "x = np.expand_dims(x, axis=0)\n",
    "X = xgb.DMatrix(x)\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(FN_input)/1490"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failed CDF\n",
    "sorted_data = np.sort(FP_time)\n",
    "cumulative_distribution = np.arange(1, len(sorted_data) + 1) / (len(sorted_data)+len(FN_input))\n",
    "\n",
    "plt.ylim([0,1])\n",
    "plt.plot(sorted_data, cumulative_distribution, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Time away from RLF (second)')\n",
    "# plt.ylabel('Cumulative Distribution Function (CDF)')\n",
    "plt.title('CDF of the false prediced Data')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "importance = model.get_score(importance_type='gain')\n",
    "\n",
    "sorted_importance = {}\n",
    "for k, v in importance.items():\n",
    "    num = int(k[1:])\n",
    "    feature_name = features[num%len(features)]\n",
    "    sorted_importance[f'{feature_name} {time_seq_len-num//34}'] = v\n",
    "\n",
    "sorted_importance = sorted(sorted_importance.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Feature Importance\n",
    "top_features = 20\n",
    "\n",
    "data, labels = [], []\n",
    "for f, score in reversed(sorted_importance[:top_features]):\n",
    "    data.append(round(score))\n",
    "    labels.append(f)\n",
    "\n",
    "bars = plt.barh(labels, data)\n",
    "plt.bar_label(bars)\n",
    "\n",
    "# 設置標題和標籤\n",
    "plt.title('Feature importance')\n",
    "plt.xlabel('F score (gain)')\n",
    "plt.ylabel('Features')\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "# 顯示圖表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
